# Deep-Learning-Notes

# The Little Book of Deep Learning

## æ·±åº¦å­¦ä¹ å°ä¹¦

### Francois Fleuret

#### æ—¥å†…ç“¦å¤§å­¦

---

## ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€

1. **æœºå™¨å­¦ä¹ **
   - 1.1 ä»æ•°æ®ä¸­å­¦ä¹ 
   - 1.2 åŸºå‡½æ•°å›å½’
   - 1.3 æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ
   - 1.4 æ¨¡å‹ç±»åˆ«

2. **é«˜æ•ˆè®¡ç®—**
   - 2.1 GPUã€TPU å’Œæ‰¹å¤„ç†
   - 2.2 å¼ é‡

3. **è®­ç»ƒ**
   - 3.1 æŸå¤±å‡½æ•°
   - 3.2 è‡ªå›å½’æ¨¡å‹
   - 3.3 æ¢¯åº¦ä¸‹é™
   - 3.4 åå‘ä¼ æ’­
   - 3.5 æ·±åº¦çš„ä»·å€¼
   - 3.6 è®­ç»ƒåè®®
   - 3.7 è§„æ¨¡çš„å¥½å¤„

### ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦æ¨¡å‹

4. **æ¨¡å‹ç»„ä»¶**
   - 4.1 å±‚çš„æ¦‚å¿µ
   - 4.2 çº¿æ€§å±‚
   - 4.3 æ¿€æ´»å‡½æ•°
   - 4.4 æ± åŒ–
   - 4.5 Dropout
   - 4.6 å½’ä¸€åŒ–å±‚
   - 4.7 è·³è·ƒè¿æ¥
   - 4.8 æ³¨æ„åŠ›å±‚
   - 4.9 è¯åµŒå…¥
   - 4.10 ä½ç½®ç¼–ç 

5. **æ¶æ„**
   - 5.1 å¤šå±‚æ„ŸçŸ¥æœº
   - 5.2 å·ç§¯ç½‘ç»œ
   - 5.3 æ³¨æ„åŠ›æ¨¡å‹

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šåº”ç”¨

6. **é¢„æµ‹**
   - 6.1 å›¾åƒå»å™ª
   - 6.2 å›¾åƒåˆ†ç±»
   - 6.3 ç›®æ ‡æ£€æµ‹
   - 6.4 è¯­ä¹‰åˆ†å‰²
   - 6.5 è¯­éŸ³è¯†åˆ«
   - 6.6 æ–‡æœ¬-å›¾åƒè¡¨ç¤º
   - 6.7 å¼ºåŒ–å­¦ä¹ 

7. **åˆæˆ**
   - 7.1 æ–‡æœ¬ç”Ÿæˆ
   - 7.2 å›¾åƒç”Ÿæˆ

8. **è®¡ç®—åˆ†è£‚**
   - 8.1 æç¤ºå·¥ç¨‹
   - 8.2 é‡åŒ–
   - 8.3 é€‚é…å™¨
   - 8.4 æ¨¡å‹åˆå¹¶

---

## å‰è¨€

å½“å‰äººå·¥æ™ºèƒ½çš„è¿›æ­¥æ˜¯ç”± Krizhevsky ç­‰äººåœ¨ 2012 å¹´å±•ç¤ºçš„ï¼Œä»–ä»¬è¯æ˜äº†äºŒåå¹´å‰è®¾è®¡çš„ç¥ç»ç½‘ç»œå¯ä»¥é€šè¿‡ç®€å•åœ°æ‰©å¤§è§„æ¨¡å’Œæ•°æ®é›†æ¥å¤§å¹…è¶…è¶Šå½“æ—¶æœ€å…ˆè¿›çš„å›¾åƒè¯†åˆ«æ–¹æ³•ã€‚è¿™ä¸€çªç ´å¾—ç›Šäºå›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ï¼Œè¿™äº›é«˜åº¦å¹¶è¡Œçš„æ¶ˆè´¹çº§è®¡ç®—è®¾å¤‡æœ€åˆæ˜¯ä¸ºå®æ—¶å›¾åƒåˆæˆå¼€å‘çš„ï¼Œåæ¥è¢«é‡æ–°ç”¨äºäººå·¥ç¥ç»ç½‘ç»œã€‚

è‡ªé‚£æ—¶èµ·ï¼Œåœ¨â€œæ·±åº¦å­¦ä¹ â€è¿™ä¸€æ€»ç§°ä¸‹ï¼Œç½‘ç»œç»“æ„çš„åˆ›æ–°ã€è®­ç»ƒç­–ç•¥çš„æ”¹è¿›ä»¥åŠä¸“ç”¨ç¡¬ä»¶çš„å¼€å‘ä½¿å¾—æ¨¡å‹çš„è§„æ¨¡å’Œè®­ç»ƒæ•°æ®çš„æ•°é‡å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨èŒƒå›´ä»è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯æ‰©å±•åˆ°è¯­éŸ³å¤„ç†ï¼Œå¹¶åœ¨ 2020 å¹´åæ¨åŠ¨äº†å…·æœ‰é€šç”¨æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚

å°½ç®¡æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå¹¶ä¸éš¾ç†è§£ï¼Œä½†å®ƒç»“åˆäº†çº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†ã€æ¦‚ç‡ã€ä¼˜åŒ–ã€ä¿¡å·å¤„ç†ã€ç¼–ç¨‹ã€ç®—æ³•å’Œé«˜æ€§èƒ½è®¡ç®—ç­‰å¤šç§ç»„ä»¶ï¼Œä½¿å¾—å­¦ä¹ è¿‡ç¨‹å˜å¾—å¤æ‚ã€‚æœ¬ä¹¦å¹¶ä¸è¯•å›¾é¢é¢ä¿±åˆ°ï¼Œè€Œæ˜¯ä¸“æ³¨äºç†è§£å‡ ä¸ªé‡è¦æ¨¡å‹æ‰€éœ€çš„èƒŒæ™¯çŸ¥è¯†ã€‚è¿™ç§ç®€æ´çš„æ–¹å¼åœ¨å‘å¸ƒåçš„ 12 ä¸ªæœˆå†…è·å¾—äº†è¶…è¿‡ 50 ä¸‡æ¬¡çš„ä¸‹è½½é‡ã€‚

å¦‚æœä½ ä¸æ˜¯ä»å®˜æ–¹ URL è·å–æœ¬ä¹¦ï¼Œè¯·è®¿é—® [https://fleuret.org/public/lbdl.pdf](https://fleuret.org/public/lbdl.pdf)ï¼Œä»¥ä¾¿ç»Ÿè®¡è¯»è€…æ•°é‡ã€‚

---

**Chapter 1: Machine Learning**

### 1.1 Learning from data

The simplest use case for a model trained from data is when a signal x is accessible, for instance, the picture of a license plate, from which one wants to predict a quantity y, such as the string of characters written on the plate.

**ä»æ•°æ®ä¸­å­¦ä¹ **

ä»æ•°æ®ä¸­è®­ç»ƒæ¨¡å‹æœ€ç®€å•çš„ç”¨ä¾‹æ˜¯å½“ä¸€ä¸ªä¿¡å· x å¯è·å–æ—¶ï¼Œä¾‹å¦‚è½¦ç‰Œçš„å›¾ç‰‡ï¼Œäººä»¬å¸Œæœ›ä»ä¸­é¢„æµ‹ä¸€ä¸ªé‡ yï¼Œæ¯”å¦‚è½¦ç‰Œä¸Šå†™çš„å­—ç¬¦å­—ç¬¦ä¸²ã€‚

---

In many real-world situations where x is a high-dimensional signal captured in an uncontrolled environment, it is too complicated to come up with an analytical recipe that relates x and y.

åœ¨è®¸å¤šçœŸå®ä¸–ç•Œçš„æƒ…å¢ƒä¸­ï¼Œx æ˜¯åœ¨ä¸å¯æ§ç¯å¢ƒä¸­æ•è·çš„é«˜ç»´ä¿¡å·ï¼Œæƒ³è¦æ‰¾åˆ°ä¸€ä¸ªå°† x å’Œ y å…³è”èµ·æ¥çš„è§£æå…¬å¼è¿‡äºå¤æ‚ã€‚

---

What one can do is to collect a large training set ğ’Ÿ of pairs (xn, yn), and devise a parametric model f. This is a piece of computer code that incorporates trainable parameters w that modulate its behavior, and such that, with the proper values wâˆ—, it is a good predictor.

äººä»¬å¯ä»¥åšçš„æ˜¯æ”¶é›†å¤§é‡çš„è®­ç»ƒé›† ğ’Ÿï¼ŒåŒ…æ‹¬ (xn, yn) å¯¹ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªå‚æ•°æ¨¡å‹ fã€‚è¿™æ˜¯ä¸€æ®µåŒ…å«å¯è®­ç»ƒå‚æ•° w çš„è®¡ç®—æœºä»£ç ï¼Œè¿™äº›å‚æ•°è°ƒèŠ‚å…¶è¡Œä¸ºï¼Œå¹¶ä¸”é€šè¿‡é€‚å½“çš„å‚æ•°å€¼ wâˆ—ï¼Œå®ƒå¯ä»¥æˆä¸ºä¸€ä¸ªè‰¯å¥½çš„é¢„æµ‹å™¨ã€‚

---

â€œGoodâ€ here means that if an x is given to this piece of code, the value \( \hat{y} = f(x; w^*) \) it computes is a good estimate of the y that would have been associated with x in the training set had it been there.

è¿™é‡Œçš„â€œè‰¯å¥½â€æ„å‘³ç€ï¼Œå¦‚æœç»™è¿™æ®µä»£ç ä¸€ä¸ª xï¼Œå®ƒè®¡ç®—å‡ºçš„å€¼ \( \hat{y} = f(x; w^*) \) æ˜¯å¯¹ y çš„ä¸€ä¸ªè‰¯å¥½ä¼°è®¡ï¼Œå‡å¦‚ x å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼Œy å°±ä¼šä¸ x å…³è”ã€‚

---

This notion of goodness is usually formalized with a loss \( \mathcal{L}(w) \) which is small when \( f(\cdot; w) \) is good on ğ’Ÿ. Then, training the model consists of computing a value \( w^* \) that minimizes \( \mathcal{L}(w^*) \).

è¿™ç§è‰¯å¥½çš„æ¦‚å¿µé€šå¸¸é€šè¿‡æŸå¤±å‡½æ•° \( \mathcal{L}(w) \) æ¥å½¢å¼åŒ–ï¼Œå½“ \( f(\cdot; w) \) åœ¨è®­ç»ƒé›† ğ’Ÿ ä¸Šè¡¨ç°è‰¯å¥½æ—¶ï¼ŒæŸå¤±è¾ƒå°ã€‚å› æ­¤ï¼Œè®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹æ˜¯è®¡ç®—ä¸€ä¸ªæœ€å°åŒ– \( \mathcal{L}(w^*) \) çš„å€¼ \( w^* \)ã€‚

---

Most of the content of this book is about the definition of f, which, in realistic scenarios, is a complex combination of pre-defined sub-modules.

æœ¬ä¹¦çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½å›´ç»•ç€ f çš„å®šä¹‰ï¼Œåœ¨ç°å®åœºæ™¯ä¸­ï¼Œf æ˜¯é¢„å®šä¹‰å­æ¨¡å—çš„å¤æ‚ç»„åˆã€‚

---

### 1.4 Categories of models

We can organize the use of machine learning models into three broad categories:

**æ¨¡å‹ç±»åˆ«**

æˆ‘ä»¬å¯ä»¥å°†æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä½¿ç”¨åˆ’åˆ†ä¸ºä¸‰å¤§ç±»ï¼š

---

- **Regression** consists of predicting a continuous-valued vector \( y \in \mathbb{R}^K \), for instance, a geometrical position of an object, given an input signal \( x \).
  
  **å›å½’** æŒ‡çš„æ˜¯é¢„æµ‹ä¸€ä¸ªè¿ç»­å€¼å‘é‡ \( y \in \mathbb{R}^K \)ï¼Œä¾‹å¦‚ï¼Œæ ¹æ®è¾“å…¥ä¿¡å· \( x \) é¢„æµ‹ç‰©ä½“çš„å‡ ä½•ä½ç½®ã€‚

---

- **Classification** aims at predicting a value from a finite set \( \{1, ..., C\} \), for instance, the label \( y \) of an image \( x \).
  
  **åˆ†ç±»** æ—¨åœ¨ä»ä¸€ä¸ªæœ‰é™é›†åˆ \( \{1, ..., C\} \) ä¸­é¢„æµ‹ä¸€ä¸ªå€¼ï¼Œä¾‹å¦‚ï¼Œç»™å®šå›¾åƒ \( x \) é¢„æµ‹å…¶æ ‡ç­¾ \( y \)ã€‚

---

- **Density modeling** has as its objective to model the probability density function of the data \( \mu_X \), for instance, images.
  
  **å¯†åº¦å»ºæ¨¡** æ—¨åœ¨å¯¹æ•°æ®çš„æ¦‚ç‡å¯†åº¦å‡½æ•° \( \mu_X \) è¿›è¡Œå»ºæ¨¡ï¼Œä¾‹å¦‚å›¾åƒæ•°æ®ã€‚

---

Both regression and classification are generally referred to as **supervised learning**, since the value to be predicted must be provided as a target during training. On the contrary, density modeling is usually seen as **unsupervised learning**, as it does not require labeled data.

å›å½’å’Œåˆ†ç±»é€šå¸¸è¢«ç§°ä¸º **ç›‘ç£å­¦ä¹ **ï¼Œå› ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¿…é¡»æä¾›ä¸€ä¸ªç›®æ ‡å€¼è¿›è¡Œé¢„æµ‹ã€‚ç›¸åï¼Œå¯†åº¦å»ºæ¨¡é€šå¸¸è¢«è§†ä¸º **æ— ç›‘ç£å­¦ä¹ **ï¼Œå› ä¸ºå®ƒä¸éœ€è¦æ ‡æ³¨æ•°æ®ã€‚

---

These three categories are not disjoint; for instance, classification can be cast as class-score regression, or discrete sequence density modeling as iterated classification.

è¿™ä¸‰ç±»å¹¶ä¸æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ä¾‹å¦‚ï¼Œåˆ†ç±»é—®é¢˜å¯ä»¥è¢«è§†ä¸ºç±»åˆ«åˆ†æ•°çš„å›å½’ï¼Œè€Œç¦»æ•£åºåˆ—çš„å¯†åº¦å»ºæ¨¡å¯ä»¥è¢«çœ‹ä½œæ˜¯è¿­ä»£çš„åˆ†ç±»é—®é¢˜ã€‚

---

Furthermore, they do not cover all cases. One may want to predict compounded quantities, multiple classes, or model a density conditional on a signal.

æ­¤å¤–ï¼Œè¿™ä¸‰ç±»å¹¶ä¸èƒ½æ¶µç›–æ‰€æœ‰æƒ…å†µã€‚æœ‰æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦é¢„æµ‹å¤åˆé‡ã€å¤šä¸ªç±»åˆ«ï¼Œæˆ–è€…å¯¹æŸä¸ªä¿¡å·çš„æ¡ä»¶å¯†åº¦è¿›è¡Œå»ºæ¨¡ã€‚

---


**Chapter 2: Efficient Computation**

### 2.1 GPUs, TPUs, and batches

From an implementation standpoint, deep learning is about executing heavy computations with large amounts of data.

**é«˜æ•ˆè®¡ç®—**

ä»å®ç°çš„è§’åº¦æ¥çœ‹ï¼Œæ·±åº¦å­¦ä¹ æ¶‰åŠä½¿ç”¨å¤§é‡æ•°æ®æ‰§è¡Œé«˜å¼ºåº¦è®¡ç®—ã€‚

---

The **Graphical Processing Units (GPUs)** have been instrumental in the success of the field by allowing such computations to be run on affordable hardware.

**å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUsï¼‰** åœ¨è¯¥é¢†åŸŸçš„æˆåŠŸä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½¿å¾—è¿™äº›è®¡ç®—å¯ä»¥åœ¨ç»æµå®æƒ çš„ç¡¬ä»¶ä¸Šè¿è¡Œã€‚

---

The importance of their use, and the resulting technical constraints on the computations that can be done efficiently, force the research in the field to constantly balance mathematical soundness and implementability of novel methods.

å®ƒä»¬çš„ä½¿ç”¨è‡³å…³é‡è¦ï¼Œå¹¶ä¸”ç”±æ­¤äº§ç”Ÿçš„æŠ€æœ¯çº¦æŸå†³å®šäº†å¯ä»¥é«˜æ•ˆå®Œæˆçš„è®¡ç®—ç±»å‹ï¼Œè¿™ä¿ƒä½¿è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸æ–­åœ¨æ•°å­¦ä¸¥è°¨æ€§å’Œæ–°æ–¹æ³•çš„å¯å®ç°æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

---

Graphical Processing Units were originally designed for real-time image synthesis, which requires highly parallel architectures that happen to be well suited for deep models.

å›¾å½¢å¤„ç†å•å…ƒæœ€åˆæ˜¯ä¸ºå®æ—¶å›¾åƒåˆæˆè€Œè®¾è®¡çš„ï¼Œè¿™éœ€è¦é«˜åº¦å¹¶è¡Œçš„æ¶æ„ï¼Œè€Œè¿™ç§æ¶æ„æ°å¥½éå¸¸é€‚åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

---

As their usage for AI has increased, GPUs have been equipped with dedicated **tensor cores**, and deep-learning specialized chips such as Google's **Tensor Processing Units (TPUs)** have been developed.

éšç€å®ƒä»¬åœ¨äººå·¥æ™ºèƒ½ä¸­çš„ä½¿ç”¨å¢åŠ ï¼ŒGPU å·²ç»é…å¤‡äº†ä¸“é—¨çš„ **å¼ é‡æ ¸å¿ƒï¼ˆtensor coresï¼‰**ï¼Œå¹¶ä¸”å¼€å‘å‡ºäº†ä¸“é—¨ç”¨äºæ·±åº¦å­¦ä¹ çš„èŠ¯ç‰‡ï¼Œå¦‚è°·æ­Œçš„ **å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUsï¼‰**ã€‚

---

A GPU possesses several thousand parallel units and its own fast memory. The limiting factor is usually not the number of computing units, but the **read-write operations to memory**.

GPU å…·æœ‰æ•°åƒä¸ªå¹¶è¡Œè®¡ç®—å•å…ƒå’Œä¸“ç”¨çš„é«˜é€Ÿå†…å­˜ã€‚é€šå¸¸çš„é™åˆ¶å› ç´ å¹¶ä¸æ˜¯è®¡ç®—å•å…ƒçš„æ•°é‡ï¼Œè€Œæ˜¯ **å¯¹å†…å­˜çš„è¯»å†™æ“ä½œ**ã€‚

---

The slowest link is between the **CPU memory and the GPU memory**, and consequently, one should avoid copying data across devices.

æœ€æ…¢çš„ç¯èŠ‚åœ¨äº **CPU å†…å­˜ä¸ GPU å†…å­˜ä¹‹é—´çš„ä¼ è¾“**ï¼Œå› æ­¤åº”å°½é‡é¿å…åœ¨è®¾å¤‡ä¹‹é—´å¤åˆ¶æ•°æ®ã€‚

---

Moreover, the structure of the GPU itself involves multiple levels of **cache memory**, which are smaller but faster, and computation should be organized to avoid unnecessary memory transfers between these different caches.

æ­¤å¤–ï¼ŒGPU æœ¬èº«çš„æ¶æ„æ¶‰åŠå¤šä¸ªå±‚æ¬¡çš„ **ç¼“å­˜å†…å­˜**ï¼Œè¿™äº›ç¼“å­˜è¾ƒå°ä½†é€Ÿåº¦æ›´å¿«ï¼Œå› æ­¤è®¡ç®—åº”å°½é‡é¿å…åœ¨è¿™äº›ä¸åŒçš„ç¼“å­˜ä¹‹é—´è¿›è¡Œä¸å¿…è¦çš„æ•°æ®ä¼ è¾“ã€‚

---

This is achieved, in particular, by organizing the computation in **batches** of samples that can fit entirely in the GPU memory and are processed in parallel.

ä¸ºæ­¤ï¼Œè®¡ç®—é€šå¸¸é‡‡ç”¨ **æ‰¹å¤„ç†ï¼ˆbatchesï¼‰**ï¼Œå³å°†å¤šä¸ªæ ·æœ¬ç»„åˆæˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œä½¿å…¶èƒ½å¤Ÿå®Œå…¨è£…å…¥ GPU å†…å­˜å¹¶è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚

---

A standard GPU has a theoretical peak **performance** of **10Â¹Â³â€“10Â¹â´ floating-point operations (FLOPs) per second**, and its memory typically ranges from **8 to 80 gigabytes**.

æ ‡å‡† GPU çš„ç†è®ºå³°å€¼ **è®¡ç®—æ€§èƒ½** çº¦ä¸º **10Â¹Â³â€“10Â¹â´ æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰æ¯ç§’**ï¼Œå…¶å†…å­˜é€šå¸¸åœ¨ **8 åˆ° 80 GB** ä¹‹é—´ã€‚

---

The standard **FP32 (32-bit floating-point encoding)** is commonly used for numerical precision, but empirical results show that using **16-bit or lower** precision does not significantly degrade performance.

æ ‡å‡†çš„ **FP32ï¼ˆ32 ä½æµ®ç‚¹ç¼–ç ï¼‰** å¸¸ç”¨äºæ•°å€¼è®¡ç®—ï¼Œä½†ç»éªŒè¡¨æ˜ï¼Œä½¿ç”¨ **16 ä½æˆ–æ›´ä½** ç²¾åº¦ä¸ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ã€‚

---

We will come back to the **large size of deep architectures** in Section 3.7.

æˆ‘ä»¬å°†åœ¨ **3.7 èŠ‚** è¿›ä¸€æ­¥æ¢è®¨ **æ·±åº¦æ¶æ„çš„è§„æ¨¡é—®é¢˜**ã€‚

---

### 2.2 Tensors

**å¼ é‡ï¼ˆTensorsï¼‰**

GPUs and deep learning frameworks such as PyTorch or JAX manipulate the quantities to be processed by organizing them as **tensors**, which are series of scalars arranged along several discrete axes.

GPU å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorch æˆ– JAXï¼‰é€šè¿‡**å¼ é‡ï¼ˆtensorsï¼‰** ç»„ç»‡å’Œå¤„ç†æ•°æ®ï¼Œå¼ é‡æ˜¯æŒ‰ç…§å¤šä¸ªç¦»æ•£è½´æ’åˆ—çš„ä¸€ç³»åˆ—æ ‡é‡ã€‚

---

Tensors are elements of \( \mathbb{R}^{N_1 \times \dots \times N_D} \), generalizing the notion of vectors and matrices.

å¼ é‡æ˜¯ \( \mathbb{R}^{N_1 \times \dots \times N_D} \) ä¸­çš„å…ƒç´ ï¼Œæ˜¯å‘é‡å’ŒçŸ©é˜µçš„å¹¿ä¹‰è¡¨ç¤ºã€‚

---

Tensors are used to represent both the signals to be processed, the **trainable parameters** of the models, and the intermediate quantities they compute. The latter are called **activations**, in reference to neuronal activations.

å¼ é‡ç”¨äºè¡¨ç¤ºå¾…å¤„ç†çš„ä¿¡å·ã€æ¨¡å‹çš„**å¯è®­ç»ƒå‚æ•°**ï¼Œä»¥åŠè®¡ç®—å‡ºçš„ä¸­é—´é‡ã€‚åè€…è¢«ç§°ä¸º **æ¿€æ´»å€¼ï¼ˆactivationsï¼‰**ï¼Œç±»ä¼¼äºç¥ç»å…ƒçš„æ¿€æ´»çŠ¶æ€ã€‚

---

For instance, a time series is naturally encoded as a **T Ã— D** tensor, where **T** is its duration and **D** is the dimension of the feature representation at every time step, often referred to as the **number of channels**.

ä¾‹å¦‚ï¼Œæ—¶é—´åºåˆ—é€šå¸¸è¢«ç¼–ç ä¸º **T Ã— D** å½¢çŠ¶çš„å¼ é‡ï¼Œå…¶ä¸­ **T** æ˜¯æŒç»­æ—¶é—´ï¼Œ**D** æ˜¯æ¯ä¸ªæ—¶é—´æ­¥ç‰¹å¾è¡¨ç¤ºçš„ç»´åº¦ï¼Œé€šå¸¸è¢«ç§°ä¸º **é€šé“æ•°ï¼ˆchannelsï¼‰**ã€‚

---

Similarly, a 2D-structured signal can be represented as a **D Ã— H Ã— W** tensor, where **H** and **W** are its height and width. An RGB image would correspond to **D = 3**, but the number of channels can grow up to several thousands in large models.

ç±»ä¼¼åœ°ï¼ŒäºŒç»´ç»“æ„åŒ–ä¿¡å·å¯ä»¥è¡¨ç¤ºä¸º **D Ã— H Ã— W** å½¢çŠ¶çš„å¼ é‡ï¼Œå…¶ä¸­ **H** å’Œ **W** åˆ†åˆ«è¡¨ç¤ºé«˜åº¦å’Œå®½åº¦ã€‚å¯¹äº RGB å›¾åƒï¼Œ**D = 3**ï¼Œä½†åœ¨å¤§å‹æ¨¡å‹ä¸­ï¼Œé€šé“æ•°å¯èƒ½ä¼šå¢é•¿åˆ°å‡ åƒä¸ªã€‚

---

**Chapter 3: Training**

### 3.1 Losses

**æŸå¤±å‡½æ•°**

As introduced in Section 1.1, training a model consists of minimizing a loss \( \mathcal{L}(w) \) which reflects the performance of the predictor \( f(\cdot; w) \) on a training set \( \mathcal{D} \).

å¦‚ 1.1 èŠ‚æ‰€è¿°ï¼Œè®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•° \( \mathcal{L}(w) \)ï¼Œè¯¥æŸå¤±å‡½æ•°åæ˜ äº†é¢„æµ‹å‡½æ•° \( f(\cdot; w) \) åœ¨è®­ç»ƒé›† \( \mathcal{D} \) ä¸Šçš„æ€§èƒ½ã€‚

---

Since models are usually extremely complex, and their performance is directly related to how well the loss is minimized, this minimization is a key challenge, which involves both computational and mathematical difficulties.

ç”±äºæ¨¡å‹é€šå¸¸æä¸ºå¤æ‚ï¼Œå¹¶ä¸”å®ƒä»¬çš„æ€§èƒ½ç›´æ¥ä¸æŸå¤±æœ€å°åŒ–çš„ç¨‹åº¦ç›¸å…³ï¼Œå› æ­¤è¿™ä¸€æœ€å°åŒ–è¿‡ç¨‹æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œæ¶‰åŠè®¡ç®—å’Œæ•°å­¦ä¸Šçš„éš¾é¢˜ã€‚

---

The example of the **mean squared error (MSE)** from Equation (1.1) is a standard loss for predicting a continuous value.

åœ¨ 1.1 èŠ‚ä¸­çš„ **å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰** æ˜¯é¢„æµ‹è¿ç»­å€¼æ—¶çš„ä¸€ç§æ ‡å‡†æŸå¤±å‡½æ•°ã€‚

---

For **density modeling**, the standard loss is the **likelihood of the data**. If \( f(x; w) \) is to be interpreted as a normalized log-probability or log-density, the loss is the opposite of the sum of its values over training samples, which corresponds to the likelihood of the dataset.

å¯¹äº **å¯†åº¦å»ºæ¨¡**ï¼Œæ ‡å‡†æŸå¤±å‡½æ•°æ˜¯ **æ•°æ®çš„ä¼¼ç„¶æ€§ï¼ˆlikelihoodï¼‰**ã€‚å¦‚æœ \( f(x; w) \) è¢«è§£é‡Šä¸ºå½’ä¸€åŒ–çš„å¯¹æ•°æ¦‚ç‡æˆ–å¯¹æ•°å¯†åº¦ï¼Œåˆ™æŸå¤±æ˜¯å…¶åœ¨è®­ç»ƒæ ·æœ¬ä¸Šçš„å€¼ä¹‹å’Œçš„ç›¸åæ•°ï¼Œå¯¹åº”äºæ•°æ®é›†çš„ä¼¼ç„¶æ€§ã€‚

---

### 3.2 Autoregressive models

**è‡ªå›å½’æ¨¡å‹**

A key class of methods, particularly for dealing with discrete sequences in natural language processing and computer vision, are the **autoregressive models**.

ä¸€ä¸ªé‡è¦çš„æ–¹æ³•ç±»åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­çš„ç¦»æ•£åºåˆ—å¤„ç†ï¼Œä¾¿æ˜¯ **è‡ªå›å½’æ¨¡å‹ï¼ˆautoregressive modelsï¼‰**ã€‚

---

#### The chain rule for probabilities

**æ¦‚ç‡çš„é“¾å¼æ³•åˆ™**

Such models put to use the **chain rule** from probability theory:

æ­¤ç±»æ¨¡å‹åˆ©ç”¨äº†æ¦‚ç‡è®ºä¸­çš„ **é“¾å¼æ³•åˆ™ï¼ˆchain ruleï¼‰**ï¼š

\[
P(X_1 = x_1, X_2 = x_2, ..., X_T = x_T) = P(X_1 = x_1) \times P(X_2 = x_2 | X_1 = x_1) \times ... \times P(X_T = x_T | X_1 = x_1, ..., X_{T-1} = x_{T-1}).
\]

---

Although this decomposition is valid for a random sequence of any type, it is particularly efficient when the signal of interest is a sequence of **tokens** from a finite **vocabulary**.

å°½ç®¡è¿™ç§åˆ†è§£å¯¹ä»»ä½•ç±»å‹çš„éšæœºåºåˆ—éƒ½é€‚ç”¨ï¼Œä½†å½“ç›®æ ‡ä¿¡å·æ˜¯æ¥è‡ªæœ‰é™ **è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰** çš„ **ä»¤ç‰Œï¼ˆtokensï¼‰** åºåˆ—æ—¶ï¼Œå®ƒå°¤å…¶é«˜æ•ˆã€‚

---

With the convention that the additional token \( \emptyset \) stands for an â€œunknownâ€ quantity, we can represent the event \( \{X_1 = x_1, ..., X_t = x_t\} \) as the vector \( (x_1, ..., x_t, \emptyset, ..., \emptyset) \).

æŒ‰ç…§æƒ¯ä¾‹ï¼Œé¢å¤–çš„ä»¤ç‰Œ \( \emptyset \) ä»£è¡¨â€œæœªçŸ¥â€å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†äº‹ä»¶ \( \{X_1 = x_1, ..., X_t = x_t\} \) è¡¨ç¤ºä¸ºå‘é‡ \( (x_1, ..., x_t, \emptyset, ..., \emptyset) \)ã€‚

---

Then, a model \( f \) which, given such an input, computes a vector of logits corresponding to \( P(X_t | X_1 = x_1, ..., X_{t-1} = x_{t-1}) \), allows one to sample one token given the previous ones.

ç„¶åï¼Œæ¨¡å‹ \( f \) åœ¨ç»™å®šæ­¤ç±»è¾“å…¥æ—¶è®¡ç®—ä¸€ç»„å¯¹åº”äº \( P(X_t | X_1 = x_1, ..., X_{t-1} = x_{t-1}) \) çš„ **logits**ï¼Œä»è€Œå…è®¸æ ¹æ®ä¹‹å‰çš„ä»¤ç‰Œé‡‡æ ·ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚

---

The chain rule ensures that by sampling \( T \) tokens \( x_t \), one at a time given the previously sampled \( x_1, ..., x_{t-1} \), we get a sequence that follows the joint distribution. This is an **autoregressive generative model**.

é“¾å¼æ³•åˆ™ä¿è¯äº†é€šè¿‡é€ä¸ªé‡‡æ · \( T \) ä¸ªä»¤ç‰Œ \( x_t \)ï¼Œåœ¨æ¯ä¸ªæ­¥éª¤åŸºäºä¹‹å‰çš„ \( x_1, ..., x_{t-1} \)ï¼Œå¯ä»¥ç”Ÿæˆç¬¦åˆè”åˆåˆ†å¸ƒçš„åºåˆ—ã€‚è¿™ä¾¿æ˜¯ **è‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆautoregressive generative modelï¼‰**ã€‚

---

Training such a model can be done by minimizing the sum across training sequences and time steps of the **cross-entropy loss**:

è®­ç»ƒæ­¤ç±»æ¨¡å‹çš„æ–¹æ³•æ˜¯æœ€å°åŒ–æ•´ä¸ªè®­ç»ƒåºåˆ—å’Œæ—¶é—´æ­¥çš„ **äº¤å‰ç†µæŸå¤±ï¼ˆcross-entropy lossï¼‰**ï¼š

\[
\mathcal{L}_{ce}(w) = \sum_{n=1}^{N} \sum_{t=1}^{T} -\log P(X_t = x_t | X_1 = x_1, ..., X_{t-1} = x_{t-1}; w).
\]

---

### Chapter 3.3 Gradient Descent

#### 3.3 æ¢¯åº¦ä¸‹é™

Except in specific cases like the linear regression we saw in Â§ 1.2, the optimal parameters \( w^* \) do not have a closed-form expression. In the general case, the tool of choice to minimize a function is gradient descent. It starts by initializing the parameters with a random \( w_0 \), and then improves this estimate by iterating gradient steps, each consisting of computing the gradient of the loss with respect to the parameters, and subtracting a fraction of it:

é™¤äº†æˆ‘ä»¬åœ¨Â§1.2ä¸­çœ‹åˆ°çš„çº¿æ€§å›å½’ç­‰ç‰¹å®šæƒ…å†µå¤–ï¼Œæœ€ä¼˜å‚æ•°\( w^* \)é€šå¸¸æ²¡æœ‰é—­å¼è§£ã€‚åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæœ€å°åŒ–å‡½æ•°çš„é¦–é€‰å·¥å…·æ˜¯æ¢¯åº¦ä¸‹é™ã€‚å®ƒä»éšæœºåˆå§‹åŒ–å‚æ•°\( w_0 \)å¼€å§‹ï¼Œç„¶åé€šè¿‡è¿­ä»£æ¢¯åº¦æ­¥éª¤æ¥æ”¹è¿›è¿™ä¸ªä¼°è®¡å€¼ï¼Œæ¯ä¸€æ­¥åŒ…æ‹¬è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºå‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶å‡å»å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼š

\[w_{n+1} = w_n - \eta \nabla \mathcal{L}_{|w}(w_n).\]

(3.1)

This procedure corresponds to moving the current estimate a bit in the direction that locally decreases \(\mathcal{L}(w)\) maximally, as illustrated in Figure 3.2.

è¿™ä¸ªè¿‡ç¨‹å¯¹åº”äºå°†å½“å‰ä¼°è®¡å€¼æ²¿ç€å±€éƒ¨æœ€å¤§å‡å°‘\(\mathcal{L}(w)\)çš„æ–¹å‘ç§»åŠ¨ä¸€å°æ­¥ï¼Œå¦‚å›¾3.2æ‰€ç¤ºã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰**ï¼šæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚å®ƒé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°ï¼Œä»è€Œé€æ­¥é€¼è¿‘æŸå¤±å‡½æ•°çš„æœ€å°å€¼ã€‚
- **å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**ï¼šå…¬å¼ä¸­çš„\(\eta\)æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°å‚æ•°çš„æ­¥é•¿ã€‚å­¦ä¹ ç‡è¿‡å¤§å¯èƒ½å¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹ä¸ç¨³å®šï¼Œè¿‡å°åˆ™å¯èƒ½å¯¼è‡´æ”¶æ•›é€Ÿåº¦è¿‡æ…¢ã€‚
- **éšæœºåˆå§‹åŒ–ï¼ˆRandom Initializationï¼‰**ï¼šåœ¨å¼€å§‹æ¢¯åº¦ä¸‹é™ä¹‹å‰ï¼Œæ¨¡å‹å‚æ•°é€šå¸¸ä¼šè¢«éšæœºåˆå§‹åŒ–ï¼Œä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

#### Learning rate

The hyper-parameter \(\eta\) is called the learning rate. It is a positive value that modulates how quickly the minimization is done, and must be chosen carefully.

è¶…å‚æ•°\(\eta\)ç§°ä¸ºå­¦ä¹ ç‡ã€‚å®ƒæ˜¯ä¸€ä¸ªæ­£å€¼ï¼Œè°ƒèŠ‚æœ€å°åŒ–çš„é€Ÿåº¦ï¼Œå¿…é¡»è°¨æ…é€‰æ‹©ã€‚

If it is too small, the optimization will be slow at best, and may be trapped in a local minimum early. If it is too large, the optimization may bounce around a good minimum and never descend into it. As we will see in Â§ 3.6, it can depend on the iteration number \( n \).

å¦‚æœå­¦ä¹ ç‡å¤ªå°ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¼šéå¸¸ç¼“æ…¢ï¼Œç”šè‡³å¯èƒ½è¿‡æ—©é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚å¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œä¼˜åŒ–è¿‡ç¨‹å¯èƒ½ä¼šåœ¨æœ€å°å€¼é™„è¿‘éœ‡è¡ï¼Œæ— æ³•æ”¶æ•›ã€‚æˆ‘ä»¬å°†åœ¨Â§3.6ä¸­çœ‹åˆ°ï¼Œå­¦ä¹ ç‡å¯ä»¥ä¾èµ–äºè¿­ä»£æ¬¡æ•°\( n \)ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å­¦ä¹ ç‡çš„é€‰æ‹©**ï¼šå­¦ä¹ ç‡çš„é€‰æ‹©å¯¹æ¢¯åº¦ä¸‹é™çš„æ•ˆæœè‡³å…³é‡è¦ã€‚è¿‡å°çš„å­¦ä¹ ç‡ä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦è¿‡æ…¢ï¼Œè€Œè¿‡å¤§çš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹ä¸ç¨³å®šï¼Œç”šè‡³æ— æ³•æ”¶æ•›ã€‚
- **å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰**ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå­¦ä¹ ç‡ä¼šéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œé€æ¸å‡å°ï¼Œè¿™æœ‰åŠ©äºåœ¨ä¼˜åŒ–åˆæœŸå¿«é€Ÿæ¥è¿‘æœ€å°å€¼ï¼Œè€Œåœ¨åæœŸç²¾ç»†è°ƒæ•´å‚æ•°ã€‚

#### Stochastic Gradient Descent

All the losses used in practice can be expressed as an average of a loss per small group of samples, or per sample such as:

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‰€æœ‰çš„æŸå¤±å‡½æ•°éƒ½å¯ä»¥è¡¨ç¤ºä¸ºæ¯ä¸ªå°æ ·æœ¬ç»„æˆ–æ¯ä¸ªæ ·æœ¬çš„æŸå¤±çš„å¹³å‡å€¼ï¼Œä¾‹å¦‚ï¼š

\[\mathcal{L}(w) = \frac{1}{N} \sum_{n=1}^{N} \ell_n(w),\]

where \(\ell_n(w) = L(f(x_n; w), y_n)\) for some \(L\), and the gradient is then:

å…¶ä¸­\(\ell_n(w) = L(f(x_n; w), y_n)\)å¯¹äºæŸä¸ª\(L\)ï¼Œæ¢¯åº¦åˆ™ä¸ºï¼š

\[\nabla \mathcal{L}_{|w}(w) = \frac{1}{N} \sum_{n=1}^{N} \nabla \ell_n |w(w)|. \tag{3.2}\]

The resulting gradient descent would compute exactly the sum in Equation 3.2, which is usually computationally heavy, and then update the parameters according to Equation 3.1. However, under reasonable assumptions of exchangeability, for instance, if the samples have been properly shuffled, any partial sum of Equation 3.2 is an unbiased estimator of the full sum, albeit noisy. So, updating the parameters from partial sums corresponds to doing more gradient steps with noisier estimates of the gradient. Due to the redundancy in the data, this happens to be a far more efficient strategy.

ç”±æ­¤äº§ç”Ÿçš„æ¢¯åº¦ä¸‹é™å°†ç²¾ç¡®è®¡ç®—å…¬å¼3.2ä¸­çš„å’Œï¼Œè¿™é€šå¸¸è®¡ç®—é‡å¾ˆå¤§ï¼Œç„¶åæ ¹æ®å…¬å¼3.1æ›´æ–°å‚æ•°ã€‚ç„¶è€Œï¼Œåœ¨åˆç†çš„å¯äº¤æ¢æ€§å‡è®¾ä¸‹ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ ·æœ¬å·²ç»è¢«é€‚å½“æ‰“ä¹±ï¼Œå…¬å¼3.2çš„ä»»ä½•éƒ¨åˆ†å’Œéƒ½æ˜¯å…¨å’Œçš„æ— åä¼°è®¡ï¼Œå°½ç®¡æœ‰å™ªå£°ã€‚å› æ­¤ï¼Œä»éƒ¨åˆ†å’Œæ›´æ–°å‚æ•°ç›¸å½“äºç”¨æ›´å˜ˆæ‚çš„æ¢¯åº¦ä¼°è®¡è¿›è¡Œæ›´å¤šçš„æ¢¯åº¦æ­¥éª¤ã€‚ç”±äºæ•°æ®ä¸­çš„å†—ä½™ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„ç­–ç•¥ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰**ï¼šä¸ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™ä¸åŒï¼ŒSGDæ¯æ¬¡åªä½¿ç”¨ä¸€ä¸ªæˆ–ä¸€å°éƒ¨åˆ†æ ·æœ¬æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†è®¡ç®—é‡ã€‚è™½ç„¶æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦ä¼°è®¡æœ‰å™ªå£°ï¼Œä½†ç”±äºæ•°æ®å†—ä½™ï¼ŒSGDä»ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ”¶æ•›ã€‚
- **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch Gradient Descentï¼‰**ï¼šSGDçš„ä¸€ç§å˜ä½“ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªå°æ‰¹é‡çš„æ ·æœ¬æ¥è®¡ç®—æ¢¯åº¦ï¼Œæ—¢å‡å°‘äº†å™ªå£°ï¼Œåˆä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚

We saw in Â§ 2.1 that processing a batch of samples small enough to fit in the computing deviceâ€™s memory is generally as fast as processing a single one. Hence, the standard approach is to split the full set _Ã˜ into batches, and to update the parameters from the estimate of the gradient computed from each. This is called mini-batch stochastic gradient descent, or stochastic gradient descent (SGD) for short.

æˆ‘ä»¬åœ¨Â§2.1ä¸­çœ‹åˆ°ï¼Œå¤„ç†ä¸€ä¸ªè¶³å¤Ÿå°çš„æ ·æœ¬æ‰¹æ¬¡ä»¥é€‚åº”è®¡ç®—è®¾å¤‡çš„å†…å­˜é€šå¸¸ä¸å¤„ç†å•ä¸ªæ ·æœ¬ä¸€æ ·å¿«ã€‚å› æ­¤ï¼Œæ ‡å‡†çš„æ–¹æ³•æ˜¯å°†æ•´ä¸ªæ•°æ®é›†_Ã˜åˆ†æˆæ‰¹æ¬¡ï¼Œå¹¶æ ¹æ®æ¯ä¸ªæ‰¹æ¬¡è®¡ç®—çš„æ¢¯åº¦ä¼°è®¡æ›´æ–°å‚æ•°ã€‚è¿™è¢«ç§°ä¸ºå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæˆ–ç®€ç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚

It is important to note that this process is extremely gradual, and that the number of minibatches and gradient steps are typically of the order of several million.

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸æ¸è¿›ï¼Œå°æ‰¹é‡å’Œæ¢¯åº¦æ­¥éª¤çš„æ•°é‡é€šå¸¸è¾¾åˆ°æ•°ç™¾ä¸‡æ¬¡ã€‚

As with many algorithms, intuition breaks down in high dimensions, and although it may seem that this procedure would be easily trapped in a local minimum, in reality, due to the number of parameters, the design of the models, and the stochasticity of the data, its efficiency is far greater than one might expect.

ä¸è®¸å¤šç®—æ³•ä¸€æ ·ï¼Œç›´è§‰åœ¨é«˜ç»´æƒ…å†µä¸‹ä¼šå¤±æ•ˆï¼Œå°½ç®¡çœ‹èµ·æ¥è¿™ä¸ªè¿‡ç¨‹å¾ˆå®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œä½†å®é™…ä¸Šï¼Œç”±äºå‚æ•°çš„æ•°é‡ã€æ¨¡å‹çš„è®¾è®¡ä»¥åŠæ•°æ®çš„éšæœºæ€§ï¼Œå…¶æ•ˆç‡è¿œè¿œè¶…å‡ºé¢„æœŸã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **é«˜ç»´ä¼˜åŒ–**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ¢¯åº¦ä¸‹é™çš„è¡¨ç°å¯èƒ½ä¸ä½ç»´ç©ºé—´ä¸­çš„ç›´è§‰ä¸åŒã€‚ç”±äºå‚æ•°æ•°é‡åºå¤§ï¼Œæ¨¡å‹è®¾è®¡å¤æ‚ï¼Œæ¢¯åº¦ä¸‹é™åœ¨é«˜ç»´ç©ºé—´ä¸­ä»ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- **å±€éƒ¨æœ€å°å€¼ä¸å…¨å±€æœ€å°å€¼**ï¼šè™½ç„¶æ¢¯åº¦ä¸‹é™å¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œç”±äºæ¨¡å‹çš„å¤æ‚æ€§å’Œæ•°æ®çš„éšæœºæ€§ï¼Œæ¢¯åº¦ä¸‹é™é€šå¸¸èƒ½å¤Ÿæ‰¾åˆ°è¶³å¤Ÿå¥½çš„è§£ï¼Œå³ä½¿ä¸æ˜¯å…¨å±€æœ€å°å€¼ã€‚

Plenty of variations of this standard strategy have been proposed, and the most popular is Adam [Kingma and Ba, 2014], which keeps running estimates of the mean and variance of each component of the gradient, and normalizes them automatically, avoiding scaling issues and different training speeds in different parts of a model.

å·²ç»æå‡ºäº†è®¸å¤šè¿™ç§æ ‡å‡†ç­–ç•¥çš„å˜ä½“ï¼Œå…¶ä¸­æœ€æµè¡Œçš„æ˜¯Adam [Kingma and Ba, 2014]ï¼Œå®ƒä¿æŒå¯¹æ¢¯åº¦æ¯ä¸ªåˆ†é‡çš„å‡å€¼å’Œæ–¹å·®çš„è¿è¡Œä¼°è®¡ï¼Œå¹¶è‡ªåŠ¨å½’ä¸€åŒ–å®ƒä»¬ï¼Œé¿å…äº†æ¨¡å‹ä¸åŒéƒ¨åˆ†çš„ç¼©æ”¾é—®é¢˜å’Œä¸åŒçš„è®­ç»ƒé€Ÿåº¦ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Adamä¼˜åŒ–å™¨**ï¼šAdamæ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•ï¼Œç»“åˆäº†åŠ¨é‡æ³•å’ŒRMSPropçš„ä¼˜ç‚¹ã€‚å®ƒé€šè¿‡è®¡ç®—æ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆå‡å€¼ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰æ¥è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œä»è€Œåœ¨ä¸åŒå‚æ•°ä¸Šå®ç°æ›´ç¨³å®šçš„è®­ç»ƒã€‚
- **è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼šAdamé€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œé¿å…äº†æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡çš„éº»çƒ¦ï¼Œå¹¶ä¸”åœ¨å¤„ç†ç¨€ç–æ¢¯åº¦æ—¶è¡¨ç°è‰¯å¥½ã€‚

---

### æ€»ç»“ï¼š
- **æ¢¯åº¦ä¸‹é™**æ˜¯æ·±åº¦å­¦ä¹ ä¸­ç”¨äºæœ€å°åŒ–æŸå¤±å‡½æ•°çš„æ ¸å¿ƒä¼˜åŒ–ç®—æ³•ã€‚
- **å­¦ä¹ ç‡**æ§åˆ¶å‚æ•°æ›´æ–°çš„æ­¥é•¿ï¼Œè¿‡å¤§æˆ–è¿‡å°éƒ½ä¼šå½±å“ä¼˜åŒ–æ•ˆæœã€‚
- **éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰**é€šè¿‡ä½¿ç”¨å°æ‰¹é‡æ•°æ®è®¡ç®—æ¢¯åº¦ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚
- **Adamä¼˜åŒ–å™¨**é€šè¿‡è‡ªé€‚åº”å­¦ä¹ ç‡æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

### Chapter 3.4 Backpropagation

#### 3.4 åå‘ä¼ æ’­

Using gradient descent requires a technical means to compute \(\nabla \mathcal{C}_{|w}(w)\) where \(\mathcal{C} = L(f(x;w);y)\). Given that \(f\) and \(L\) are both compositions of standard tensor operations, as for any mathematical expression, the chain rule from differential calculus allows us to get an expression of it.

ä½¿ç”¨æ¢¯åº¦ä¸‹é™éœ€è¦ä¸€ä¸ªæŠ€æœ¯æ‰‹æ®µæ¥è®¡ç®—\(\nabla \mathcal{C}_{|w}(w)\)ï¼Œå…¶ä¸­\(\mathcal{C} = L(f(x;w);y)\)ã€‚ç”±äº\(f\)å’Œ\(L\)éƒ½æ˜¯ç”±æ ‡å‡†çš„å¼ é‡æ“ä½œç»„æˆçš„ï¼Œå°±åƒä»»ä½•æ•°å­¦è¡¨è¾¾å¼ä¸€æ ·ï¼Œå¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™å…è®¸æˆ‘ä»¬å¾—åˆ°å®ƒçš„è¡¨è¾¾å¼ã€‚

For the sake of making notation lighter, we will not specify at which point gradients are computed, since the context makes it clear.

ä¸ºäº†ç®€åŒ–ç¬¦å·ï¼Œæˆ‘ä»¬å°†ä¸æŒ‡å®šåœ¨å“ªä¸ªç‚¹è®¡ç®—æ¢¯åº¦ï¼Œå› ä¸ºä¸Šä¸‹æ–‡å·²ç»æ˜ç¡®äº†ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼šåå‘ä¼ æ’­æ˜¯æ·±åº¦å­¦ä¹ ä¸­ç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦çš„ç®—æ³•ã€‚å®ƒé€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚é€å±‚è®¡ç®—æ¢¯åº¦ã€‚
- **é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰**ï¼šé“¾å¼æ³•åˆ™æ˜¯å¾®ç§¯åˆ†ä¸­çš„ä¸€ä¸ªåŸºæœ¬æ³•åˆ™ï¼Œç”¨äºè®¡ç®—å¤åˆå‡½æ•°çš„å¯¼æ•°ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé“¾å¼æ³•åˆ™ç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¯ä¸€å±‚å‚æ•°çš„æ¢¯åº¦ã€‚

#### Forward and backward passes

Consider the simple case of a composition of mappings:

è€ƒè™‘ä¸€ä¸ªç®€å•çš„æ˜ å°„ç»„åˆæƒ…å†µï¼š

\[ f = f^{(D)} \circ f^{(D-1)} \circ \cdots \circ f^{(1)}. \]

The output of \( f(x; w) \) can be computed by starting with \( x^{(0)} = x \) and applying iteratively:

\( f(x; w) \)çš„è¾“å‡ºå¯ä»¥é€šè¿‡ä»\( x^{(0)} = x \)å¼€å§‹å¹¶è¿­ä»£åº”ç”¨ä»¥ä¸‹å…¬å¼æ¥è®¡ç®—ï¼š

\[ x^{(d)} = f^{(d)} \left( x^{(d-1)}; w_d \right), \]

with \( x^{(D)} \) as the final value.

å…¶ä¸­\( x^{(D)} \)æ˜¯æœ€ç»ˆå€¼ã€‚

The individual scalar values of these intermediate results \( x^{(d)} \) are traditionally called activations in reference to neuron activations, the value \( D \) is the depth of the model, the individual mappings \( f^{(d)} \) are referred to as layers, as we will see in Â§ 4.1, and their sequential evaluation is the forward pass (see Figure 3.3, top).

è¿™äº›ä¸­é—´ç»“æœ\( x^{(d)} \)çš„å„ä¸ªæ ‡é‡å€¼ä¼ ç»Ÿä¸Šè¢«ç§°ä¸ºæ¿€æ´»å€¼ï¼Œå‚è€ƒç¥ç»å…ƒçš„æ¿€æ´»ï¼Œå€¼\( D \)æ˜¯æ¨¡å‹çš„æ·±åº¦ï¼Œå„ä¸ªæ˜ å°„\( f^{(d)} \)è¢«ç§°ä¸ºå±‚ï¼Œæˆ‘ä»¬å°†åœ¨Â§4.1ä¸­çœ‹åˆ°ï¼Œå®ƒä»¬çš„é¡ºåºè¯„ä¼°æ˜¯å‰å‘ä¼ æ’­ï¼ˆè§å›¾3.3ï¼Œé¡¶éƒ¨ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰**ï¼šå‰å‘ä¼ æ’­æ˜¯æŒ‡ä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚é€å±‚è®¡ç®—æ¯ä¸€å±‚çš„æ¿€æ´»å€¼çš„è¿‡ç¨‹ã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹çš„åŸºç¡€ã€‚
- **æ¿€æ´»å€¼ï¼ˆActivationsï¼‰**ï¼šæ¿€æ´»å€¼æ˜¯æ¯ä¸€å±‚ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé€šå¸¸é€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰å¯¹çº¿æ€§å˜æ¢çš„ç»“æœè¿›è¡Œéçº¿æ€§å˜æ¢å¾—åˆ°ã€‚

Conversely, the gradient \( \nabla_{\ell} |_{x^{(d-1)}} \) of the loss with respect to the output \( x^{(d-1)} \) of \( f^{(d-1)} \) is the product of the gradient \( \nabla_{\ell} |_{x^{(d)}} \) with respect to the output of \( f^{(d)} \) multiplied by the Jacobian \( J_f^{(d-1)} |_x \) of \( f^{(d-1)} \) with respect to its variable \( x \). Thus, the gradients with respect to the outputs of all the \( f^{(d)} \)'s can be computed recursively backward, starting with \( \nabla_{\ell} |_{x^{(D)}} = \nabla L |_x \).

ç›¸åï¼ŒæŸå¤±ç›¸å¯¹äº\( f^{(d-1)} \)çš„è¾“å‡º\( x^{(d-1)} \)çš„æ¢¯åº¦\( \nabla_{\ell} |_{x^{(d-1)}} \)æ˜¯æŸå¤±ç›¸å¯¹äº\( f^{(d)} \)çš„è¾“å‡º\( x^{(d)} \)çš„æ¢¯åº¦\( \nabla_{\ell} |_{x^{(d)}} \)ä¸\( f^{(d-1)} \)ç›¸å¯¹äºå…¶å˜é‡\( x \)çš„é›…å¯æ¯”çŸ©é˜µ\( J_f^{(d-1)} |_x \)çš„ä¹˜ç§¯ã€‚å› æ­¤ï¼Œç›¸å¯¹äºæ‰€æœ‰\( f^{(d)} \)çš„è¾“å‡ºçš„æ¢¯åº¦å¯ä»¥ä»\( \nabla_{\ell} |_{x^{(D)}} = \nabla L |_x \)å¼€å§‹é€’å½’åœ°å‘åè®¡ç®—ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **åå‘ä¼ æ’­çš„è®¡ç®—**ï¼šåå‘ä¼ æ’­é€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚é€å±‚è®¡ç®—æ¢¯åº¦ã€‚æ¯ä¸€å±‚çš„æ¢¯åº¦æ˜¯åä¸€å±‚æ¢¯åº¦ä¸è¯¥å±‚æ¿€æ´»å‡½æ•°çš„å¯¼æ•°çš„ä¹˜ç§¯ã€‚
- **é›…å¯æ¯”çŸ©é˜µï¼ˆJacobian Matrixï¼‰**ï¼šé›…å¯æ¯”çŸ©é˜µæ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶å…ƒç´ æ˜¯å¤šå…ƒå‡½æ•°çš„åå¯¼æ•°ã€‚åœ¨åå‘ä¼ æ’­ä¸­ï¼Œé›…å¯æ¯”çŸ©é˜µç”¨äºè¡¨ç¤ºæ¯ä¸€å±‚æ¿€æ´»å‡½æ•°ç›¸å¯¹äºè¾“å…¥çš„å¯¼æ•°ã€‚

And the gradient that we are interested in for training, that is \( \nabla c |_{w_d} \), is the gradient with respect to the output of \( f(d) \) multiplied by the Jacobian \( J_f (d) |_{w} \) of \( f(d) \) with respect to the parameters.

æˆ‘ä»¬è®­ç»ƒä¸­æ„Ÿå…´è¶£çš„æ¢¯åº¦ï¼Œå³\( \nabla c |_{w_d} \)ï¼Œæ˜¯ç›¸å¯¹äº\( f(d) \)çš„è¾“å‡ºçš„æ¢¯åº¦ä¹˜ä»¥\( f(d) \)ç›¸å¯¹äºå‚æ•°çš„é›…å¯æ¯”çŸ©é˜µ\( J_f (d) |_{w} \)ã€‚

This iterative computation of the gradients with respect to the intermediate activations, combined with that of the gradients with respect to the layersâ€™ parameters, is the backward pass (see Figure 3.3, bottom). The combination of this computation with the procedure of gradient descent is called backpropagation.

è¿™ç§ç›¸å¯¹äºä¸­é—´æ¿€æ´»å€¼çš„æ¢¯åº¦çš„è¿­ä»£è®¡ç®—ï¼Œç»“åˆç›¸å¯¹äºå±‚å‚æ•°çš„æ¢¯åº¦çš„è®¡ç®—ï¼Œå°±æ˜¯åå‘ä¼ æ’­ï¼ˆè§å›¾3.3ï¼Œåº•éƒ¨ï¼‰ã€‚è¿™ç§è®¡ç®—ä¸æ¢¯åº¦ä¸‹é™è¿‡ç¨‹çš„ç»“åˆè¢«ç§°ä¸ºåå‘ä¼ æ’­ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **åå‘ä¼ æ’­ä¸æ¢¯åº¦ä¸‹é™çš„ç»“åˆ**ï¼šåå‘ä¼ æ’­ç”¨äºè®¡ç®—æ¢¯åº¦ï¼Œè€Œæ¢¯åº¦ä¸‹é™åˆ™åˆ©ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ä¸¤è€…ç»“åˆæ„æˆäº†æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒè¿‡ç¨‹ã€‚

In practice, the implementation details of the forward and backward passes are hidden from programmers. Deep learning frameworks are able to automatically construct the sequence of operations to compute gradients.

åœ¨å®è·µä¸­ï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å®ç°ç»†èŠ‚å¯¹ç¨‹åºå‘˜æ˜¯éšè—çš„ã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨æ„å»ºè®¡ç®—æ¢¯åº¦çš„æ“ä½œåºåˆ—ã€‚

A particularly convenient algorithm is Autograd [Baydin et al., 2015], which tracks tensor operations and builds, on the fly, the combination of operators for gradients. Thanks to this, a piece of imperative programming that manipulates tensors can automatically compute the gradient of any quantity with respect to any other.

ä¸€ä¸ªç‰¹åˆ«æ–¹ä¾¿çš„ç®—æ³•æ˜¯Autograd [Baydin et al., 2015]ï¼Œå®ƒè·Ÿè¸ªå¼ é‡æ“ä½œå¹¶åŠ¨æ€æ„å»ºæ¢¯åº¦è®¡ç®—çš„ç®—å­ç»„åˆã€‚å¤šäºäº†è¿™ä¸€ç‚¹ï¼Œä¸€æ®µæ“ä½œå¼ é‡çš„å‘½ä»¤å¼ç¼–ç¨‹å¯ä»¥è‡ªåŠ¨è®¡ç®—ä»»ä½•é‡ç›¸å¯¹äºä»»ä½•å…¶ä»–é‡çš„æ¢¯åº¦ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Autograd**ï¼šAutogradæ˜¯ä¸€ç§è‡ªåŠ¨å¾®åˆ†å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è®¡ç®—å¤æ‚å‡½æ•°çš„æ¢¯åº¦ã€‚å®ƒé€šè¿‡è·Ÿè¸ªå¼ é‡æ“ä½œå¹¶åŠ¨æ€æ„å»ºè®¡ç®—å›¾æ¥å®ç°è¿™ä¸€åŠŸèƒ½ã€‚
- **è‡ªåŠ¨å¾®åˆ†ï¼ˆAutomatic Differentiationï¼‰**ï¼šè‡ªåŠ¨å¾®åˆ†æ˜¯ä¸€ç§è®¡ç®—å‡½æ•°å¯¼æ•°çš„æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œç”¨äºè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

#### Resource usage

Regarding the computational cost, as we will see, the bulk of the computation goes into linear operations, each requiring one matrix product for the forward pass and two for the products by the Jacobians for the backward pass, making the latter roughly twice as costly as the former.

å…³äºè®¡ç®—æˆæœ¬ï¼Œæ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œå¤§éƒ¨åˆ†è®¡ç®—éƒ½ç”¨äºçº¿æ€§æ“ä½œï¼Œå‰å‘ä¼ æ’­æ¯ä¸ªæ“ä½œéœ€è¦ä¸€ä¸ªçŸ©é˜µä¹˜ç§¯ï¼Œè€Œåå‘ä¼ æ’­æ¯ä¸ªæ“ä½œéœ€è¦ä¸¤ä¸ªçŸ©é˜µä¹˜ç§¯ï¼ˆä¸€ä¸ªç”¨äºé›…å¯æ¯”çŸ©é˜µï¼‰ï¼Œä½¿å¾—åè€…çš„æˆæœ¬å¤§çº¦æ˜¯å‰è€…çš„ä¸¤å€ã€‚

The memory requirement during inference is roughly equal to that of the most demanding individual layer. For training, however, the backward pass requires keeping the activations computed during the forward pass to compute the Jacobians, which results in a memory usage that grows proportionally to the modelâ€™s depth. Techniques exist to trade the memory usage for computation by either relying on reversible layers [Gomez et al., 2017], or using checkpointing, which consists of storing activations for some layers only and recomputing the others on the fly with partial forward passes during the backward pass [Chen et al., 2016].

æ¨ç†æœŸé—´çš„å†…å­˜éœ€æ±‚å¤§è‡´ç­‰äºæœ€è€—èµ„æºçš„å•ä¸ªå±‚çš„å†…å­˜éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œåå‘ä¼ æ’­éœ€è¦ä¿ç•™å‰å‘ä¼ æ’­æœŸé—´è®¡ç®—çš„æ¿€æ´»å€¼ä»¥è®¡ç®—é›…å¯æ¯”çŸ©é˜µï¼Œè¿™å¯¼è‡´å†…å­˜ä½¿ç”¨é‡éšæ¨¡å‹æ·±åº¦æˆæ¯”ä¾‹å¢é•¿ã€‚å­˜åœ¨ä¸€äº›æŠ€æœ¯å¯ä»¥é€šè¿‡ä¾èµ–å¯é€†å±‚[Gomez et al., 2017]æˆ–ä½¿ç”¨æ£€æŸ¥ç‚¹ï¼ˆcheckpointingï¼‰æ¥åœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œæ£€æŸ¥ç‚¹æŠ€æœ¯åŒ…æ‹¬ä»…å­˜å‚¨æŸäº›å±‚çš„æ¿€æ´»å€¼ï¼Œå¹¶åœ¨åå‘ä¼ æ’­æœŸé—´é€šè¿‡éƒ¨åˆ†å‰å‘ä¼ æ’­é‡æ–°è®¡ç®—å…¶ä»–å±‚çš„æ¿€æ´»å€¼[Chen et al., 2016]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å†…å­˜ä¸è®¡ç®—æƒè¡¡**ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚åå‘ä¼ æ’­éœ€è¦å­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœï¼Œè¿™å¯èƒ½å¯¼è‡´å†…å­˜ä½¿ç”¨é‡éšæ¨¡å‹æ·±åº¦å¢åŠ ã€‚é€šè¿‡ä½¿ç”¨å¯é€†å±‚æˆ–æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚
- **å¯é€†å±‚ï¼ˆReversible Layersï¼‰**ï¼šå¯é€†å±‚æ˜¯ä¸€ç§ç‰¹æ®Šçš„è®¾è®¡ï¼Œå…è®¸åœ¨åå‘ä¼ æ’­æœŸé—´é€šè¿‡å‰å‘ä¼ æ’­é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼Œä»è€Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- **æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ˆCheckpointingï¼‰**ï¼šæ£€æŸ¥ç‚¹æŠ€æœ¯é€šè¿‡å­˜å‚¨éƒ¨åˆ†å±‚çš„æ¿€æ´»å€¼å¹¶åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å…¶ä»–å±‚çš„æ¿€æ´»å€¼ï¼Œæ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

#### Vanishing gradient

A key historical issue when training a large network is that when the gradient propagates backwards through an operator, it may be scaled by a factor smaller than one, resulting in an exponential decrease of its magnitude. This is called the vanishing gradient, and it may make the training impossible, or, in its milder form, cause different parts of the model to be updated at different speeds, degrading their co-adaptation [Glorot and Bengio, 2010].

è®­ç»ƒå¤§å‹ç½‘ç»œæ—¶çš„ä¸€ä¸ªå…³é”®å†å²é—®é¢˜æ˜¯ï¼Œå½“æ¢¯åº¦é€šè¿‡ä¸€ä¸ªæ“ä½œç¬¦å‘åä¼ æ’­æ—¶ï¼Œå®ƒå¯èƒ½ä¼šè¢«ä¸€ä¸ªå°äº1çš„å› å­ç¼©æ”¾ï¼Œå¯¼è‡´å…¶å¹…å€¼å‘ˆæŒ‡æ•°ä¸‹é™ã€‚è¿™è¢«ç§°ä¸ºæ¢¯åº¦æ¶ˆå¤±ï¼Œå®ƒå¯èƒ½ä½¿è®­ç»ƒæ— æ³•è¿›è¡Œï¼Œæˆ–è€…åœ¨å…¶è¾ƒæ¸©å’Œçš„å½¢å¼ä¸­ï¼Œå¯¼è‡´æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ä»¥ä¸åŒçš„é€Ÿåº¦æ›´æ–°ï¼Œä»è€Œé™ä½å®ƒä»¬çš„ååŒé€‚åº”èƒ½åŠ›[Glorot and Bengio, 2010]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing Gradientï¼‰**ï¼šæ¢¯åº¦æ¶ˆå¤±æ˜¯æŒ‡åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å˜å°ï¼Œå¯¼è‡´æ·±å±‚ç½‘ç»œçš„å‚æ•°æ›´æ–°éå¸¸ç¼“æ…¢ç”šè‡³åœæ­¢ã€‚è¿™æ˜¯è®­ç»ƒæ·±å±‚ç½‘ç»œæ—¶çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚
- **ååŒé€‚åº”ï¼ˆCo-adaptationï¼‰**ï¼šååŒé€‚åº”æ˜¯æŒ‡ç½‘ç»œä¸­ä¸åŒéƒ¨åˆ†çš„å‚æ•°ç›¸äº’é€‚åº”ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ¢¯åº¦æ¶ˆå¤±å¯èƒ½å¯¼è‡´ä¸åŒéƒ¨åˆ†çš„å‚æ•°æ›´æ–°é€Ÿåº¦ä¸ä¸€è‡´ï¼Œä»è€Œå½±å“ç½‘ç»œçš„æ•´ä½“æ€§èƒ½ã€‚

---

### æ€»ç»“ï¼š
- **åå‘ä¼ æ’­**æ˜¯æ·±åº¦å­¦ä¹ ä¸­ç”¨äºè®¡ç®—æ¢¯åº¦çš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚é€å±‚è®¡ç®—æ¢¯åº¦ã€‚
- **å‰å‘ä¼ æ’­**ç”¨äºè®¡ç®—æ¯ä¸€å±‚çš„æ¿€æ´»å€¼ï¼Œè€Œåå‘ä¼ æ’­ç”¨äºè®¡ç®—æ¢¯åº¦ã€‚
- **Autograd**ç­‰è‡ªåŠ¨å¾®åˆ†å·¥å…·ä½¿å¾—æ¢¯åº¦è®¡ç®—è‡ªåŠ¨åŒ–ï¼Œç®€åŒ–äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®ç°ã€‚
- **æ¢¯åº¦æ¶ˆå¤±**æ˜¯è®­ç»ƒæ·±å±‚ç½‘ç»œæ—¶çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨é€‚å½“çš„æ¿€æ´»å‡½æ•°å’Œåˆå§‹åŒ–æ–¹æ³•æ¥ç¼“è§£ã€‚

ç”±äºæ–‡ä»¶å†…å®¹è¾ƒé•¿ï¼Œæˆ‘å°†ç»§ç»­ä»Chapter 3.5å¼€å§‹ç¿»è¯‘ï¼Œå¹¶é€æ­¥å®Œæˆæ•´ä¸ªPDFçš„ç¿»è¯‘ã€‚ä»¥ä¸‹æ˜¯Chapter 3.5çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 3.5 The Value of Depth

#### 3.5 æ·±åº¦çš„ä»·å€¼

As the term "deep learning" indicates, useful models are generally compositions of long series of mappings. Training them with gradient descent results in a sophisticated co-adaptation of the mappings, even though this procedure is gradual and local.

æ­£å¦‚â€œæ·±åº¦å­¦ä¹ â€ä¸€è¯æ‰€ç¤ºï¼Œæœ‰ç”¨çš„æ¨¡å‹é€šå¸¸æ˜¯ç”±ä¸€ç³»åˆ—é•¿æ˜ å°„ç»„æˆçš„ã€‚é€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒå®ƒä»¬ä¼šå¯¼è‡´è¿™äº›æ˜ å°„ä¹‹é—´å¤æ‚çš„ååŒé€‚åº”ï¼Œå°½ç®¡è¿™ä¸ªè¿‡ç¨‹æ˜¯æ¸è¿›çš„å’Œå±€éƒ¨çš„ã€‚

We can illustrate this behavior with a simple model \(\mathbb{R}^2 \rightarrow \mathbb{R}^2\) that combines eight layers, each multiplying its input by a \(2 \times 2\) matrix and applying Tanh per component, with a final linear classifier. This is a simplified version of the standard Multi-Layer Perceptron that we will see in Â§ 5.1.

æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹\(\mathbb{R}^2 \rightarrow \mathbb{R}^2\)æ¥è¯´æ˜è¿™ç§è¡Œä¸ºï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å…«å±‚ï¼Œæ¯å±‚å°†å…¶è¾“å…¥ä¹˜ä»¥ä¸€ä¸ª\(2 \times 2\)çŸ©é˜µï¼Œå¹¶å¯¹æ¯ä¸ªåˆ†é‡åº”ç”¨Tanhï¼Œæœ€åæ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨Â§5.1ä¸­å°†è¦çœ‹åˆ°çš„æ ‡å‡†å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„ç®€åŒ–ç‰ˆæœ¬ã€‚

If we train this model with SGD and cross-entropy on a toy binary classification task (Figure 3.4, top left), the matrices co-adapt to deform the space until the classification is correct, which implies that the data have been made linearly separable before the final affine operation (Figure 3.4, bottom right).

å¦‚æœæˆ‘ä»¬åœ¨ä¸€ä¸ªç©å…·äºŒåˆ†ç±»ä»»åŠ¡ä¸Šä½¿ç”¨SGDå’Œäº¤å‰ç†µè®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼ˆå›¾3.4ï¼Œå·¦ä¸Šï¼‰ï¼ŒçŸ©é˜µä¼šååŒé€‚åº”ä»¥å˜å½¢ç©ºé—´ï¼Œç›´åˆ°åˆ†ç±»æ­£ç¡®ï¼Œè¿™æ„å‘³ç€åœ¨æœ€åçš„ä»¿å°„æ“ä½œä¹‹å‰ï¼Œæ•°æ®å·²ç»è¢«çº¿æ€§å¯åˆ†ï¼ˆå›¾3.4ï¼Œå³ä¸‹ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ·±åº¦æ¨¡å‹ï¼ˆDeep Modelsï¼‰**ï¼šæ·±åº¦æ¨¡å‹ç”±å¤šä¸ªå±‚æ¬¡ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½å¯¹è¾“å…¥è¿›è¡Œä¸€å®šçš„å˜æ¢ã€‚é€šè¿‡å¤šå±‚å˜æ¢ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **ååŒé€‚åº”ï¼ˆCo-adaptationï¼‰**ï¼šåœ¨æ·±åº¦æ¨¡å‹ä¸­ï¼Œä¸åŒå±‚çš„å‚æ•°ä¼šç›¸äº’é€‚åº”ï¼Œä»¥å…±åŒä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§ååŒé€‚åº”ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

Such an example gives a glimpse of what a deep model can achieve; however, it is partially misleading due to the low dimension of both the signal to process and the internal representations. Everything is kept in 2D here for the sake of visualization, but in practice, the signal to process is often of very high dimension, and the internal representations are of even higher dimension, which, in particular, facilitates the optimization by providing many degrees of freedom.

è¿™ä¸ªä¾‹å­å±•ç¤ºäº†æ·±åº¦æ¨¡å‹å¯ä»¥å®ç°çš„æ•ˆæœï¼›ç„¶è€Œï¼Œç”±äºä¿¡å·å’Œå†…éƒ¨è¡¨ç¤ºçš„ç»´åº¦è¾ƒä½ï¼Œè¿™ä¸ªä¾‹å­æœ‰äº›è¯¯å¯¼ã€‚è¿™é‡Œä¸ºäº†å¯è§†åŒ–ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¿æŒåœ¨2Dï¼Œä½†åœ¨å®è·µä¸­ï¼Œè¦å¤„ç†çš„ä¿¡å·é€šå¸¸å…·æœ‰éå¸¸é«˜çš„ç»´åº¦ï¼Œè€Œå†…éƒ¨è¡¨ç¤ºçš„ç»´åº¦ç”šè‡³æ›´é«˜ï¼Œè¿™å°¤å…¶é€šè¿‡æä¾›è®¸å¤šè‡ªç”±åº¦æ¥ä¿ƒè¿›ä¼˜åŒ–ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **é«˜ç»´ä¿¡å·ï¼ˆHigh-Dimensional Signalsï¼‰**ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¾“å…¥ä¿¡å·é€šå¸¸æ˜¯é«˜ç»´çš„ï¼Œå¦‚å›¾åƒã€æ–‡æœ¬ç­‰ã€‚æ·±åº¦æ¨¡å‹é€šè¿‡å¤šå±‚å˜æ¢ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è¿™äº›é«˜ç»´ä¿¡å·ã€‚
- **è‡ªç”±åº¦ï¼ˆDegrees of Freedomï¼‰**ï¼šé«˜ç»´å†…éƒ¨è¡¨ç¤ºä¸ºæ¨¡å‹æä¾›äº†æ›´å¤šçš„è‡ªç”±åº¦ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆå¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚

Empirical evidence accumulated over twenty years demonstrates that state-of-the-art performance across application domains necessitates models with tens of layers, such as residual networks (see Â§ 5.2) or Transformers (see Â§ 5.3).

äºŒåå¹´æ¥ç§¯ç´¯çš„ç»éªŒè¯æ®è¡¨æ˜ï¼Œè·¨åº”ç”¨é¢†åŸŸçš„æœ€å…ˆè¿›æ€§èƒ½éœ€è¦å…·æœ‰æ•°åå±‚çš„æ¨¡å‹ï¼Œå¦‚æ®‹å·®ç½‘ç»œï¼ˆè§Â§5.2ï¼‰æˆ–Transformerï¼ˆè§Â§5.3ï¼‰ã€‚

Theoretical results show that, for a fixed computational budget or number of parameters, increasing the depth leads to a greater complexity of the resulting mapping [Telgarsky, 2016].

ç†è®ºç»“æœè¡¨æ˜ï¼Œåœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—æˆ–å‚æ•°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œå¢åŠ æ·±åº¦ä¼šå¯¼è‡´ç”Ÿæˆçš„æ˜ å°„å…·æœ‰æ›´å¤§çš„å¤æ‚æ€§[Telgarsky, 2016]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ·±åº¦ä¸æ¨¡å‹å¤æ‚æ€§ï¼ˆDepth and Model Complexityï¼‰**ï¼šå¢åŠ æ¨¡å‹çš„æ·±åº¦å¯ä»¥å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„æ˜ å°„å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¢åŠ äº†è®­ç»ƒçš„éš¾åº¦ï¼Œå¦‚æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚
- **æ®‹å·®ç½‘ç»œï¼ˆResidual Networksï¼‰**ï¼šæ®‹å·®ç½‘ç»œé€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥ï¼ˆskip connectionsï¼‰æ¥è§£å†³æ·±åº¦æ¨¡å‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚

---

### Chapter 3.6 Training Protocols

#### 3.6 è®­ç»ƒåè®®

Training a deep network requires defining a protocol to make the most of computation and data, and to ensure that performance will be good on new data.

è®­ç»ƒæ·±åº¦ç½‘ç»œéœ€è¦å®šä¹‰ä¸€ä¸ªåè®®ï¼Œä»¥å……åˆ†åˆ©ç”¨è®¡ç®—å’Œæ•°æ®ï¼Œå¹¶ç¡®ä¿åœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½è‰¯å¥½ã€‚

As we saw in Â§ 1.3, the performance on the training samples may be misleading, so in the simplest setup one needs at least two sets of samples: one is a training set, used to optimize the model parameters, and the other is a test set, to evaluate the performance of the trained model.

æ­£å¦‚æˆ‘ä»¬åœ¨Â§1.3ä¸­çœ‹åˆ°çš„ï¼Œè®­ç»ƒæ ·æœ¬ä¸Šçš„æ€§èƒ½å¯èƒ½å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› æ­¤åœ¨æœ€ç®€å•çš„è®¾ç½®ä¸­ï¼Œè‡³å°‘éœ€è¦ä¸¤ç»„æ ·æœ¬ï¼šä¸€ç»„æ˜¯è®­ç»ƒé›†ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œå¦ä¸€ç»„æ˜¯æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°è®­ç»ƒåæ¨¡å‹çš„æ€§èƒ½ã€‚

Additionally, there are usually hyper-parameters to adapt, in particular, those related to the model architecture, the learning rate, and the regularization terms in the loss. In that case, one needs a validation set that is disjoint from both the training and test sets to assess the best configuration.

æ­¤å¤–ï¼Œé€šå¸¸è¿˜éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼Œç‰¹åˆ«æ˜¯ä¸æ¨¡å‹æ¶æ„ã€å­¦ä¹ ç‡å’ŒæŸå¤±ä¸­çš„æ­£åˆ™åŒ–é¡¹ç›¸å…³çš„è¶…å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦ä¸€ä¸ªä¸è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½ä¸é‡å çš„éªŒè¯é›†æ¥è¯„ä¼°æœ€ä½³é…ç½®ã€‚

The full training is usually decomposed into epochs, each of which corresponds to going through all the training examples once. The usual dynamic of the losses is that the training loss decreases as long as the optimization runs, while the validation loss may reach a minimum after a certain number of epochs and then start to increase, reflecting an overfitting regime, as introduced in Â§ 1.3 and illustrated in Figure 3.5.

å®Œæ•´çš„è®­ç»ƒé€šå¸¸è¢«åˆ†è§£ä¸ºå¤šä¸ªepochï¼Œæ¯ä¸ªepochå¯¹åº”äºéå†æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸€æ¬¡ã€‚æŸå¤±çš„é€šå¸¸åŠ¨æ€æ˜¯ï¼Œåªè¦ä¼˜åŒ–è¿è¡Œï¼Œè®­ç»ƒæŸå¤±å°±ä¼šå‡å°‘ï¼Œè€ŒéªŒè¯æŸå¤±å¯èƒ½ä¼šåœ¨ä¸€å®šæ•°é‡çš„epochåè¾¾åˆ°æœ€å°å€¼ï¼Œç„¶åå¼€å§‹å¢åŠ ï¼Œåæ˜ å‡ºè¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œå¦‚Â§1.3ä¸­ä»‹ç»å¹¶åœ¨å›¾3.5ä¸­æ‰€ç¤ºã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆTraining, Validation, and Test Setsï¼‰**ï¼šè®­ç»ƒé›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼ŒéªŒè¯é›†ç”¨äºè°ƒæ•´è¶…å‚æ•°å’Œé€‰æ‹©æ¨¡å‹ï¼Œæµ‹è¯•é›†ç”¨äºæœ€ç»ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
- **è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**ï¼šè¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³çš„ç°è±¡ã€‚é€šè¿‡ç›‘æ§éªŒè¯æŸå¤±ï¼Œå¯ä»¥æ£€æµ‹å’Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

Paradoxically, although they should suffer from severe overfitting due to their capacity, large models usually continue to improve as training progresses. This may be due to the [inductive] bias of the model becoming the main driver of performance, and the optimization process being able to find configurations that generalize well despite the large number of parameters.

çŸ›ç›¾çš„æ˜¯ï¼Œå°½ç®¡ç”±äºå®ƒä»¬çš„å®¹é‡ï¼Œå¤§å‹æ¨¡å‹åº”è¯¥é­å—ä¸¥é‡çš„è¿‡æ‹Ÿåˆï¼Œä½†å®ƒä»¬é€šå¸¸éšç€è®­ç»ƒçš„è¿›è¡Œè€Œç»§ç»­æ”¹è¿›ã€‚è¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹çš„[å½’çº³]åå·®æˆä¸ºæ€§èƒ½çš„ä¸»è¦é©±åŠ¨åŠ›ï¼Œå¹¶ä¸”ä¼˜åŒ–è¿‡ç¨‹èƒ½å¤Ÿæ‰¾åˆ°å°½ç®¡å‚æ•°æ•°é‡ä¼—å¤šä½†ä»èƒ½å¾ˆå¥½æ³›åŒ–çš„é…ç½®ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å½’çº³åå·®ï¼ˆInductive Biasï¼‰**ï¼šå½’çº³åå·®æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æŸäº›å‡è®¾çš„åå¥½ã€‚æ·±åº¦æ¨¡å‹çš„å½’çº³åå·®ä½¿å…¶èƒ½å¤Ÿåœ¨å¤§é‡å‚æ•°çš„æƒ…å†µä¸‹ä»ç„¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **æ³›åŒ–ï¼ˆGeneralizationï¼‰**ï¼šæ³›åŒ–æ˜¯æŒ‡æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°èƒ½åŠ›ã€‚æ·±åº¦æ¨¡å‹é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹èƒ½å¤Ÿæ‰¾åˆ°æ³›åŒ–è‰¯å¥½çš„é…ç½®ã€‚

An important design choice is the learning rate schedule during training, that is, the specification of the value of the learning rate at each iteration of the gradient descent. The general policy is that the learning rate should be initially large to avoid having the optimization being trapped in a bad local minimum early, and that it should get smaller so that the optimized parameter values do not bounce around and reach a good minimum in a narrow valley of the loss landscape.

ä¸€ä¸ªé‡è¦çš„è®¾è®¡é€‰æ‹©æ˜¯è®­ç»ƒæœŸé—´çš„å­¦ä¹ ç‡è°ƒåº¦ï¼Œå³åœ¨æ¢¯åº¦ä¸‹é™çš„æ¯æ¬¡è¿­ä»£ä¸­æŒ‡å®šå­¦ä¹ ç‡çš„å€¼ã€‚ä¸€èˆ¬ç­–ç•¥æ˜¯ï¼Œå­¦ä¹ ç‡æœ€åˆåº”è¯¥è¾ƒå¤§ï¼Œä»¥é¿å…ä¼˜åŒ–è¿‡ç¨‹è¿‡æ—©é™·å…¥ä¸è‰¯çš„å±€éƒ¨æœ€å°å€¼ï¼Œç„¶ååº”è¯¥é€æ¸å‡å°ï¼Œä»¥ä¾¿ä¼˜åŒ–çš„å‚æ•°å€¼ä¸ä¼šåœ¨æŸå¤±å‡½æ•°çš„ç‹­çª„è°·åº•ä¸­éœ‡è¡ï¼Œå¹¶è¾¾åˆ°è‰¯å¥½çš„æœ€å°å€¼ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰**ï¼šå­¦ä¹ ç‡è°ƒåº¦æ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•åŒ…æ‹¬å­¦ä¹ ç‡è¡°å‡å’Œä½™å¼¦é€€ç«ç­‰ã€‚
- **å±€éƒ¨æœ€å°å€¼ï¼ˆLocal Minimaï¼‰**ï¼šå±€éƒ¨æœ€å°å€¼æ˜¯æŒ‡æŸå¤±å‡½æ•°ä¸­çš„ä¸€ä¸ªä½ç‚¹ï¼Œä½†ä¸æ˜¯å…¨å±€æœ€ä½ç‚¹ã€‚é€šè¿‡é€‚å½“çš„å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¯ä»¥é¿å…ä¼˜åŒ–è¿‡ç¨‹é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚

The training of very large models may take months on thousands of powerful GPUs and have a financial cost of several million dollars. At this scale, the training may involve many manual interventions, informed, in particular, by the dynamics of the loss evolution.

éå¸¸å¤§çš„æ¨¡å‹çš„è®­ç»ƒå¯èƒ½éœ€è¦æ•°åƒä¸ªå¼ºå¤§çš„GPUèŠ±è´¹æ•°æœˆæ—¶é—´ï¼Œå¹¶ä¸”è´¢åŠ¡æˆæœ¬å¯èƒ½é«˜è¾¾æ•°ç™¾ä¸‡ç¾å…ƒã€‚åœ¨è¿™ç§è§„æ¨¡ä¸‹ï¼Œè®­ç»ƒå¯èƒ½æ¶‰åŠè®¸å¤šæ‰‹åŠ¨å¹²é¢„ï¼Œç‰¹åˆ«æ˜¯æ ¹æ®æŸå¤±æ¼”å˜çš„åŠ¨æ€è¿›è¡Œå¹²é¢„ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¤§è§„æ¨¡è®­ç»ƒï¼ˆLarge-Scale Trainingï¼‰**ï¼šè®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒè¿‡ç¨‹é€šå¸¸éœ€è¦äººå·¥å¹²é¢„ï¼Œä»¥ç›‘æ§å’Œè°ƒæ•´è®­ç»ƒè¿‡ç¨‹ã€‚

---

### Chapter 3.7 The Benefits of Scale

#### 3.7 è§„æ¨¡çš„å¥½å¤„

There is an accumulation of empirical results showing that performance, for instance, estimated through the loss on test data, improves with the amount of data according to remarkable scaling laws, as long as the model size increases correspondingly [Kaplan et al., 2020] (see Figure 3.6).

æœ‰å¤§é‡çš„ç»éªŒç»“æœè¡¨æ˜ï¼Œåªè¦æ¨¡å‹è§„æ¨¡ç›¸åº”å¢åŠ ï¼Œæ€§èƒ½ï¼ˆä¾‹å¦‚é€šè¿‡æµ‹è¯•æ•°æ®ä¸Šçš„æŸå¤±ä¼°è®¡ï¼‰ä¼šéšç€æ•°æ®é‡çš„å¢åŠ è€Œæé«˜ï¼Œéµå¾ªæ˜¾è‘—çš„ç¼©æ”¾å®šå¾‹[Kaplan et al., 2020]ï¼ˆè§å›¾3.6ï¼‰ã€‚

Benefiting from these scaling laws in the multi-billion sample regime is possible in part thanks to the structure of deep models which can be scaled up arbitrarily, as we will see, by increasing the number of layers or feature dimensions. But it is also made possible by the distributed nature of the computation they implement, and by the stochastic gradient descent, which requires only a fraction of the data at a time and can operate with datasets whose size is orders of magnitude greater than that of the computing deviceâ€™s memory. This has resulted in an exponential growth of the models, as illustrated in Figure 3.7.

åœ¨æ•°åäº¿æ ·æœ¬çš„è§„æ¨¡ä¸‹å—ç›Šäºè¿™äº›ç¼©æ”¾å®šå¾‹ï¼Œéƒ¨åˆ†å½’åŠŸäºæ·±åº¦æ¨¡å‹çš„ç»“æ„ï¼Œè¿™äº›ç»“æ„å¯ä»¥é€šè¿‡å¢åŠ å±‚æ•°æˆ–ç‰¹å¾ç»´åº¦æ¥ä»»æ„æ‰©å±•ã€‚ä½†ä¹Ÿå½’åŠŸäºå®ƒä»¬å®ç°çš„åˆ†å¸ƒå¼è®¡ç®—æ€§è´¨ï¼Œä»¥åŠéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå®ƒæ¯æ¬¡åªéœ€è¦ä¸€éƒ¨åˆ†æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥å¤„ç†æ¯”è®¡ç®—è®¾å¤‡å†…å­˜å¤§å‡ ä¸ªæ•°é‡çº§çš„æ•°æ®é›†ã€‚è¿™å¯¼è‡´äº†æ¨¡å‹çš„æŒ‡æ•°å¢é•¿ï¼Œå¦‚å›¾3.7æ‰€ç¤ºã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **ç¼©æ”¾å®šå¾‹ï¼ˆScaling Lawsï¼‰**ï¼šç¼©æ”¾å®šå¾‹æè¿°äº†æ¨¡å‹æ€§èƒ½å¦‚ä½•éšç€æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæé«˜ã€‚æ·±åº¦æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¢åŠ å±‚æ•°å’Œç‰¹å¾ç»´åº¦æ¥æ‰©å±•ï¼Œä»è€Œå—ç›Šäºè¿™äº›ç¼©æ”¾å®šå¾‹ã€‚
- **åˆ†å¸ƒå¼è®¡ç®—ï¼ˆDistributed Computingï¼‰**ï¼šæ·±åº¦æ¨¡å‹çš„è®­ç»ƒé€šå¸¸éœ€è¦åˆ†å¸ƒå¼è®¡ç®—ï¼Œä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹å‚æ•°ã€‚

Typical vision models have 10â€“100 million trainable parameters and require \( 10^{18} \sim 10^{19} \) FLOPs for training [He et al., 2015; Sevilla et al., 2022]. Language models have from 100 million to hundreds of billions of trainable parameters and require \( 10^{20}-10^{23} \) FLOPs for training [Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022; Sevilla et al., 2022]. These latter models require machines with multiple high-end GPUs.

å…¸å‹çš„è§†è§‰æ¨¡å‹å…·æœ‰10åˆ°1äº¿ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå¹¶ä¸”éœ€è¦\( 10^{18} \sim 10^{19} \)æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰è¿›è¡Œè®­ç»ƒ[He et al., 2015; Sevilla et al., 2022]ã€‚è¯­è¨€æ¨¡å‹å…·æœ‰ä»1äº¿åˆ°æ•°åƒäº¿ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå¹¶ä¸”éœ€è¦\( 10^{20}-10^{23} \)æ¬¡æµ®ç‚¹è¿ç®—è¿›è¡Œè®­ç»ƒ[Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022; Sevilla et al., 2022]ã€‚è¿™äº›æ¨¡å‹éœ€è¦é…å¤‡å¤šä¸ªé«˜ç«¯GPUçš„æœºå™¨ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è®¡ç®—éœ€æ±‚ï¼ˆComputational Requirementsï¼‰**ï¼šè®­ç»ƒå¤§è§„æ¨¡æ·±åº¦æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯è¯­è¨€æ¨¡å‹ï¼Œå…¶å‚æ•°æ•°é‡å’Œè®¡ç®—éœ€æ±‚éƒ½éå¸¸åºå¤§ã€‚
- **GPUåŠ é€Ÿï¼ˆGPU Accelerationï¼‰**ï¼šGPUç”±äºå…¶å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œè¢«å¹¿æ³›ç”¨äºåŠ é€Ÿæ·±åº¦æ¨¡å‹çš„è®­ç»ƒã€‚

Training these large models is impossible using datasets with a detailed ground-truth costly to produce, which can only be of moderate size. Instead, it is done with datasets automatically produced by combining data available on the internet with minimal curation, if any. These sets may combine multiple modalities, such as text and images from web pages, or sound and images from videos, which can be used for large-scale supervised training.

ä½¿ç”¨éœ€è¦é«˜æˆæœ¬ç”Ÿæˆçš„è¯¦ç»†æ ‡æ³¨æ•°æ®é›†æ¥è®­ç»ƒè¿™äº›å¤§å‹æ¨¡å‹æ˜¯ä¸å¯èƒ½çš„ï¼Œè¿™äº›æ•°æ®é›†åªèƒ½å…·æœ‰ä¸­ç­‰è§„æ¨¡ã€‚ç›¸åï¼Œè®­ç»ƒæ˜¯é€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®é›†å®Œæˆçš„ï¼Œè¿™äº›æ•°æ®é›†é€šè¿‡ç»„åˆäº’è”ç½‘ä¸Šå¯ç”¨çš„æ•°æ®å¹¶å°½å¯èƒ½å°‘åœ°è¿›è¡Œæ•´ç†ã€‚è¿™äº›æ•°æ®é›†å¯èƒ½ç»“åˆå¤šç§æ¨¡æ€ï¼Œä¾‹å¦‚æ¥è‡ªç½‘é¡µçš„æ–‡æœ¬å’Œå›¾åƒï¼Œæˆ–æ¥è‡ªè§†é¢‘çš„å£°éŸ³å’Œå›¾åƒï¼Œè¿™äº›å¯ä»¥ç”¨äºå¤§è§„æ¨¡ç›‘ç£è®­ç»ƒã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆLarge-Scale Datasetsï¼‰**ï¼šè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸é€šè¿‡è‡ªåŠ¨æ”¶é›†å’Œæ•´ç†äº’è”ç½‘ä¸Šçš„æ•°æ®ç”Ÿæˆã€‚
- **å¤šæ¨¡æ€æ•°æ®ï¼ˆMultimodal Dataï¼‰**ï¼šå¤šæ¨¡æ€æ•°æ®æ˜¯æŒ‡åŒ…å«å¤šç§ç±»å‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€å£°éŸ³ç­‰ï¼‰çš„æ•°æ®é›†ã€‚è¿™äº›æ•°æ®å¯ä»¥ç”¨äºè®­ç»ƒèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥ç±»å‹çš„æ¨¡å‹ã€‚

As of 2024, the most powerful models are the ones with the largest number of parameters, such as GPT-4 [Brown et al., 2020] and PaLM [Chowdhery et al., 2022], which we will see in Â§ 5.3 and Â§ 7.1, trained on extremely large text datasets (see Table 3.1).

æˆªè‡³2024å¹´ï¼Œæœ€å¼ºå¤§çš„æ¨¡å‹æ˜¯å…·æœ‰æœ€å¤šå‚æ•°çš„æ¨¡å‹ï¼Œä¾‹å¦‚GPT-4 [Brown et al., 2020]å’ŒPaLM [Chowdhery et al., 2022]ï¼Œæˆ‘ä»¬å°†åœ¨Â§5.3å’ŒÂ§7.1ä¸­çœ‹åˆ°ï¼Œè¿™äº›æ¨¡å‹æ˜¯åœ¨æå¤§çš„æ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼ˆè§è¡¨3.1ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLarge-Scale Language Modelsï¼‰**ï¼šå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¦‚GPT-4å’ŒPaLMå…·æœ‰æ•°åäº¿ç”šè‡³æ•°åƒäº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚
- **æ–‡æœ¬æ•°æ®é›†ï¼ˆText Datasetsï¼‰**ï¼šè¿™äº›æ¨¡å‹é€šå¸¸åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†åŒ…å«æ¥è‡ªäº’è”ç½‘çš„å¤§é‡æ–‡æœ¬æ•°æ®ã€‚

---

### æ€»ç»“ï¼š
- **æ·±åº¦æ¨¡å‹**é€šè¿‡å¤šå±‚å˜æ¢èƒ½å¤Ÿå­¦ä¹ åˆ°å¤æ‚çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¢åŠ æ·±åº¦å¯ä»¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- **è®­ç»ƒåè®®**åŒ…æ‹¬ä½¿ç”¨è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†æ¥ä¼˜åŒ–æ¨¡å‹å’Œè¯„ä¼°æ€§èƒ½ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å¤§è§„æ¨¡è®­ç»ƒ**éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œé€šå¸¸éœ€è¦åˆ†å¸ƒå¼è®¡ç®—å’ŒGPUåŠ é€Ÿã€‚
- **ç¼©æ”¾å®šå¾‹**æè¿°äº†æ¨¡å‹æ€§èƒ½å¦‚ä½•éšç€æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæé«˜ï¼Œæ·±åº¦æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¢åŠ å±‚æ•°å’Œç‰¹å¾ç»´åº¦æ¥æ‰©å±•ã€‚



å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 4çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 4 Model Components

#### 4 æ¨¡å‹ç»„ä»¶

A deep model is nothing more than a complex tensorial computation that can ultimately be decomposed into standard mathematical operations from linear algebra and analysis. Over the years, the field has developed a large collection of high-level modules with a clear semantic, and complex models combining these modules, which have proven to be effective in specific application domains.

æ·±åº¦æ¨¡å‹ä¸è¿‡æ˜¯ä¸€ä¸ªå¤æ‚çš„å¼ é‡è®¡ç®—ï¼Œæœ€ç»ˆå¯ä»¥åˆ†è§£ä¸ºçº¿æ€§ä»£æ•°å’Œåˆ†æä¸­çš„æ ‡å‡†æ•°å­¦æ“ä½œã€‚å¤šå¹´æ¥ï¼Œè¯¥é¢†åŸŸå·²ç»å¼€å‘äº†å¤§é‡å…·æœ‰æ˜ç¡®è¯­ä¹‰çš„é«˜çº§æ¨¡å—ï¼Œä»¥åŠç»“åˆè¿™äº›æ¨¡å—çš„å¤æ‚æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç‰¹å®šåº”ç”¨é¢†åŸŸä¸­è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚

Empirical evidence and theoretical results show that greater performance is achieved with deeper architectures, that is, long compositions of mappings. As we saw in section Â§ 3.4, training such a model is challenging due to the vanishing gradient, and multiple important technical contributions have mitigated this issue.

ç»éªŒè¯æ®å’Œç†è®ºç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ›´æ·±çš„æ¶æ„ï¼ˆå³é•¿æ˜ å°„ç»„åˆï¼‰å¯ä»¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨Â§3.4ä¸­çœ‹åˆ°çš„ï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹ç”±äºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜è€Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¤šä¸ªé‡è¦çš„æŠ€æœ¯è´¡çŒ®å·²ç»ç¼“è§£äº†è¿™ä¸ªé—®é¢˜ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ·±åº¦æ¨¡å‹çš„ç»“æ„**ï¼šæ·±åº¦æ¨¡å‹ç”±å¤šä¸ªå±‚æ¬¡ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½å¯¹è¾“å…¥è¿›è¡Œä¸€å®šçš„å˜æ¢ã€‚é€šè¿‡å¤šå±‚å˜æ¢ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆVanishing Gradient Problemï¼‰**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å˜å°ï¼Œå¯¼è‡´æ·±å±‚ç½‘ç»œçš„å‚æ•°æ›´æ–°éå¸¸ç¼“æ…¢ç”šè‡³åœæ­¢ã€‚é€šè¿‡ä½¿ç”¨é€‚å½“çš„æ¿€æ´»å‡½æ•°å’Œåˆå§‹åŒ–æ–¹æ³•ï¼Œå¯ä»¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

---

### Chapter 4.1 The Notion of Layer

#### 4.1 å±‚çš„æ¦‚å¿µ

We call layers standard complex compounded tensor operations that have been designed and empirically identified as being generic and efficient. They often incorporate trainable parameters and correspond to a convenient level of granularity for designing and describing large deep models. The term is inherited from simple multi-layer neural networks, even though modern models may take the form of a complex graph of such modules, incorporating multiple parallel pathways.

æˆ‘ä»¬å°†å±‚ç§°ä¸ºæ ‡å‡†å¤æ‚çš„å¤åˆå¼ é‡æ“ä½œï¼Œè¿™äº›æ“ä½œç»è¿‡è®¾è®¡å¹¶é€šè¿‡ç»éªŒéªŒè¯ä¸ºé€šç”¨ä¸”é«˜æ•ˆã€‚å®ƒä»¬é€šå¸¸åŒ…å«å¯è®­ç»ƒå‚æ•°ï¼Œå¹¶å¯¹åº”äºè®¾è®¡å’Œæè¿°å¤§å‹æ·±åº¦æ¨¡å‹çš„æ–¹ä¾¿ç²’åº¦çº§åˆ«ã€‚è¿™ä¸ªæœ¯è¯­ç»§æ‰¿è‡ªç®€å•çš„å¤šå±‚ç¥ç»ç½‘ç»œï¼Œå°½ç®¡ç°ä»£æ¨¡å‹å¯èƒ½é‡‡ç”¨è¿™ç§æ¨¡å—çš„å¤æ‚å›¾å½¢å¼ï¼ŒåŒ…å«å¤šä¸ªå¹¶è¡Œè·¯å¾„ã€‚

In the following pages, I try to stick to the convention for model depiction illustrated above:

åœ¨æ¥ä¸‹æ¥çš„å‡ é¡µä¸­ï¼Œæˆ‘å°½é‡éµå¾ªä¸Šè¿°æ¨¡å‹æè¿°çš„çº¦å®šï¼š

- operators / layers are depicted as boxes,
- darker coloring indicates that they embed trainable parameters,
- non-default valued hyper-parameters are specified in the box,
- a dashed outer frame with a multiplicative factor indicates that a group of layers is replicated in series, each with its own set of trainable parameters, if any, and
- in some cases, the dimension of their output is specified on the right when it differs from their input.

- æ“ä½œç¬¦/å±‚è¢«æç»˜ä¸ºæ–¹æ¡†ï¼Œ
- è¾ƒæ·±çš„é¢œè‰²è¡¨ç¤ºå®ƒä»¬åŒ…å«å¯è®­ç»ƒå‚æ•°ï¼Œ
- éé»˜è®¤å€¼çš„è¶…å‚æ•°åœ¨æ–¹æ¡†ä¸­æŒ‡å®šï¼Œ
- å¸¦æœ‰ä¹˜æ³•å› å­çš„è™šçº¿å¤–æ¡†è¡¨ç¤ºä¸€ç»„å±‚è¢«ä¸²è”å¤åˆ¶ï¼Œæ¯ç»„å±‚éƒ½æœ‰è‡ªå·±çš„å¯è®­ç»ƒå‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œ
- åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå½“è¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¸åŒæ—¶ï¼Œè¾“å‡ºç»´åº¦åœ¨å³ä¾§æŒ‡å®šã€‚

Additionally, layers that have a complex internal structure are depicted with a greater height.

æ­¤å¤–ï¼Œå…·æœ‰å¤æ‚å†…éƒ¨ç»“æ„çš„å±‚ä»¥æ›´å¤§çš„é«˜åº¦æç»˜ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å±‚çš„æ¦‚å¿µ**ï¼šå±‚æ˜¯æ·±åº¦æ¨¡å‹ä¸­çš„åŸºæœ¬æ„å»ºå—ï¼Œæ¯ä¸ªå±‚éƒ½å¯¹è¾“å…¥è¿›è¡Œä¸€å®šçš„å˜æ¢ã€‚å±‚å¯ä»¥åŒ…å«å¯è®­ç»ƒå‚æ•°ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç»„åˆå¤šä¸ªå±‚æ¥æ„å»ºå¤æ‚çš„æ¨¡å‹ã€‚
- **æ¨¡å‹æè¿°çº¦å®š**ï¼šä¸ºäº†ç®€åŒ–æ¨¡å‹æè¿°ï¼Œé€šå¸¸ä½¿ç”¨æ–¹æ¡†è¡¨ç¤ºå±‚ï¼Œè¾ƒæ·±çš„é¢œè‰²è¡¨ç¤ºåŒ…å«å¯è®­ç»ƒå‚æ•°ï¼Œè™šçº¿å¤–æ¡†è¡¨ç¤ºå±‚çš„å¤åˆ¶ã€‚

---

### Chapter 4.2 Linear Layers

#### 4.2 çº¿æ€§å±‚

The most important modules in terms of computation and number of parameters are the linear layers. They benefit from decades of research and engineering in algorithmic and chip design for matrix operations.

åœ¨è®¡ç®—å’Œå‚æ•°æ•°é‡æ–¹é¢ï¼Œæœ€é‡è¦çš„æ¨¡å—æ˜¯çº¿æ€§å±‚ã€‚å®ƒä»¬å—ç›Šäºæ•°åå¹´æ¥åœ¨çŸ©é˜µæ“ä½œçš„ç®—æ³•å’ŒèŠ¯ç‰‡è®¾è®¡æ–¹é¢çš„ç ”ç©¶å’Œå·¥ç¨‹ã€‚

Note that the term "linear" in deep learning generally refers improperly to an affine operation, which is the sum of a linear expression and a constant bias.

è¯·æ³¨æ„ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„â€œçº¿æ€§â€ä¸€è¯é€šå¸¸ä¸æ­£ç¡®åœ°æŒ‡ä»£ä»¿å°„æ“ä½œï¼Œå³çº¿æ€§è¡¨è¾¾å¼ä¸å¸¸æ•°åç½®çš„å’Œã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **çº¿æ€§å±‚ï¼ˆLinear Layersï¼‰**ï¼šçº¿æ€§å±‚æ˜¯æ·±åº¦æ¨¡å‹ä¸­çš„åŸºæœ¬æ„å»ºå—ï¼Œé€šå¸¸æŒ‡ä»¿å°„å˜æ¢ï¼ˆçº¿æ€§å˜æ¢åŠ ä¸Šåç½®ï¼‰ã€‚çº¿æ€§å±‚åœ¨è®¡ç®—ä¸Šéå¸¸é«˜æ•ˆï¼Œå¹¶ä¸”åœ¨æ¨¡å‹å‚æ•°ä¸­å æ®ä¸»è¦éƒ¨åˆ†ã€‚

#### Fully connected layers

The most basic linear layer is the fully connected layer, parameterized by a trainable weight matrix \( W \) of size \( D' \times D \) and bias vector \( b \) of dimension \( D' \). It implements an affine transformation generalized to arbitrary tensor shapes, where the supplementary dimensions are interpreted as vector indexes. Formally, given an input \( X \) of dimension \( D_1 \times \cdots \times D_K \times D \), it computes an output \( Y \) of dimension \( D_1 \times \cdots \times D_K \times D' \) with

æœ€åŸºæœ¬çš„çº¿æ€§å±‚æ˜¯å…¨è¿æ¥å±‚ï¼Œç”±å¤§å°ä¸º\( D' \times D \)çš„å¯è®­ç»ƒæƒé‡çŸ©é˜µ\( W \)å’Œç»´åº¦ä¸º\( D' \)çš„åç½®å‘é‡\( b \)å‚æ•°åŒ–ã€‚å®ƒå®ç°äº†æ¨å¹¿åˆ°ä»»æ„å¼ é‡å½¢çŠ¶çš„ä»¿å°„å˜æ¢ï¼Œå…¶ä¸­é™„åŠ ç»´åº¦è¢«è§£é‡Šä¸ºå‘é‡ç´¢å¼•ã€‚å½¢å¼ä¸Šï¼Œç»™å®šç»´åº¦ä¸º\( D_1 \times \cdots \times D_K \times D \)çš„è¾“å…¥\( X \)ï¼Œå®ƒè®¡ç®—ç»´åº¦ä¸º\( D_1 \times \cdots \times D_K \times D' \)çš„è¾“å‡º\( Y \)ï¼Œå…¶ä¸­

\[\forall d_1, \ldots, d_K,\]
\[Y[d_1, \ldots, d_K] = WX[d_1, \ldots, d_K] + b.\]

While at first sight such an affine operation may seem limited to simple transformations such as rotations, symmetries, and translations, it can in fact do more than that. In particular, projections for dimension reduction or signal filtering, but also, from the perspective of the dot product being a measure of similarity, a matrix-vector product can be interpreted as computing matching scores between the queries, as encoded by the input vectors, and keys, as encoded by the matrix rows.

è™½ç„¶ä¹ä¸€çœ‹è¿™ç§ä»¿å°„æ“ä½œä¼¼ä¹ä»…é™äºç®€å•çš„å˜æ¢ï¼Œå¦‚æ—‹è½¬ã€å¯¹ç§°å’Œå¹³ç§»ï¼Œä½†å®é™…ä¸Šå®ƒå¯ä»¥åšå¾—æ›´å¤šã€‚ç‰¹åˆ«æ˜¯ï¼Œç”¨äºé™ç»´æˆ–ä¿¡å·æ»¤æ³¢çš„æŠ•å½±ï¼Œè€Œä¸”ä»ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼æ€§åº¦é‡çš„è§’åº¦æ¥çœ‹ï¼ŒçŸ©é˜µ-å‘é‡ä¹˜ç§¯å¯ä»¥è§£é‡Šä¸ºè®¡ç®—æŸ¥è¯¢ï¼ˆç”±è¾“å…¥å‘é‡ç¼–ç ï¼‰å’Œé”®ï¼ˆç”±çŸ©é˜µè¡Œç¼–ç ï¼‰ä¹‹é—´çš„åŒ¹é…åˆ†æ•°ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å…¨è¿æ¥å±‚ï¼ˆFully Connected Layersï¼‰**ï¼šå…¨è¿æ¥å±‚æ˜¯çº¿æ€§å±‚çš„ä¸€ç§ï¼Œå®ƒå¯¹è¾“å…¥è¿›è¡Œä»¿å°„å˜æ¢ï¼ˆçº¿æ€§å˜æ¢åŠ ä¸Šåç½®ï¼‰ã€‚å…¨è¿æ¥å±‚å¯ä»¥ç”¨äºé™ç»´ã€ä¿¡å·æ»¤æ³¢ç­‰ä»»åŠ¡ã€‚
- **ç‚¹ç§¯ï¼ˆDot Productï¼‰**ï¼šç‚¹ç§¯æ˜¯è¡¡é‡ä¸¤ä¸ªå‘é‡ç›¸ä¼¼æ€§çš„ä¸€ç§æ–¹æ³•ã€‚åœ¨å…¨è¿æ¥å±‚ä¸­ï¼ŒçŸ©é˜µ-å‘é‡ä¹˜ç§¯å¯ä»¥è§£é‡Šä¸ºè®¡ç®—æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„åŒ¹é…åˆ†æ•°ã€‚

As we saw in Â§ 3.3, the gradient descent starts with the parametersâ€™ random initialization. If this is done too naively, as seen in Â§ 3.4, the network may suffer from exploding or vanishing activations and gradients [Glorot and Bengio, 2010]. Deep learning frameworks implement initialization methods that in particular scale the random parameters according to the dimension of the input to keep the variance of the activations constant and prevent pathological behaviors.

æ­£å¦‚æˆ‘ä»¬åœ¨Â§3.3ä¸­çœ‹åˆ°çš„ï¼Œæ¢¯åº¦ä¸‹é™ä»å‚æ•°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ã€‚å¦‚æœè¿™ä¸ªè¿‡ç¨‹è¿‡äºç®€å•ï¼Œå¦‚Â§3.4ä¸­æ‰€ç¤ºï¼Œç½‘ç»œå¯èƒ½ä¼šé­å—æ¿€æ´»å€¼å’Œæ¢¯åº¦çš„çˆ†ç‚¸æˆ–æ¶ˆå¤±é—®é¢˜[Glorot and Bengio, 2010]ã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°äº†åˆå§‹åŒ–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ ¹æ®è¾“å…¥çš„ç»´åº¦ç¼©æ”¾éšæœºå‚æ•°ï¼Œä»¥ä¿æŒæ¿€æ´»å€¼çš„æ–¹å·®æ’å®šå¹¶é˜²æ­¢ç—…æ€è¡Œä¸ºã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å‚æ•°åˆå§‹åŒ–ï¼ˆParameter Initializationï¼‰**ï¼šå‚æ•°åˆå§‹åŒ–æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æ­¥éª¤ã€‚é€‚å½“çš„åˆå§‹åŒ–æ–¹æ³•å¯ä»¥é˜²æ­¢æ¿€æ´»å€¼å’Œæ¢¯åº¦çš„çˆ†ç‚¸æˆ–æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§ã€‚
- **æ¿€æ´»å€¼çš„æ–¹å·®ï¼ˆVariance of Activationsï¼‰**ï¼šåœ¨æ·±åº¦ç½‘ç»œä¸­ï¼Œä¿æŒæ¿€æ´»å€¼çš„æ–¹å·®æ’å®šæœ‰åŠ©äºé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜ã€‚é€šè¿‡é€‚å½“çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯ä»¥ç¡®ä¿æ¿€æ´»å€¼çš„æ–¹å·®åœ¨ç½‘ç»œçš„æ¯ä¸€å±‚éƒ½ä¿æŒç¨³å®šã€‚

---

### Chapter 4.3 Activation Functions

#### 4.3 æ¿€æ´»å‡½æ•°

If a network were combining only linear components, it would itself be a linear operator, so it is essential to have non-linear operations. These are implemented in particular with activation functions, which are layers that transform each component of the input tensor individually through a mapping, resulting in a tensor of the same shape.

å¦‚æœä¸€ä¸ªç½‘ç»œä»…ç»„åˆçº¿æ€§ç»„ä»¶ï¼Œé‚£ä¹ˆå®ƒæœ¬èº«å°†æ˜¯ä¸€ä¸ªçº¿æ€§æ“ä½œç¬¦ï¼Œå› æ­¤å¿…é¡»å¼•å…¥éçº¿æ€§æ“ä½œã€‚è¿™äº›æ“ä½œç‰¹åˆ«æ˜¯é€šè¿‡æ¿€æ´»å‡½æ•°å®ç°çš„ï¼Œæ¿€æ´»å‡½æ•°æ˜¯é€šè¿‡æ˜ å°„é€ä¸ªå˜æ¢è¾“å…¥å¼ é‡çš„æ¯ä¸ªåˆ†é‡çš„å±‚ï¼Œç”Ÿæˆç›¸åŒå½¢çŠ¶çš„å¼ é‡ã€‚

There are many different activation functions, but the most used is the \textit{Rectified Linear Unit (ReLU)} [Glorot et al., 2011], which sets negative values to zero and keeps positive values unchanged (see Figure 4.5, top right):

æœ‰è®¸å¤šä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œä½†æœ€å¸¸ç”¨çš„æ˜¯\textit{ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰} [Glorot et al., 2011]ï¼Œå®ƒå°†è´Ÿå€¼è®¾ä¸ºé›¶å¹¶ä¿æŒæ­£å€¼ä¸å˜ï¼ˆè§å›¾4.5ï¼Œå³ä¸Šï¼‰ï¼š

\[\text{relu}(x) = 
\begin{cases} 
0 & \text{if } x < 0, \\ 
x & \text{otherwise}. 
\end{cases}\]

Given that the core training strategy of deep-learning relies on the gradient, it may seem problematic to have a mapping that is not differentiable at zero and constant on half the real line. However, the main property gradient descent requires is that the gradient is informative on average. Parameter initialization and data normalization make half of the activations positive when the training starts, ensuring that this is the case.

é‰´äºæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒè®­ç»ƒç­–ç•¥ä¾èµ–äºæ¢¯åº¦ï¼Œä½¿ç”¨åœ¨é›¶ç‚¹ä¸å¯å¯¼ä¸”åœ¨ä¸€åŠå®æ•°çº¿ä¸Šä¸ºå¸¸æ•°çš„æ˜ å°„å¯èƒ½çœ‹èµ·æ¥æœ‰é—®é¢˜ã€‚ç„¶è€Œï¼Œæ¢¯åº¦ä¸‹é™æ‰€éœ€çš„ä¸»è¦å±æ€§æ˜¯æ¢¯åº¦åœ¨å¹³å‡æƒ…å†µä¸‹æ˜¯æœ‰ä¿¡æ¯çš„ã€‚å‚æ•°åˆå§‹åŒ–å’Œæ•°æ®å½’ä¸€åŒ–ä½¿å¾—è®­ç»ƒå¼€å§‹æ—¶ä¸€åŠçš„æ¿€æ´»å€¼ä¸ºæ­£ï¼Œç¡®ä¿äº†è¿™ä¸€ç‚¹ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionsï¼‰**ï¼šæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚ReLUæ˜¯æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼Œå®ƒå°†è´Ÿè¾“å…¥è®¾ä¸ºé›¶ï¼Œä¿æŒæ­£è¾“å…¥ä¸å˜ã€‚
- **ReLUçš„ä¼˜ç‚¹**ï¼šReLUè®¡ç®—ç®€å•ä¸”åœ¨å®è·µä¸­è¡¨ç°è‰¯å¥½ï¼Œå°½ç®¡åœ¨é›¶ç‚¹ä¸å¯å¯¼ï¼Œä½†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸ä¼šå½±å“è®­ç»ƒã€‚

Before the generalization of ReLU, the standard activation function was the hyperbolic tangent (Tanh, see Figure 4.5, top left) which saturates exponentially fast on both the negative and positive sides, aggravating the vanishing gradient.

åœ¨ReLUæ™®åŠä¹‹å‰ï¼Œæ ‡å‡†çš„æ¿€æ´»å‡½æ•°æ˜¯åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆTanhï¼Œè§å›¾4.5ï¼Œå·¦ä¸Šï¼‰ï¼Œå®ƒåœ¨è´Ÿä¾§å’Œæ­£ä¾§éƒ½å¿«é€Ÿé¥±å’Œï¼ŒåŠ å‰§äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆTanhï¼‰**ï¼šTanhæ¿€æ´»å‡½æ•°å°†è¾“å…¥æ˜ å°„åˆ°[-1, 1]ä¹‹é—´ï¼Œä½†åœ¨è¾“å…¥è¾ƒå¤§æˆ–è¾ƒå°æ—¶ä¼šé¥±å’Œï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

Other popular activation functions follow the same idea of keeping positive values unchanged and squashing the negative values. Leaky ReLU [Maas et al., 2013] applies a small positive multiplier to negative values (see Figure 4.5, bottom left):

å…¶ä»–æµè¡Œçš„æ¿€æ´»å‡½æ•°éµå¾ªç›¸åŒçš„æ€è·¯ï¼Œä¿æŒæ­£å€¼ä¸å˜å¹¶å‹ç¼©è´Ÿå€¼ã€‚Leaky ReLU [Maas et al., 2013]å¯¹è´Ÿå€¼åº”ç”¨ä¸€ä¸ªå°çš„æ­£ä¹˜æ•°ï¼ˆè§å›¾4.5ï¼Œå·¦ä¸‹ï¼‰ï¼š

\[ \text{leaky relu}(x) = \begin{cases} 
ax \text{ if } x < 0, \\ 
x \text{ otherwise.}
\end{cases} \]

And GELU [Hendrycks and Gimpel, 2016] is defined using the cumulative distribution function of the Gaussian distribution, that is:

è€ŒGELU [Hendrycks and Gimpel, 2016]ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å®šä¹‰ï¼Œå³ï¼š

\[ \text{gelu}(x) = xP(Z \leq x), \]

where \( Z \sim \mathcal{N}(0,1) \). It roughly behaves like a smooth ReLU (see Figure 4.5, bottom right).

å…¶ä¸­\( Z \sim \mathcal{N}(0,1) \)ã€‚å®ƒçš„è¡Œä¸ºå¤§è‡´ç±»ä¼¼äºå¹³æ»‘çš„ReLUï¼ˆè§å›¾4.5ï¼Œå³ä¸‹ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Leaky ReLU**ï¼šLeaky ReLUé€šè¿‡å¼•å…¥ä¸€ä¸ªå°çš„æ–œç‡æ¥ç¼“è§£ReLUçš„â€œæ­»äº¡â€é—®é¢˜ï¼ˆå³æŸäº›ç¥ç»å…ƒå¯èƒ½æ°¸è¿œä¸è¢«æ¿€æ´»ï¼‰ã€‚
- **GELU**ï¼šGELUæ˜¯ä¸€ç§å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒåœ¨è¾“å…¥è¾ƒå°æ—¶æ¥è¿‘äºé›¶ï¼Œåœ¨è¾“å…¥è¾ƒå¤§æ—¶æ¥è¿‘äºçº¿æ€§å‡½æ•°ã€‚GELUåœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºReLUã€‚

The choice of an activation function, in particular among the variants of ReLU, is generally driven by empirical performance.

æ¿€æ´»å‡½æ•°çš„é€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯åœ¨ReLUçš„å˜ä½“ä¸­ï¼Œé€šå¸¸ç”±ç»éªŒæ€§èƒ½é©±åŠ¨ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ¿€æ´»å‡½æ•°çš„é€‰æ‹©**ï¼šä¸åŒçš„æ¿€æ´»å‡½æ•°åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸åŒï¼Œé€šå¸¸éœ€è¦é€šè¿‡å®éªŒæ¥é€‰æ‹©æœ€é€‚åˆçš„æ¿€æ´»å‡½æ•°ã€‚

---

### æ€»ç»“ï¼š
- **çº¿æ€§å±‚**æ˜¯æ·±åº¦æ¨¡å‹ä¸­çš„åŸºæœ¬æ„å»ºå—ï¼Œé€šå¸¸æŒ‡ä»¿å°„å˜æ¢ï¼ˆçº¿æ€§å˜æ¢åŠ ä¸Šåç½®ï¼‰ã€‚
- **æ¿€æ´»å‡½æ•°**å¼•å…¥éçº¿æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚ReLUæ˜¯æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼ŒLeaky ReLUå’ŒGELUæ˜¯å…¶å˜ä½“ã€‚
- **å‚æ•°åˆå§‹åŒ–**æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æ­¥éª¤ï¼Œé€‚å½“çš„åˆå§‹åŒ–æ–¹æ³•å¯ä»¥é˜²æ­¢æ¿€æ´»å€¼å’Œæ¢¯åº¦çš„çˆ†ç‚¸æˆ–æ¶ˆå¤±é—®é¢˜ã€‚

è¿™äº›çŸ¥è¯†ç‚¹æ˜¯ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–è¿‡ç¨‹çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬æœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

---


å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 4.4åˆ°Chapter 4.6çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 4.4 Pooling

#### 4.4 æ± åŒ–

A classical strategy to reduce the signal size is to use a pooling operation that combines multiple activations into one that ideally summarizes the information. The most standard operation of this class is the max pooling layer, which, similarly to convolution, can operate in 1D and 2D and is defined by a kernel size.

å‡å°‘ä¿¡å·å¤§å°çš„ç»å…¸ç­–ç•¥æ˜¯ä½¿ç”¨æ± åŒ–æ“ä½œï¼Œå°†å¤šä¸ªæ¿€æ´»å€¼ç»„åˆæˆä¸€ä¸ªç†æƒ³æƒ…å†µä¸‹èƒ½æ¦‚æ‹¬ä¿¡æ¯çš„æ¿€æ´»å€¼ã€‚è¿™ç±»æ“ä½œä¸­æœ€æ ‡å‡†çš„æ˜¯æœ€å¤§æ± åŒ–å±‚ï¼Œå®ƒä¸å·ç§¯ç±»ä¼¼ï¼Œå¯ä»¥åœ¨1Då’Œ2Dä¸Šæ“ä½œï¼Œå¹¶ç”±æ ¸å¤§å°å®šä¹‰ã€‚

In its standard form, this layer computes the maximum activation per channel, over non-overlapping sub-tensors of spatial size equal to the kernel size. These values are stored in a result tensor with the same number of channels as the input, and whose spatial size is divided by the kernel size. As with the convolution, this operator has three hyper-parameters: padding, stride, and dilation, with the stride being equal to the kernel size by default. A smaller stride results in a larger resulting tensor, following the same formula as for convolutions (see Â§ 4.2).

åœ¨å…¶æ ‡å‡†å½¢å¼ä¸­ï¼Œè¯¥å±‚è®¡ç®—æ¯ä¸ªé€šé“çš„æœ€å¤§æ¿€æ´»å€¼ï¼Œè¦†ç›–ç©ºé—´å¤§å°ç­‰äºæ ¸å¤§å°çš„éé‡å å­å¼ é‡ã€‚è¿™äº›å€¼å­˜å‚¨åœ¨ç»“æœå¼ é‡ä¸­ï¼Œç»“æœå¼ é‡å…·æœ‰ä¸è¾“å…¥ç›¸åŒçš„é€šé“æ•°ï¼Œå¹¶ä¸”å…¶ç©ºé—´å¤§å°é™¤ä»¥æ ¸å¤§å°ã€‚ä¸å·ç§¯ä¸€æ ·ï¼Œè¯¥æ“ä½œç¬¦æœ‰ä¸‰ä¸ªè¶…å‚æ•°ï¼šå¡«å……ï¼ˆpaddingï¼‰ã€æ­¥å¹…ï¼ˆstrideï¼‰å’Œæ‰©å¼ ï¼ˆdilationï¼‰ï¼Œé»˜è®¤æƒ…å†µä¸‹æ­¥å¹…ç­‰äºæ ¸å¤§å°ã€‚è¾ƒå°çš„æ­¥å¹…ä¼šå¯¼è‡´è¾ƒå¤§çš„ç»“æœå¼ é‡ï¼Œéµå¾ªä¸å·ç§¯ç›¸åŒçš„å…¬å¼ï¼ˆè§Â§4.2ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ± åŒ–ï¼ˆPoolingï¼‰**ï¼šæ± åŒ–æ“ä½œé€šè¿‡å°†å¤šä¸ªæ¿€æ´»å€¼ç»„åˆæˆä¸€ä¸ªå€¼æ¥å‡å°‘ä¿¡å·çš„å¤§å°ã€‚æœ€å¤§æ± åŒ–æ˜¯æœ€å¸¸ç”¨çš„æ± åŒ–æ“ä½œï¼Œå®ƒé€‰æ‹©æ¯ä¸ªåŒºåŸŸä¸­çš„æœ€å¤§å€¼ã€‚
- **æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰**ï¼šæœ€å¤§æ± åŒ–å±‚é€šè¿‡é€‰æ‹©æ¯ä¸ªåŒºåŸŸä¸­çš„æœ€å¤§å€¼æ¥å‡å°‘ä¿¡å·çš„å¤§å°ï¼ŒåŒæ—¶ä¿ç•™æœ€é‡è¦çš„ç‰¹å¾ã€‚

The max operation can be intuitively interpreted as a logical disjunction, or, when it follows a series of convolutional layers that compute local scores for the presence of parts, as a way of encoding that at least one instance of a part is present. It loses precise location, making it invariant to local deformations.

æœ€å¤§æ“ä½œå¯ä»¥ç›´è§‚åœ°è§£é‡Šä¸ºé€»è¾‘æå–ï¼Œæˆ–è€…å½“å®ƒè·Ÿéšä¸€ç³»åˆ—å·ç§¯å±‚æ—¶ï¼Œè¿™äº›å·ç§¯å±‚è®¡ç®—éƒ¨åˆ†å­˜åœ¨çš„å±€éƒ¨åˆ†æ•°ï¼Œæœ€å¤§æ± åŒ–å¯ä»¥ç¼–ç è‡³å°‘å­˜åœ¨ä¸€ä¸ªéƒ¨åˆ†å®ä¾‹ã€‚å®ƒå¤±å»äº†ç²¾ç¡®çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿å…¶å¯¹å±€éƒ¨å˜å½¢å…·æœ‰ä¸å˜æ€§ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ± åŒ–çš„ä¸å˜æ€§ï¼ˆInvariance of Poolingï¼‰**ï¼šæ± åŒ–æ“ä½œé€šè¿‡é€‰æ‹©åŒºåŸŸä¸­çš„æœ€å¤§å€¼ï¼Œä½¿å¾—æ¨¡å‹å¯¹å±€éƒ¨å˜å½¢å…·æœ‰ä¸å˜æ€§ã€‚è¿™ç§ä¸å˜æ€§æœ‰åŠ©äºæ¨¡å‹åœ¨å¤„ç†å›¾åƒç­‰æ•°æ®æ—¶å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚

A standard alternative is the \underline{average pooling} layer that computes the average instead of the maximum over the sub-tensors. This is a linear operation, whereas max pooling is not.

ä¸€ä¸ªæ ‡å‡†çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯\underline{å¹³å‡æ± åŒ–}å±‚ï¼Œå®ƒè®¡ç®—å­å¼ é‡çš„å¹³å‡å€¼è€Œä¸æ˜¯æœ€å¤§å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªçº¿æ€§æ“ä½œï¼Œè€Œæœ€å¤§æ± åŒ–ä¸æ˜¯ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰**ï¼šå¹³å‡æ± åŒ–å±‚é€šè¿‡è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„å¹³å‡å€¼æ¥å‡å°‘ä¿¡å·çš„å¤§å°ã€‚ä¸æœ€å¤§æ± åŒ–ä¸åŒï¼Œå¹³å‡æ± åŒ–æ˜¯ä¸€ä¸ªçº¿æ€§æ“ä½œï¼Œé€‚ç”¨äºæŸäº›éœ€è¦å¹³æ»‘ç‰¹å¾çš„ä»»åŠ¡ã€‚

---

### Chapter 4.5 Dropout

#### 4.5 Dropout

Some layers have been designed to explicitly facilitate training or improve the learned representations.

ä¸€äº›å±‚è¢«è®¾è®¡ç”¨æ¥æ˜¾å¼åœ°ä¿ƒè¿›è®­ç»ƒæˆ–æ”¹è¿›å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚

One of the main contributions of that sort was dropout [Srivastava et al., 2014]. Such a layer has no trainable parameters, but one hyperparameter, \( p \), and takes as input a tensor of arbitrary shape.

è¿™ç±»å±‚çš„ä¸»è¦è´¡çŒ®ä¹‹ä¸€æ˜¯dropout [Srivastava et al., 2014]ã€‚è¿™æ ·çš„å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œä½†æœ‰ä¸€ä¸ªè¶…å‚æ•°\( p \)ï¼Œå¹¶æ¥å—ä»»æ„å½¢çŠ¶çš„å¼ é‡ä½œä¸ºè¾“å…¥ã€‚

It is usually switched off during testing, in which case its output is equal to its input. When it is active, it has a probability \( p \) of setting to zero each activation of the input tensor independently, and it re-scales all the activations by a factor of \(\frac{1}{1-p}\) to maintain the expected value unchanged (see Figure 4.7).

åœ¨æµ‹è¯•æœŸé—´é€šå¸¸å…³é—­å®ƒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…¶è¾“å‡ºç­‰äºè¾“å…¥ã€‚å½“å®ƒæ¿€æ´»æ—¶ï¼Œå®ƒä»¥æ¦‚ç‡\( p \)ç‹¬ç«‹åœ°å°†è¾“å…¥å¼ é‡çš„æ¯ä¸ªæ¿€æ´»å€¼è®¾ä¸ºé›¶ï¼Œå¹¶é€šè¿‡ä¹˜ä»¥\(\frac{1}{1-p}\)æ¥é‡æ–°ç¼©æ”¾æ‰€æœ‰æ¿€æ´»å€¼ï¼Œä»¥ä¿æŒæœŸæœ›å€¼ä¸å˜ï¼ˆè§å›¾4.7ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Dropout**ï¼šDropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒæ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒDropoutå±‚è¢«å…³é—­ï¼Œæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸è®¡ç®—ã€‚
- **Dropoutçš„ä½œç”¨**ï¼šDropouté€šè¿‡éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œä½¿å¾—æ¨¡å‹ä¸èƒ½ä¾èµ–äºæŸäº›ç‰¹å®šçš„ç¥ç»å…ƒï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

The motivation behind dropout is to favor meaningful individual activation and discourage group representation. Since the probability that a group of \( k \) activations remains intact through a dropout layer is \((1 - p)^k\), joint representations become unreliable, making the training procedure avoid them. It can also be seen as a noise injection that makes the training more robust.

Dropoutçš„åŠ¨æœºæ˜¯é¼“åŠ±æœ‰æ„ä¹‰çš„ä¸ªä½“æ¿€æ´»å¹¶æŠ‘åˆ¶ç¾¤ä½“è¡¨ç¤ºã€‚ç”±äºä¸€ç»„\( k \)ä¸ªæ¿€æ´»å€¼é€šè¿‡Dropoutå±‚ä¿æŒå®Œæ•´çš„æ¦‚ç‡æ˜¯\((1 - p)^k\)ï¼Œè”åˆè¡¨ç¤ºå˜å¾—ä¸å¯é ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹é¿å…å®ƒä»¬ã€‚å®ƒä¹Ÿå¯ä»¥è¢«è§†ä¸ºä¸€ç§å™ªå£°æ³¨å…¥ï¼Œä½¿è®­ç»ƒæ›´åŠ é²æ£’ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Dropoutçš„åŠ¨æœº**ï¼šDropouté€šè¿‡éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ é²æ£’çš„ç‰¹å¾è¡¨ç¤ºï¼Œé¿å…è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šçš„ç¥ç»å…ƒç»„åˆã€‚

When dealing with images and 2D tensors, the short-term correlation of the signals and the resulting redundancy negate the effect of dropout, since activations set to zero can be inferred from their neighbors. Hence, dropout for \( 2D \) tensors sets entire channels to zero instead of individual activations (see Figure 4.8).

åœ¨å¤„ç†å›¾åƒå’Œ2Då¼ é‡æ—¶ï¼Œä¿¡å·çš„çŸ­æœŸç›¸å…³æ€§å’Œç”±æ­¤äº§ç”Ÿçš„å†—ä½™æŠµæ¶ˆäº†Dropoutçš„æ•ˆæœï¼Œå› ä¸ºè¢«è®¾ä¸ºé›¶çš„æ¿€æ´»å€¼å¯ä»¥ä»å…¶é‚»å±…æ¨æ–­å‡ºæ¥ã€‚å› æ­¤ï¼Œå¯¹äº\( 2D \)å¼ é‡çš„Dropoutå°†æ•´ä¸ªé€šé“è®¾ä¸ºé›¶ï¼Œè€Œä¸æ˜¯å•ä¸ªæ¿€æ´»å€¼ï¼ˆè§å›¾4.8ï¼‰ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **2D Dropout**ï¼šåœ¨å¤„ç†å›¾åƒç­‰2Dæ•°æ®æ—¶ï¼ŒDropouté€šå¸¸ä½œç”¨äºæ•´ä¸ªé€šé“ï¼Œè€Œä¸æ˜¯å•ä¸ªåƒç´ ã€‚è¿™æ˜¯å› ä¸ºå›¾åƒä¸­çš„åƒç´ ä¹‹é—´å­˜åœ¨è¾ƒå¼ºçš„ç›¸å…³æ€§ï¼Œå•ä¸ªåƒç´ çš„Dropoutæ•ˆæœä¸æ˜æ˜¾ã€‚

Although dropout is generally used to improve training and is inactive during inference, it can be used in certain setups as a randomization strategy, for instance, to estimate empirically confidence scores [Gal and Ghahramani, 2015].

å°½ç®¡Dropouté€šå¸¸ç”¨äºæ”¹è¿›è®­ç»ƒå¹¶åœ¨æ¨ç†æœŸé—´ä¸æ´»åŠ¨ï¼Œä½†å®ƒå¯ä»¥åœ¨æŸäº›è®¾ç½®ä¸­ç”¨ä½œéšæœºåŒ–ç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œç”¨äºç»éªŒä¼°è®¡ç½®ä¿¡åº¦åˆ†æ•°[Gal and Ghahramani, 2015]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Dropoutçš„å…¶ä»–ç”¨é€”**ï¼šé™¤äº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒDropoutè¿˜å¯ä»¥ç”¨äºä¼°è®¡æ¨¡å‹çš„ä¸ç¡®å®šæ€§æˆ–ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å‡ºã€‚

---

### Chapter 4.6 Normalizing Layers

#### 4.6 å½’ä¸€åŒ–å±‚

An important class of operators to facilitate the training of deep architectures are the normalizing layers, which force the empirical mean and variance of groups of activations.

ä¿ƒè¿›æ·±åº¦æ¶æ„è®­ç»ƒçš„ä¸€ç±»é‡è¦æ“ä½œç¬¦æ˜¯å½’ä¸€åŒ–å±‚ï¼Œå®ƒä»¬å¼ºåˆ¶æ¿€æ´»å€¼ç»„çš„ç»éªŒå‡å€¼å’Œæ–¹å·®ã€‚

The main layer in that family is batch normalization [Ioffe and Szegedy, 2015], which is the only standard layer to process batches instead of individual samples. It is parameterized by a hyper-parameter \( D \) and two series of trainable scalar parameters \(\beta_1, \ldots, \beta_D\) and \(\gamma_1, \ldots, \gamma_D\).

è¯¥å®¶æ—ä¸­çš„ä¸»è¦å±‚æ˜¯æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰[Ioffe and Szegedy, 2015]ï¼Œå®ƒæ˜¯å”¯ä¸€å¤„ç†æ‰¹æ¬¡è€Œä¸æ˜¯å•ä¸ªæ ·æœ¬çš„æ ‡å‡†å±‚ã€‚å®ƒç”±ä¸€ä¸ªè¶…å‚æ•°\( D \)å’Œä¸¤ä¸ªå¯è®­ç»ƒæ ‡é‡å‚æ•°ç³»åˆ—\(\beta_1, \ldots, \beta_D\)å’Œ\(\gamma_1, \ldots, \gamma_D\)å‚æ•°åŒ–ã€‚

Given a batch of \( B \) samples \( x_1, \ldots, x_B \) of dimension \( D \), it first computes for each of the \( D \) components an empirical mean \(\hat{m}_d\) and variance \(\hat{v}_d\) across the batch:

ç»™å®šä¸€æ‰¹ç»´åº¦ä¸º\( D \)çš„\( B \)ä¸ªæ ·æœ¬\( x_1, \ldots, x_B \)ï¼Œå®ƒé¦–å…ˆä¸ºæ¯ä¸ª\( D \)åˆ†é‡è®¡ç®—æ‰¹æ¬¡ä¸Šçš„ç»éªŒå‡å€¼\(\hat{m}_d\)å’Œæ–¹å·®\(\hat{v}_d\)ï¼š

\[\hat{m}_d = \frac{1}{B} \sum_{b=1}^{B} x_b,d\]

\[\hat{v}_d = \frac{1}{B} \sum_{b=1}^{B} (x_{b,d} - \hat{m}_d)^2,\]

from which it computes for every component \( x_{b,d} \) a normalized value \( z_{b,d} \), with empirical mean 0 and variance 1, and from it the final result value \( y_{b,d} \) with mean \(\beta_d\) and standard de-

ç„¶åå®ƒä¸ºæ¯ä¸ªåˆ†é‡\( x_{b,d} \)è®¡ç®—ä¸€ä¸ªå½’ä¸€åŒ–å€¼\( z_{b,d} \)ï¼Œå…¶ç»éªŒå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œå¹¶ä»ä¸­è®¡ç®—æœ€ç»ˆç»“æœå€¼\( y_{b,d} \)ï¼Œå…¶å‡å€¼ä¸º\(\beta_d\)ï¼Œæ ‡å‡†å·®ä¸º\(\gamma_d\)ï¼š

\[\forall b, \quad z_{b,d} = \frac{x_{b,d} - \hat{m}_d}{\sqrt{\hat{v}_d + \epsilon}}\]

\[y_{b,d} = \gamma_d z_{b,d} + \beta_d.\]

Because this normalization is defined across a batch, it is done only during training. During testing, the layer transforms individual samples according to the \(\hat{m}_d s\) and \(\hat{v}_d s\) estimated with a moving average over the full training set, which boils down to a fixed affine transformation per component.

ç”±äºè¿™ç§å½’ä¸€åŒ–æ˜¯åœ¨æ‰¹æ¬¡ä¸Šå®šä¹‰çš„ï¼Œå› æ­¤ä»…åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œã€‚åœ¨æµ‹è¯•æœŸé—´ï¼Œè¯¥å±‚æ ¹æ®åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šä¼°è®¡çš„\(\hat{m}_d s\)å’Œ\(\hat{v}_d s\)å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œå˜æ¢ï¼Œè¿™å½’ç»“ä¸ºæ¯ä¸ªåˆ†é‡çš„å›ºå®šä»¿å°„å˜æ¢ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰**ï¼šæ‰¹å½’ä¸€åŒ–é€šè¿‡åœ¨è®­ç»ƒæœŸé—´å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—æ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
- **å½’ä¸€åŒ–çš„ä½œç”¨**ï¼šå½’ä¸€åŒ–å±‚é€šè¿‡å¼ºåˆ¶æ¿€æ´»å€¼çš„å‡å€¼å’Œæ–¹å·®ï¼Œä½¿å¾—æ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œä»è€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚

The motivation behind batch normalization was to avoid that a change in scaling in an early layer of the network during training impacts all the layers that follow, which then have to adapt their trainable parameters accordingly. Although the actual mode of action may be more complicated than this initial motivation, this layer considerably facilitates the training of deep models.

æ‰¹å½’ä¸€åŒ–çš„åŠ¨æœºæ˜¯é¿å…è®­ç»ƒæœŸé—´ç½‘ç»œæ—©æœŸå±‚çš„ç¼©æ”¾å˜åŒ–å½±å“æ‰€æœ‰åç»­å±‚ï¼Œè¿™äº›å±‚éšåå¿…é¡»ç›¸åº”åœ°è°ƒæ•´å…¶å¯è®­ç»ƒå‚æ•°ã€‚å°½ç®¡å®é™…ä½œç”¨æ¨¡å¼å¯èƒ½æ¯”è¿™ä¸ªåˆå§‹åŠ¨æœºæ›´å¤æ‚ï¼Œä½†è¯¥å±‚å¤§å¤§ä¿ƒè¿›äº†æ·±åº¦æ¨¡å‹çš„è®­ç»ƒã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ‰¹å½’ä¸€åŒ–çš„åŠ¨æœº**ï¼šæ‰¹å½’ä¸€åŒ–é€šè¿‡ç¨³å®šæ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ€§èƒ½ã€‚

In the case of \(2D\) tensors, to follow the principle of convolutional layers of processing all locations similarly, the normalization is done per-channel across all \(2D\) positions, and \(\beta\) and \(\gamma\) remain vectors of dimension \(D\) so that the scaling/shift does not depend on the \(2D\) position. Hence, if the tensor to be processed is of shape \( B \times D \times H \times W \), the layer computes \((m_d, v_d)\), for \( d = 1, \ldots, D \) from the corresponding \( B \times H \times W \) slice, normalizes it accordingly, and finally scales and shifts its components with the trainable parameters \(\beta_d\) and \(\gamma_d\).

åœ¨å¤„ç†\(2D\)å¼ é‡æ—¶ï¼Œä¸ºäº†éµå¾ªå·ç§¯å±‚å¯¹æ‰€æœ‰ä½ç½®è¿›è¡Œç±»ä¼¼å¤„ç†çš„åŸåˆ™ï¼Œå½’ä¸€åŒ–æ˜¯åœ¨æ‰€æœ‰\(2D\)ä½ç½®ä¸ŠæŒ‰é€šé“è¿›è¡Œçš„ï¼Œ\(\beta\)å’Œ\(\gamma\)ä¿æŒä¸ºç»´åº¦\(D\)çš„å‘é‡ï¼Œå› æ­¤ç¼©æ”¾/å¹³ç§»ä¸ä¾èµ–äº\(2D\)ä½ç½®ã€‚å› æ­¤ï¼Œå¦‚æœè¦å¤„ç†çš„å¼ é‡å½¢çŠ¶ä¸º\( B \times D \times H \times W \)ï¼Œåˆ™è¯¥å±‚ä»ç›¸åº”çš„\( B \times H \times W \)åˆ‡ç‰‡è®¡ç®—\((m_d, v_d)\)ï¼Œå¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æœ€ç»ˆä½¿ç”¨å¯è®­ç»ƒå‚æ•°\(\beta_d\)å’Œ\(\gamma_d\)ç¼©æ”¾å’Œå¹³ç§»å…¶åˆ†é‡ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **2Dæ‰¹å½’ä¸€åŒ–**ï¼šåœ¨å¤„ç†å›¾åƒç­‰2Dæ•°æ®æ—¶ï¼Œæ‰¹å½’ä¸€åŒ–é€šå¸¸æŒ‰é€šé“è¿›è¡Œï¼Œå³å¯¹æ¯ä¸ªé€šé“çš„æ‰€æœ‰ä½ç½®è¿›è¡Œå½’ä¸€åŒ–ã€‚

So, given a \( B \times D \) tensor, batch normalization normalizes it across \( b \) and scales/shifts it according to \( d \), which can be implemented as a component-wise product by \(\gamma\) and a sum with \(\beta\). Given a \( B \times D \times H \times W \) tensor, it normalizes across \( b, h, w \) and scales/shifts according to \( d \) (see Figure 4.9, left).

å› æ­¤ï¼Œç»™å®šä¸€ä¸ª\( B \times D \)å¼ é‡ï¼Œæ‰¹å½’ä¸€åŒ–åœ¨\( b \)ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ ¹æ®\( d \)è¿›è¡Œç¼©æ”¾/å¹³ç§»ï¼Œè¿™å¯ä»¥é€šè¿‡\(\gamma\)çš„é€åˆ†é‡ä¹˜ç§¯å’Œä¸\(\beta\)çš„å’Œæ¥å®ç°ã€‚ç»™å®šä¸€ä¸ª\( B \times D \times H \times W \)å¼ é‡ï¼Œå®ƒåœ¨\( b, h, w \)ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ ¹æ®\( d \)è¿›è¡Œç¼©æ”¾/å¹³ç§»ï¼ˆè§å›¾4.9ï¼Œå·¦ï¼‰ã€‚

This can be generalized depending on these dimensions. For instance, layer normalization [Ba et al., 2016] computes moments and normalizes across all components of individual samples, and scales and shifts components individually (see Figure 4.9, right). So, given a \( B \times D \) tensor, it normalizes across \( d \) and scales/shifts also according to the same. Given a \( B \times D \times H \times W \) tensor, it normalizes it across \( d, h, w \) and scales/shifts according to the same.

è¿™å¯ä»¥æ ¹æ®è¿™äº›ç»´åº¦è¿›è¡Œæ¨å¹¿ã€‚ä¾‹å¦‚ï¼Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰[Ba et al., 2016]è®¡ç®—çŸ©å¹¶åœ¨å•ä¸ªæ ·æœ¬çš„æ‰€æœ‰åˆ†é‡ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶å•ç‹¬ç¼©æ”¾å’Œå¹³ç§»åˆ†é‡ï¼ˆè§å›¾4.9ï¼Œå³ï¼‰ã€‚å› æ­¤ï¼Œç»™å®šä¸€ä¸ª\( B \times D \)å¼ é‡ï¼Œå®ƒåœ¨\( d \)ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ ¹æ®ç›¸åŒçš„\( d \)è¿›è¡Œç¼©æ”¾/å¹³ç§»ã€‚ç»™å®šä¸€ä¸ª\( B \times D \times H \times W \)å¼ é‡ï¼Œå®ƒåœ¨\( d, h, w \)ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ ¹æ®ç›¸åŒçš„\( d, h, w \)è¿›è¡Œç¼©æ”¾/å¹³ç§»ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰**ï¼šå±‚å½’ä¸€åŒ–åœ¨å•ä¸ªæ ·æœ¬çš„æ‰€æœ‰åˆ†é‡ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œé€‚ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ï¼‰æˆ–å°æ‰¹é‡æ•°æ®ã€‚

Contrary to batch normalization, since it processes samples individually, layer normalization behaves the same during training and testing.

ä¸æ‰¹å½’ä¸€åŒ–ä¸åŒï¼Œç”±äºå±‚å½’ä¸€åŒ–é€ä¸ªå¤„ç†æ ·æœ¬ï¼Œå› æ­¤åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´è¡Œä¸ºç›¸åŒã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å±‚å½’ä¸€åŒ–çš„ä¼˜ç‚¹**ï¼šå±‚å½’ä¸€åŒ–ä¸ä¾èµ–äºæ‰¹æ¬¡å¤§å°ï¼Œå› æ­¤åœ¨å¤„ç†å°æ‰¹é‡æ•°æ®æˆ–åºåˆ—æ•°æ®æ—¶è¡¨ç°æ›´å¥½ã€‚

---

### æ€»ç»“ï¼š
- **æ± åŒ–**é€šè¿‡å°†å¤šä¸ªæ¿€æ´»å€¼ç»„åˆæˆä¸€ä¸ªå€¼æ¥å‡å°‘ä¿¡å·çš„å¤§å°ï¼Œæœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–æ˜¯å¸¸ç”¨çš„æ± åŒ–æ“ä½œã€‚
- **Dropout**é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒæ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **å½’ä¸€åŒ–å±‚**é€šè¿‡å¼ºåˆ¶æ¿€æ´»å€¼çš„å‡å€¼å’Œæ–¹å·®ï¼Œä½¿å¾—æ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ‰¹å½’ä¸€åŒ–å’Œå±‚å½’ä¸€åŒ–æ˜¯å¸¸ç”¨çš„å½’ä¸€åŒ–æ–¹æ³•ã€‚

è¿™äº›çŸ¥è¯†ç‚¹æ˜¯ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–è¿‡ç¨‹çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬æœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

---

å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 4.7åˆ°Chapter 4.10çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 4.7 Skip Connections

#### 4.7 è·³è·ƒè¿æ¥

Another technique that mitigates the vanishing gradient and allows the training of deep architectures are skip connections [Long et al., 2014; Ronneberger et al., 2015]. They are not layers per se, but an architectural design in which outputs of some layers are transported as-is to other layers further in the model, bypassing processing in between. This unmodified signal can be concatenated or added to the input of the layer the connection branches into (see Figure 4.10). A particular type of skip connections are the residual connections which combine the signal with a sum, and usually skip only a few layers (see Figure 4.10, right).

å¦ä¸€ç§ç¼“è§£æ¢¯åº¦æ¶ˆå¤±å¹¶å…è®¸è®­ç»ƒæ·±åº¦æ¶æ„çš„æŠ€æœ¯æ˜¯è·³è·ƒè¿æ¥ï¼ˆSkip Connectionsï¼‰[Long et al., 2014; Ronneberger et al., 2015]ã€‚å®ƒä»¬æœ¬èº«ä¸æ˜¯å±‚ï¼Œè€Œæ˜¯ä¸€ç§æ¶æ„è®¾è®¡ï¼Œå…¶ä¸­æŸäº›å±‚çš„è¾“å‡ºè¢«åŸå°ä¸åŠ¨åœ°ä¼ è¾“åˆ°æ¨¡å‹ä¸­æ›´è¿œçš„å±‚ï¼Œç»•è¿‡ä¸­é—´çš„å¤„ç†ã€‚è¿™ä¸ªæœªä¿®æ”¹çš„ä¿¡å·å¯ä»¥è¿æ¥åˆ°åˆ†æ”¯è¿›å…¥çš„å±‚çš„è¾“å…¥ï¼ˆè§å›¾4.10ï¼‰ã€‚ä¸€ç§ç‰¹æ®Šç±»å‹çš„è·³è·ƒè¿æ¥æ˜¯æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰ï¼Œå®ƒä»¬é€šè¿‡æ±‚å’Œå°†ä¿¡å·ç»„åˆåœ¨ä¸€èµ·ï¼Œé€šå¸¸åªè·³è¿‡å‡ å±‚ï¼ˆè§å›¾4.10ï¼Œå³ï¼‰ã€‚

The most desirable property of this design is to ensure that, even in the case of gradient-killing processing at a certain stage, the gradient will still propagate through the skip connections. Residual connections, in particular, allow for the building of deep models with up to several hundred layers, and key models, such as the residual networks [He et al., 2015] in computer vision (see Â§ 5.2), and the Transformers [Vaswani et al., 2017] in natural language processing (see Â§ 5.3), are entirely composed of blocks of layers with residual connections.

è¿™ç§è®¾è®¡çš„æœ€ç†æƒ³ç‰¹æ€§æ˜¯ç¡®ä¿å³ä½¿åœ¨æŸä¸ªé˜¶æ®µå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±çš„å¤„ç†ï¼Œæ¢¯åº¦ä»ç„¶å¯ä»¥é€šè¿‡è·³è·ƒè¿æ¥ä¼ æ’­ã€‚ç‰¹åˆ«æ˜¯æ®‹å·®è¿æ¥ï¼Œå…è®¸æ„å»ºå…·æœ‰å¤šè¾¾æ•°ç™¾å±‚çš„æ·±åº¦æ¨¡å‹ï¼Œå…³é”®æ¨¡å‹å¦‚è®¡ç®—æœºè§†è§‰ä¸­çš„æ®‹å·®ç½‘ç»œï¼ˆResidual Networksï¼‰[He et al., 2015]ï¼ˆè§Â§5.2ï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„Transformer [Vaswani et al., 2017]ï¼ˆè§Â§5.3ï¼‰ï¼Œå®Œå…¨ç”±å¸¦æœ‰æ®‹å·®è¿æ¥çš„å±‚å—ç»„æˆã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è·³è·ƒè¿æ¥ï¼ˆSkip Connectionsï¼‰**ï¼šè·³è·ƒè¿æ¥é€šè¿‡å°†æŸäº›å±‚çš„è¾“å‡ºç›´æ¥ä¼ é€’åˆ°æ›´è¿œçš„å±‚ï¼Œç»•è¿‡ä¸­é—´çš„å¤„ç†ï¼Œä»è€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰**ï¼šæ®‹å·®è¿æ¥æ˜¯ä¸€ç§ç‰¹æ®Šçš„è·³è·ƒè¿æ¥ï¼Œé€šè¿‡å°†è¾“å…¥ä¸è¾“å‡ºç›¸åŠ ï¼Œä½¿å¾—æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ æ’­ï¼Œä»è€Œå…è®¸è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œã€‚

Their role can also be to facilitate multi-scale reasoning in models that reduce the signal size before re-expanding it, by connecting layers with compatible sizes, for instance for semantic segmentation (see Â§ 6.4). In the case of residual connections, they may also facilitate learning by simplifying the task to finding a differential improvement instead of a full update.

å®ƒä»¬çš„ä½œç”¨è¿˜å¯ä»¥é€šè¿‡åœ¨æ¨¡å‹ä¸­å°†ä¿¡å·å¤§å°ç¼©å°åå†é‡æ–°æ‰©å±•æ—¶è¿æ¥å…·æœ‰å…¼å®¹å¤§å°çš„å±‚ï¼Œæ¥ä¿ƒè¿›å¤šå°ºåº¦æ¨ç†ï¼Œä¾‹å¦‚ç”¨äºè¯­ä¹‰åˆ†å‰²ï¼ˆè§Â§6.4ï¼‰ã€‚åœ¨æ®‹å·®è¿æ¥çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬è¿˜å¯ä»¥é€šè¿‡å°†ä»»åŠ¡ç®€åŒ–ä¸ºå¯»æ‰¾å·®åˆ†æ”¹è¿›è€Œä¸æ˜¯å®Œå…¨æ›´æ–°æ¥ä¿ƒè¿›å­¦ä¹ ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¤šå°ºåº¦æ¨ç†ï¼ˆMulti-scale Reasoningï¼‰**ï¼šè·³è·ƒè¿æ¥å¯ä»¥ç”¨äºå¤šå°ºåº¦æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ä¸­ï¼Œé€šè¿‡è¿æ¥ä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ã€‚

---

### Chapter 4.8 Attention Layers

#### 4.8 æ³¨æ„åŠ›å±‚

In many applications, there is a need for an operation able to combine local information at locations far apart in a tensor. For instance, this could be distant details for coherent and realistic image synthesis, or words at different positions in a paragraph to make a grammatical or semantic decision in Natural Language Processing.

åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œéœ€è¦ä¸€ç§æ“ä½œèƒ½å¤Ÿç»„åˆå¼ é‡ä¸­ç›¸è·è¾ƒè¿œçš„å±€éƒ¨ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè¿™å¯èƒ½æ˜¯ç”¨äºè¿è´¯å’Œé€¼çœŸçš„å›¾åƒåˆæˆçš„è¿œå¤„ç»†èŠ‚ï¼Œæˆ–è€…æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ç”¨äºåšå‡ºè¯­æ³•æˆ–è¯­ä¹‰å†³ç­–çš„æ®µè½ä¸­ä¸åŒä½ç½®çš„å•è¯ã€‚

Fully connected layers cannot process large-dimension signals, nor signals of variable size, and \underline{convolutional} layers are not able to propagate information quickly. Strategies that aggregate the results of convolutions, for instance, by averaging them over large spatial areas, suffer from mixing multiple signals into a limited number of dimensions.

å…¨è¿æ¥å±‚æ— æ³•å¤„ç†å¤§ç»´åº¦ä¿¡å·ï¼Œä¹Ÿæ— æ³•å¤„ç†å¯å˜å¤§å°çš„ä¿¡å·ï¼Œè€Œ\underline{å·ç§¯}å±‚æ— æ³•å¿«é€Ÿä¼ æ’­ä¿¡æ¯ã€‚é€šè¿‡åœ¨å¤§ç©ºé—´åŒºåŸŸä¸Šå¹³å‡å·ç§¯ç»“æœç­‰ç­–ç•¥ï¼Œä¼šå°†å¤šä¸ªä¿¡å·æ··åˆåˆ°æœ‰é™æ•°é‡çš„ç»´åº¦ä¸­ã€‚

Attention layers specifically address this problem by computing an attention score for each component of the resulting tensor to each component of the input tensor, without locality constraints, and averaging the features across the full tensor accordingly [Vaswani et al., 2017].

æ³¨æ„åŠ›å±‚é€šè¿‡ä¸ºç»“æœå¼ é‡çš„æ¯ä¸ªåˆ†é‡è®¡ç®—ä¸è¾“å…¥å¼ é‡æ¯ä¸ªåˆ†é‡çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¸“é—¨è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸï¼Œå¹¶ç›¸åº”åœ°å¹³å‡æ•´ä¸ªå¼ é‡ä¸Šçš„ç‰¹å¾[Vaswani et al., 2017]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**ï¼šæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è®¡ç®—è¾“å…¥å¼ é‡ä¸­æ¯ä¸ªåˆ†é‡ä¸è¾“å‡ºå¼ é‡ä¸­æ¯ä¸ªåˆ†é‡çš„ç›¸å…³æ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
- **æ³¨æ„åŠ›å±‚çš„åº”ç”¨**ï¼šæ³¨æ„åŠ›å±‚å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨Transformeræ¨¡å‹ä¸­ã€‚

Even though they are substantially more complicated than other layers, they have become a standard element in many recent models. They are, in particular, the key building block of \underline{Transformers}, the dominant architecture for Large Language Models. See Â§ 5.3 and Â§ 7.1.

å°½ç®¡å®ƒä»¬æ¯”å…¶ä»–å±‚å¤æ‚å¾—å¤šï¼Œä½†å®ƒä»¬å·²æˆä¸ºè®¸å¤šæœ€æ–°æ¨¡å‹ä¸­çš„æ ‡å‡†å…ƒç´ ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒä»¬æ˜¯\underline{Transformer}çš„å…³é”®æ„å»ºå—ï¼ŒTransformeræ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»å¯¼æ¶æ„ã€‚å‚è§Â§5.3å’ŒÂ§7.1ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Transformeræ¨¡å‹**ï¼šTransformeræ¨¡å‹é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰å®ç°äº†å¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

---

### Chapter 4.9 Token Embedding

#### 4.9 è¯åµŒå…¥

In many situations, we need to convert discrete tokens into vectors. This can be done with an embedding layer, which consists of a lookup table that directly maps integers to vectors.

åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å°†ç¦»æ•£çš„æ ‡è®°ï¼ˆtokensï¼‰è½¬æ¢ä¸ºå‘é‡ã€‚è¿™å¯ä»¥é€šè¿‡åµŒå…¥å±‚ï¼ˆEmbedding Layerï¼‰å®ç°ï¼ŒåµŒå…¥å±‚ç”±ä¸€ä¸ªæŸ¥æ‰¾è¡¨ç»„æˆï¼Œç›´æ¥å°†æ•´æ•°æ˜ å°„åˆ°å‘é‡ã€‚

Such a layer is defined by two hyper-parameters: the number \( N \) of possible token values, and the dimension \( D \) of the output vectors, and one trainable \( N \times D \) weight matrix \( M \).

è¿™æ ·çš„å±‚ç”±ä¸¤ä¸ªè¶…å‚æ•°å®šä¹‰ï¼šå¯èƒ½çš„æ ‡è®°å€¼æ•°é‡\( N \)å’Œè¾“å‡ºå‘é‡çš„ç»´åº¦\( D \)ï¼Œä»¥åŠä¸€ä¸ªå¯è®­ç»ƒçš„\( N \times D \)æƒé‡çŸ©é˜µ\( M \)ã€‚

Given as input an integer tensor \( X \) of dimension \( D_1 \times \cdots \times D_K \) and values in \(\{0, \ldots, N - 1\}\) such a layer returns a real-valued tensor \( Y \) of dimension \( D_1 \times \cdots \times D_K \times D \) with

ç»™å®šä¸€ä¸ªç»´åº¦ä¸º\( D_1 \times \cdots \times D_K \)ä¸”å€¼åœ¨\(\{0, \ldots, N - 1\}\)ä¸­çš„æ•´æ•°å¼ é‡\( X \)ï¼Œè¿™æ ·çš„å±‚è¿”å›ä¸€ä¸ªç»´åº¦ä¸º\( D_1 \times \cdots \times D_K \times D \)çš„å®å€¼å¼ é‡\( Y \)ï¼Œå…¶ä¸­

\[\forall d_1, \ldots, d_K,\]
\[Y[d_1, \ldots, d_K] = M[X[d_1, \ldots, d_K]].\]

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è¯åµŒå…¥ï¼ˆToken Embeddingï¼‰**ï¼šè¯åµŒå…¥å±‚å°†ç¦»æ•£çš„æ ‡è®°ï¼ˆå¦‚å•è¯æˆ–å­—ç¬¦ï¼‰æ˜ å°„åˆ°è¿ç»­çš„å‘é‡ç©ºé—´ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ–‡æœ¬æ•°æ®ã€‚
- **åµŒå…¥å±‚çš„åº”ç”¨**ï¼šåµŒå…¥å±‚å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ï¼Œå°†å•è¯æˆ–å­—ç¬¦è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚

---

### Chapter 4.10 Positional Encoding

#### 4.10 ä½ç½®ç¼–ç 

While the processing of a fully connected layer is specific to both the positions of the features in the input tensor and to the positions of the resulting activations in the output tensor, convolutional layers and Multi-Head Attention layers are oblivious to the absolute position in the tensor. This is key to their strong invariance and inductive bias, which is beneficial for dealing with a stationary signal.

è™½ç„¶å…¨è¿æ¥å±‚çš„å¤„ç†ç‰¹å®šäºè¾“å…¥å¼ é‡ä¸­ç‰¹å¾çš„ä½ç½®å’Œè¾“å‡ºå¼ é‡ä¸­ç»“æœæ¿€æ´»å€¼çš„ä½ç½®ï¼Œä½†å·ç§¯å±‚å’Œå¤šå¤´æ³¨æ„åŠ›å±‚å¯¹å¼ é‡ä¸­çš„ç»å¯¹ä½ç½®ä¸æ•æ„Ÿã€‚è¿™æ˜¯å®ƒä»¬å¼ºä¸å˜æ€§å’Œå½’çº³åå·®çš„å…³é”®ï¼Œè¿™å¯¹äºå¤„ç†å¹³ç¨³ä¿¡å·æ˜¯æœ‰ç›Šçš„ã€‚

However, this can be an issue in certain situations where proper processing has to access the absolute positioning. This is the case, for instance, for image synthesis, where the statistics of a scene are not totally stationary, or in natural language processing, where the relative positions of words strongly modulate the meaning of a sentence.

ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºé€‚å½“çš„å¤„ç†å¿…é¡»è®¿é—®ç»å¯¹ä½ç½®ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒåˆæˆä¸­ï¼Œåœºæ™¯çš„ç»Ÿè®¡ç‰¹æ€§å¹¶ä¸å®Œå…¨å¹³ç¨³ï¼Œæˆ–è€…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå•è¯çš„ç›¸å¯¹ä½ç½®å¼ºçƒˆè°ƒèŠ‚å¥å­çš„å«ä¹‰ã€‚

The standard way of coping with this problem is to add or concatenate to the feature representation, at every position, a positional encoding, which is a feature vector that depends on the position in the tensor. This positional encoding can be learned as other layer parameters, or defined analytically.

è§£å†³è¿™ä¸ªé—®é¢˜çš„æ ‡å‡†æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªä½ç½®çš„ç‰¹å¾è¡¨ç¤ºä¸­æ·»åŠ æˆ–è¿æ¥ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¾èµ–äºå¼ é‡ä¸­ä½ç½®çš„ç‰¹å¾å‘é‡ã€‚è¿™ä¸ªä½ç½®ç¼–ç å¯ä»¥åƒå…¶ä»–å±‚å‚æ•°ä¸€æ ·å­¦ä¹ ï¼Œæˆ–è€…é€šè¿‡åˆ†æå®šä¹‰ã€‚

For instance, in the original Transformer model, for a series of vectors of dimension \( D \), Vaswani et al. [2017] add an encoding of the sequence index as a series of sines and cosines at various frequencies:

ä¾‹å¦‚ï¼Œåœ¨åŸå§‹çš„Transformeræ¨¡å‹ä¸­ï¼Œå¯¹äºç»´åº¦ä¸º\( D \)çš„ä¸€ç³»åˆ—å‘é‡ï¼ŒVaswani et al. [2017]æ·»åŠ äº†åºåˆ—ç´¢å¼•çš„ç¼–ç ï¼Œä½œä¸ºä¸€ç³»åˆ—ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦ï¼š

pos-enc[\( t, d \)]

\[\begin{cases}
\sin\left(\frac{t}{T^d/D}\right) & \text{if } d \in 2\mathbb{N} \\
\cos\left(\frac{t}{T^{(d-1)/D}}\right) & \text{otherwise},
\end{cases}\]

with \( T = 10^4 \).

å…¶ä¸­\( T = 10^4 \)ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**ï¼šä½ç½®ç¼–ç ç”¨äºåœ¨æ¨¡å‹ä¸­å¼•å…¥åºåˆ—çš„ä½ç½®ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬æˆ–æ—¶é—´åºåˆ—ï¼‰æ—¶ã€‚Transformeræ¨¡å‹é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç ã€‚
- **ä½ç½®ç¼–ç çš„ä½œç”¨**ï¼šä½ç½®ç¼–ç ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åºåˆ—ä¸­å…ƒç´ çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†åºåˆ—æ•°æ®ã€‚

---

### æ€»ç»“ï¼š
- **è·³è·ƒè¿æ¥**é€šè¿‡å°†æŸäº›å±‚çš„è¾“å‡ºç›´æ¥ä¼ é€’åˆ°æ›´è¿œçš„å±‚ï¼Œç»•è¿‡ä¸­é—´çš„å¤„ç†ï¼Œä»è€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ®‹å·®è¿æ¥æ˜¯ä¸€ç§ç‰¹æ®Šçš„è·³è·ƒè¿æ¥ï¼Œå¹¿æ³›åº”ç”¨äºæ·±åº¦æ¨¡å‹ä¸­ã€‚
- **æ³¨æ„åŠ›å±‚**é€šè¿‡è®¡ç®—è¾“å…¥å¼ é‡ä¸­æ¯ä¸ªåˆ†é‡ä¸è¾“å‡ºå¼ é‡ä¸­æ¯ä¸ªåˆ†é‡çš„ç›¸å…³æ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ã€‚
- **è¯åµŒå…¥å±‚**å°†ç¦»æ•£çš„æ ‡è®°æ˜ å°„åˆ°è¿ç»­çš„å‘é‡ç©ºé—´ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ–‡æœ¬æ•°æ®ã€‚
- **ä½ç½®ç¼–ç **ç”¨äºåœ¨æ¨¡å‹ä¸­å¼•å…¥åºåˆ—çš„ä½ç½®ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ã€‚Transformeræ¨¡å‹é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç ã€‚

è¿™äº›çŸ¥è¯†ç‚¹æ˜¯ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–è¿‡ç¨‹çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬æœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

---

å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 5çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 5 Architectures

#### 5 æ¶æ„

The field of deep learning has developed over the years for each application domain multiple deep architectures that exhibit good trade-offs with respect to multiple criteria of interest: e.g. ease of training, accuracy of prediction, memory footprint, computational cost, scalability.

å¤šå¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸä¸ºæ¯ä¸ªåº”ç”¨é¢†åŸŸå¼€å‘äº†å¤šç§æ·±åº¦æ¶æ„ï¼Œè¿™äº›æ¶æ„åœ¨å¤šä¸ªæ„Ÿå…´è¶£çš„æ ‡å‡†ä¹‹é—´è¡¨ç°å‡ºè‰¯å¥½çš„æƒè¡¡ï¼šä¾‹å¦‚ï¼Œæ˜“äºè®­ç»ƒã€é¢„æµ‹å‡†ç¡®æ€§ã€å†…å­˜å ç”¨ã€è®¡ç®—æˆæœ¬ã€å¯æ‰©å±•æ€§ã€‚

---

### Chapter 5.1 Multi-Layer Perceptrons

#### 5.1 å¤šå±‚æ„ŸçŸ¥å™¨

The simplest deep architecture is the Multi-Layer Perceptron (MLP), which takes the form of a succession of fully connected layers separated by activation functions. See an example in Figure 5.1. For historical reasons, in such a model, the number of hidden layers refers to the number of linear layers, excluding the last one.

æœ€ç®€å•çš„æ·±åº¦æ¶æ„æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œå®ƒç”±ä¸€ç³»åˆ—å…¨è¿æ¥å±‚ç»„æˆï¼Œä¸­é—´ç”±æ¿€æ´»å‡½æ•°åˆ†éš”ã€‚è§å›¾5.1ä¸­çš„ç¤ºä¾‹ã€‚ç”±äºå†å²åŸå› ï¼Œåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œéšè—å±‚çš„æ•°é‡æŒ‡çš„æ˜¯çº¿æ€§å±‚çš„æ•°é‡ï¼Œä¸åŒ…æ‹¬æœ€åä¸€å±‚ã€‚

A key theoretical result is the universal approximation theorem [Cybenko, 1989] which states that, if the activation function \(\sigma\) is continuous

ä¸€ä¸ªå…³é”®çš„ç†è®ºç»“æœæ˜¯é€šç”¨é€¼è¿‘å®šç† [Cybenko, 1989]ï¼Œå®ƒæŒ‡å‡ºï¼Œå¦‚æœæ¿€æ´»å‡½æ•°\(\sigma\)æ˜¯è¿ç»­çš„

\[\begin{array}{c}
Y \\
\uparrow \\
\text{fully-conn} \quad 2 \\
\downarrow \\
\text{relu} \\
\downarrow \\
\text{fully-conn} \quad 10 \\
\downarrow \\
\text{relu} \\
\downarrow \\
\text{fully-conn} \quad 25 \\
\downarrow \\
X \quad 50
\end{array}\]

Hidden layers

Figure 5.1: This multi-layer perceptron takes as input a one-dimensional tensor of size 50, is composed of three fully connected layers with outputs of dimensions respectively 25, 10, and 2, the two first followed by ReLU layers.

å›¾5.1ï¼šè¿™ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨æ¥å—å¤§å°ä¸º50çš„ä¸€ç»´å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œç”±ä¸‰ä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œè¾“å‡ºç»´åº¦åˆ†åˆ«ä¸º25ã€10å’Œ2ï¼Œå‰ä¸¤å±‚åé¢è·Ÿç€ReLUå±‚ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰**ï¼šMLPæ˜¯æœ€ç®€å•çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”±å¤šä¸ªå…¨è¿æ¥å±‚å’Œæ¿€æ´»å‡½æ•°ç»„æˆã€‚å®ƒèƒ½å¤Ÿé€¼è¿‘ä»»æ„å¤æ‚çš„å‡½æ•°ã€‚
- **é€šç”¨é€¼è¿‘å®šç†ï¼ˆUniversal Approximation Theoremï¼‰**ï¼šè¯¥å®šç†æŒ‡å‡ºï¼Œåªè¦æ¿€æ´»å‡½æ•°æ˜¯è¿ç»­çš„ï¼Œå…·æœ‰ä¸€ä¸ªéšè—å±‚çš„MLPå¯ä»¥é€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ã€‚

In spite of their simplicity, MLPs remain an important tool when the dimension of the signal to be processed is not too large.

å°½ç®¡MLPç»“æ„ç®€å•ï¼Œä½†åœ¨å¤„ç†ç»´åº¦ä¸å¤§çš„ä¿¡å·æ—¶ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„å·¥å…·ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **MLPçš„åº”ç”¨**ï¼šMLPé€‚ç”¨äºå¤„ç†ä½ç»´æ•°æ®ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€å›å½’ä»»åŠ¡ç­‰ã€‚ç„¶è€Œï¼Œå¯¹äºé«˜ç»´æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰é€šå¸¸æ›´ä¸ºæœ‰æ•ˆã€‚

---

### Chapter 5.2 Convolutional Networks

#### 5.2 å·ç§¯ç½‘ç»œ

The standard architecture for processing images is a convolutional network, or \underline{convnet}, that combines multiple convolutional layers, either to reduce the signal size before it can be processed by fully connected layers, or to output a 2D signal also of large size.

å¤„ç†å›¾åƒçš„æ ‡å‡†æ¶æ„æ˜¯å·ç§¯ç½‘ç»œï¼ˆConvNetï¼‰ï¼Œå®ƒç»“åˆäº†å¤šä¸ªå·ç§¯å±‚ï¼Œè¦ä¹ˆåœ¨ä¿¡å·è¢«å…¨è¿æ¥å±‚å¤„ç†ä¹‹å‰å‡å°‘ä¿¡å·å¤§å°ï¼Œè¦ä¹ˆè¾“å‡ºåŒæ ·å…·æœ‰å¤§å°ºå¯¸çš„2Dä¿¡å·ã€‚

#### LeNet-like

The original LeNet model for image classification [LeCun et al., 1998] combines a series of 2D convolutional layers and max pooling layers that play the role of feature extractor, with a series of fully connected layers which act as a MLP and perform the classification per se (see Figure 5.2).

åŸå§‹çš„LeNetæ¨¡å‹ç”¨äºå›¾åƒåˆ†ç±» [LeCun et al., 1998]ï¼Œå®ƒç»“åˆäº†ä¸€ç³»åˆ—2Då·ç§¯å±‚å’Œæœ€å¤§æ± åŒ–å±‚ï¼Œè¿™äº›å±‚å……å½“ç‰¹å¾æå–å™¨ï¼Œä»¥åŠä¸€ç³»åˆ—å…¨è¿æ¥å±‚ï¼Œè¿™äº›å±‚å……å½“MLPå¹¶æ‰§è¡Œåˆ†ç±»æœ¬èº«ï¼ˆè§å›¾5.2ï¼‰ã€‚

This architecture was the blueprint for many models that share its structure and are simply larger, such as AlexNet [Krizhevsky et al., 2012] or the VGG family [Simonyan and Zisserman, 2014].

è¿™ç§æ¶æ„æ˜¯è®¸å¤šæ¨¡å‹çš„è“å›¾ï¼Œè¿™äº›æ¨¡å‹å…±äº«å…¶ç»“æ„å¹¶ä¸”è§„æ¨¡æ›´å¤§ï¼Œä¾‹å¦‚AlexNet [Krizhevsky et al., 2012] æˆ– VGGç³»åˆ— [Simonyan and Zisserman, 2014]ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **LeNet**ï¼šLeNetæ˜¯æœ€æ—©çš„å·ç§¯ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚å®ƒé€šè¿‡å·ç§¯å±‚å’Œæ± åŒ–å±‚æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»ã€‚
- **AlexNetå’ŒVGG**ï¼šAlexNetå’ŒVGGæ˜¯LeNetçš„æ‰©å±•ç‰ˆæœ¬ï¼Œå…·æœ‰æ›´å¤šçš„å·ç§¯å±‚å’Œæ›´å¤§çš„è§„æ¨¡ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒåˆ†ç±»çš„æ€§èƒ½ã€‚

#### Residual networks

Standard convolutional neural networks that follow the architecture of the LeNet family are not easily extended to deep architectures and suffer from the vanishing gradient problem. The residual networks, or ResNets, proposed by He et al. [2015] explicitly address the issue of the vanishing gradient with residual connections, which allow hundreds of layers. They have become standard architectures for computer vision applications, and exist in multiple versions depending on the number of layers. We are going to look in detail at the architecture of the ResNet-50 for classification.

éµå¾ªLeNetç³»åˆ—æ¶æ„çš„æ ‡å‡†å·ç§¯ç¥ç»ç½‘ç»œä¸å®¹æ˜“æ‰©å±•åˆ°æ·±åº¦æ¶æ„ï¼Œå¹¶ä¸”å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ®‹å·®ç½‘ç»œï¼ˆResNetsï¼‰ç”±He et al. [2015]æå‡ºï¼Œé€šè¿‡æ®‹å·®è¿æ¥æ˜ç¡®è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå…è®¸æ•°ç™¾å±‚çš„æ·±åº¦ã€‚å®ƒä»¬å·²æˆä¸ºè®¡ç®—æœºè§†è§‰åº”ç”¨çš„æ ‡å‡†æ¶æ„ï¼Œå¹¶æ ¹æ®å±‚æ•°å­˜åœ¨å¤šä¸ªç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹ç”¨äºåˆ†ç±»çš„ResNet-50æ¶æ„ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **æ®‹å·®ç½‘ç»œï¼ˆResNetsï¼‰**ï¼šæ®‹å·®ç½‘ç»œé€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥ï¼ˆskip connectionsï¼‰è§£å†³äº†æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚
- **ResNet-50**ï¼šResNet-50æ˜¯ä¸€ä¸ªå…·æœ‰50å±‚çš„æ®‹å·®ç½‘ç»œï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ã€‚

---

### Chapter 5.3 Attention Models

#### 5.3 æ³¨æ„åŠ›æ¨¡å‹

As stated in Â§ 4.8, many applications, particularly from natural language processing, benefit greatly from models that include attention mechanisms. The architecture of choice for such tasks, which has been instrumental in recent advances in deep learning, is the \textit{Transformer} proposed by Vaswani et al. [2017].

æ­£å¦‚åœ¨Â§4.8ä¸­æ‰€è¿°ï¼Œè®¸å¤šåº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä»»åŠ¡ï¼Œæå¤§åœ°å—ç›ŠäºåŒ…å«æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ã€‚è¿™ç±»ä»»åŠ¡çš„é¦–é€‰æ¶æ„æ˜¯Vaswani et al. [2017]æå‡ºçš„\textit{Transformer}ï¼Œå®ƒåœ¨æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

#### Transformer

The original Transformer, pictured in Figure 5.7, was designed for sequence-to-sequence translation. It combines an encoder that processes the input sequence to get a refined representation, and an autoregressive decoder that generates each token of the result sequence, given the encoderâ€™s representation of the input sequence and the output tokens generated so far.

åŸå§‹çš„Transformerï¼ˆè§å›¾5.7ï¼‰æ˜¯ä¸ºåºåˆ—åˆ°åºåˆ—ç¿»è¯‘è®¾è®¡çš„ã€‚å®ƒç»“åˆäº†ä¸€ä¸ªç¼–ç å™¨ï¼Œç”¨äºå¤„ç†è¾“å…¥åºåˆ—ä»¥è·å¾—ç²¾ç»†çš„è¡¨ç¤ºï¼Œä»¥åŠä¸€ä¸ªè‡ªå›å½’è§£ç å™¨ï¼Œæ ¹æ®ç¼–ç å™¨å¯¹è¾“å…¥åºåˆ—çš„è¡¨ç¤ºå’Œè¿„ä»Šä¸ºæ­¢ç”Ÿæˆçš„è¾“å‡ºæ ‡è®°ï¼Œç”Ÿæˆç»“æœåºåˆ—çš„æ¯ä¸ªæ ‡è®°ã€‚

As the residual convolutional networks of Â§ 5.2, both the encoder and the decoder of the Transformer are sequences of compounded blocks built with residual connections.

æ­£å¦‚Â§5.2ä¸­çš„æ®‹å·®å·ç§¯ç½‘ç»œï¼ŒTransformerçš„ç¼–ç å™¨å’Œè§£ç å™¨éƒ½æ˜¯ç”±æ®‹å·®è¿æ¥æ„å»ºçš„å¤åˆå—åºåˆ—ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Transformer**ï¼šTransformeræ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
- **ç¼–ç å™¨-è§£ç å™¨æ¶æ„**ï¼šTransformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨å¤„ç†è¾“å…¥åºåˆ—ï¼Œè§£ç å™¨ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚

#### Generative Pre-trained Transformer

The \textit{Generative Pre-trained Transformer (GPT)} [Radford et al., 2018, 2019], pictured in Figure 5.8 is a pure autoregressive model that consists of a succession of causal self-attention blocks, hence a causal version of the original Transformer encoder.

\textit{ç”Ÿæˆå¼é¢„è®­ç»ƒTransformerï¼ˆGPTï¼‰} [Radford et al., 2018, 2019]ï¼ˆè§å›¾5.8ï¼‰æ˜¯ä¸€ä¸ªçº¯ç²¹çš„è‡ªå›å½’æ¨¡å‹ï¼Œç”±ä¸€ç³»åˆ—å› æœè‡ªæ³¨æ„åŠ›å—ç»„æˆï¼Œå› æ­¤æ˜¯åŸå§‹Transformerç¼–ç å™¨çš„å› æœç‰ˆæœ¬ã€‚

This class of models scales extremely well, up to hundreds of billions of trainable parameters [Brown et al., 2020]. We will come back to their use for text generation in Â§ 7.1.

è¿™ç±»æ¨¡å‹æ‰©å±•æ€§æå¥½ï¼Œå¯è¾¾åˆ°æ•°åƒäº¿ä¸ªå¯è®­ç»ƒå‚æ•° [Brown et al., 2020]ã€‚æˆ‘ä»¬å°†åœ¨Â§7.1ä¸­å›åˆ°å®ƒä»¬åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **GPT**ï¼šGPTæ˜¯ä¸€ç§åŸºäºTransformerçš„è‡ªå›å½’æ¨¡å‹ï¼Œå¹¿æ³›ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚GPT-3æ˜¯å½“å‰æœ€å¤§çš„è¯­è¨€æ¨¡å‹ä¹‹ä¸€ï¼Œå…·æœ‰1750äº¿ä¸ªå‚æ•°ã€‚

#### Vision Transformer

Transformers have been put to use for image classification with the Vision Transformer (ViT) model [Dosovitskiy et al., 2020] (see Figure 5.9).

Transformerå·²è¢«ç”¨äºå›¾åƒåˆ†ç±»ï¼Œé€šè¿‡Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ [Dosovitskiy et al., 2020]ï¼ˆè§å›¾5.9ï¼‰ã€‚

It splits the three-channel input image into \( M \) patches of resolution \( P \times P \), which are then flattened to create a sequence of vectors \( X_1, \ldots, X_M \) of shape \( M \times 3P^2 \). This sequence is multiplied by a trainable matrix \( W^e \) of shape \( 3P^2 \times D \) to map it to an \( M \times D \) sequence, to which is concatenated one trainable vector \( E_0 \). The resulting \((M+1) \times D\) sequence \( E_0, \ldots, E_M \) is then processed through multiple self-attention blocks. See Â§ 5.3 and Figure 5.6.

å®ƒå°†ä¸‰é€šé“è¾“å…¥å›¾åƒåˆ†å‰²ä¸º\( M \)ä¸ªåˆ†è¾¨ç‡ä¸º\( P \times P \)çš„è¡¥ä¸ï¼Œç„¶åå°†å®ƒä»¬å±•å¹³ä»¥åˆ›å»ºå½¢çŠ¶ä¸º\( M \times 3P^2 \)çš„å‘é‡åºåˆ—\( X_1, \ldots, X_M \)ã€‚è¯¥åºåˆ—ä¹˜ä»¥å½¢çŠ¶ä¸º\( 3P^2 \times D \)çš„å¯è®­ç»ƒçŸ©é˜µ\( W^e \)ï¼Œå°†å…¶æ˜ å°„åˆ°\( M \times D \)åºåˆ—ï¼Œå¹¶è¿æ¥ä¸€ä¸ªå¯è®­ç»ƒå‘é‡\( E_0 \)ã€‚ç”Ÿæˆçš„\((M+1) \times D\)åºåˆ—\( E_0, \ldots, E_M \)ç„¶åé€šè¿‡å¤šä¸ªè‡ªæ³¨æ„åŠ›å—è¿›è¡Œå¤„ç†ã€‚å‚è§Â§5.3å’Œå›¾5.6ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **Vision Transformerï¼ˆViTï¼‰**ï¼šViTå°†å›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªè¡¥ä¸ï¼Œå¹¶å°†è¿™äº›è¡¥ä¸ä½œä¸ºåºåˆ—è¾“å…¥Transformeræ¨¡å‹ï¼Œä»è€Œåº”ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

---

### æ€»ç»“ï¼š
- **å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰**æ˜¯æœ€ç®€å•çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé€‚ç”¨äºä½ç»´æ•°æ®çš„å¤„ç†ã€‚
- **å·ç§¯ç½‘ç»œï¼ˆConvNetï¼‰**é€šè¿‡å·ç§¯å±‚å’Œæ± åŒ–å±‚æå–å›¾åƒç‰¹å¾ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚LeNetã€AlexNetå’ŒVGGæ˜¯ç»å…¸çš„å·ç§¯ç½‘ç»œæ¶æ„ã€‚
- **æ®‹å·®ç½‘ç»œï¼ˆResNetsï¼‰**é€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥è§£å†³äº†æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå…è®¸è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œã€‚
- **Transformer**æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ã€‚GPTå’ŒViTæ˜¯Transformerçš„å˜ä½“ï¼Œåˆ†åˆ«ç”¨äºæ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒåˆ†ç±»ã€‚



å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 6çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 6 Prediction

#### 6 é¢„æµ‹

A first category of applications, such as face recognition, sentiment analysis, object detection, or speech recognition, requires predicting an unknown value from an available signal.

ç¬¬ä¸€ç±»åº”ç”¨ï¼Œå¦‚äººè„¸è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€ç›®æ ‡æ£€æµ‹æˆ–è¯­éŸ³è¯†åˆ«ï¼Œéœ€è¦ä»å¯ç”¨ä¿¡å·ä¸­é¢„æµ‹æœªçŸ¥å€¼ã€‚

---

### Chapter 6.1 Image Denoising

#### 6.1 å›¾åƒå»å™ª

A direct application of deep models to image processing is to recover from degradation by utilizing the redundancy in the statistical structure of images. The petals of a sunflower in a grayscale picture can be colored with high confidence, and the texture of a geometric shape such as a table on a low-light, grainy picture can be corrected by averaging it over a large area likely to be uniform.

æ·±åº¦æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä¸­çš„ç›´æ¥åº”ç”¨æ˜¯é€šè¿‡åˆ©ç”¨å›¾åƒç»Ÿè®¡ç»“æ„ä¸­çš„å†—ä½™æ¥æ¢å¤é€€åŒ–ã€‚ä¾‹å¦‚ï¼Œç°åº¦å›¾åƒä¸­çš„å‘æ—¥è‘µèŠ±ç“£å¯ä»¥é«˜ç½®ä¿¡åº¦åœ°ç€è‰²ï¼Œè€Œä½å…‰ã€é¢—ç²’çŠ¶å›¾ç‰‡ä¸­çš„å‡ ä½•å½¢çŠ¶ï¼ˆå¦‚æ¡Œå­ï¼‰çš„çº¹ç†å¯ä»¥é€šè¿‡åœ¨å¯èƒ½å‡åŒ€çš„å¤§åŒºåŸŸä¸Šå¹³å‡æ¥æ ¡æ­£ã€‚

A denoising autoencoder is a model that takes a degraded signal \( \widetilde{X} \) as input and computes an estimate of the original signal \( X \). For images, it is a convolutional network that may integrate skip-connections, in particular to combine representations at the same resolution obtained early and late in the model, as well as attention layers to facilitate taking into account elements that are far away from each other.

å»å™ªè‡ªç¼–ç å™¨æ˜¯ä¸€ç§æ¨¡å‹ï¼Œå®ƒä»¥é€€åŒ–ä¿¡å·\( \widetilde{X} \)ä½œä¸ºè¾“å…¥ï¼Œå¹¶è®¡ç®—åŸå§‹ä¿¡å·\( X \)çš„ä¼°è®¡å€¼ã€‚å¯¹äºå›¾åƒï¼Œå®ƒæ˜¯ä¸€ä¸ªå·ç§¯ç½‘ç»œï¼Œå¯èƒ½é›†æˆè·³è·ƒè¿æ¥ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†ç»“åˆæ¨¡å‹æ—©æœŸå’ŒåæœŸè·å¾—çš„ç›¸åŒåˆ†è¾¨ç‡çš„è¡¨ç¤ºï¼Œä»¥åŠæ³¨æ„åŠ›å±‚ï¼Œä»¥ä¾¿äºè€ƒè™‘å½¼æ­¤ç›¸è·è¾ƒè¿œçš„å…ƒç´ ã€‚

Such a model is trained by collecting a large number of clean samples paired with their degraded inputs. The latter can be captured in degraded conditions, such as low-light or inadequate focus, or generated algorithmically, for instance, by converting the clean sample to grayscale, reducing its size, or aggressively compressing it with a lossy compression method.

è¿™ç§æ¨¡å‹é€šè¿‡æ”¶é›†å¤§é‡å¹²å‡€æ ·æœ¬åŠå…¶é€€åŒ–è¾“å…¥è¿›è¡Œè®­ç»ƒã€‚åè€…å¯ä»¥åœ¨é€€åŒ–æ¡ä»¶ä¸‹æ•è·ï¼Œä¾‹å¦‚ä½å…‰æˆ–å¯¹ç„¦ä¸å½“ï¼Œæˆ–è€…é€šè¿‡ç®—æ³•ç”Ÿæˆï¼Œä¾‹å¦‚ï¼Œå°†å¹²å‡€æ ·æœ¬è½¬æ¢ä¸ºç°åº¦ã€ç¼©å°å…¶å°ºå¯¸æˆ–ä½¿ç”¨æœ‰æŸå‹ç¼©æ–¹æ³•è¿›è¡Œå‹ç¼©ã€‚

The standard training procedure for denoising autoencoders uses the MSE loss summed across all pixels, in which case the model aims at computing the best average clean picture, given the degraded one, that is \( \mathbb{E}[X | \bar{X}] \). This quantity may be problematic when \( X \) is not completely determined by \( \bar{X} \), in which case some parts of the generated signal may be an unrealistic, blurry average.

å»å™ªè‡ªç¼–ç å™¨çš„æ ‡å‡†è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨æ‰€æœ‰åƒç´ ä¸Šçš„MSEæŸå¤±ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ—¨åœ¨è®¡ç®—ç»™å®šé€€åŒ–å›¾åƒçš„æœ€ä½³å¹³å‡å¹²å‡€å›¾åƒï¼Œå³\( \mathbb{E}[X | \bar{X}] \)ã€‚å½“\( X \)ä¸å®Œå…¨ç”±\( \bar{X} \)å†³å®šæ—¶ï¼Œè¿™ä¸ªé‡å¯èƒ½ä¼šæœ‰é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”Ÿæˆä¿¡å·çš„æŸäº›éƒ¨åˆ†å¯èƒ½æ˜¯ä¸ç°å®çš„æ¨¡ç³Šå¹³å‡å€¼ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å›¾åƒå»å™ªï¼ˆImage Denoisingï¼‰**ï¼šå›¾åƒå»å™ªæ˜¯é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ä»é€€åŒ–å›¾åƒä¸­æ¢å¤åŸå§‹å›¾åƒçš„è¿‡ç¨‹ã€‚å»å™ªè‡ªç¼–ç å™¨æ˜¯ä¸€ç§å¸¸ç”¨çš„å»å™ªæ¨¡å‹ã€‚
- **å»å™ªè‡ªç¼–ç å™¨ï¼ˆDenoising Autoencoderï¼‰**ï¼šå»å™ªè‡ªç¼–ç å™¨é€šè¿‡è®­ç»ƒæ¨¡å‹ä»é€€åŒ–å›¾åƒä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œé€šå¸¸ä½¿ç”¨å·ç§¯ç½‘ç»œå’Œè·³è·ƒè¿æ¥æ¥æé«˜æ€§èƒ½ã€‚

---

### Chapter 6.2 Image Classification

#### 6.2 å›¾åƒåˆ†ç±»

Image classification is the simplest strategy for extracting semantics from an image and consists of predicting a class from a finite, predefined number of classes, given an input image.

å›¾åƒåˆ†ç±»æ˜¯ä»å›¾åƒä¸­æå–è¯­ä¹‰çš„æœ€ç®€å•ç­–ç•¥ï¼Œå®ƒæ¶‰åŠä»æœ‰é™çš„é¢„å®šä¹‰ç±»åˆ«ä¸­é¢„æµ‹ä¸€ä¸ªç±»åˆ«ï¼Œç»™å®šè¾“å…¥å›¾åƒã€‚

The standard models for this task are convolutional networks, such as ResNets (see Â§ 5.2), and attention-based models such as ViT (see Â§ 5.3). These models generate a vector of logits with as many dimensions as there are classes.

æ­¤ä»»åŠ¡çš„æ ‡å‡†æ¨¡å‹æ˜¯å·ç§¯ç½‘ç»œï¼Œå¦‚ResNetsï¼ˆè§Â§5.2ï¼‰ï¼Œä»¥åŠåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œå¦‚ViTï¼ˆè§Â§5.3ï¼‰ã€‚è¿™äº›æ¨¡å‹ç”Ÿæˆä¸€ä¸ªå…·æœ‰ä¸ç±»åˆ«æ•°é‡ç›¸åŒç»´åº¦çš„logitså‘é‡ã€‚

The training procedure simply minimizes the cross-entropy loss (see Â§ 3.1). Usually, performance can be improved with data augmentation, which consists of modifying the training samples with hand-designed random transformations that do not change the semantic content of the image, such as cropping, scaling, mirroring, or color changes.

è®­ç»ƒè¿‡ç¨‹ç®€å•åœ°æœ€å°åŒ–äº¤å‰ç†µæŸå¤±ï¼ˆè§Â§3.1ï¼‰ã€‚é€šå¸¸ï¼Œå¯ä»¥é€šè¿‡æ•°æ®å¢å¼ºæ¥æé«˜æ€§èƒ½ï¼Œæ•°æ®å¢å¼ºåŒ…æ‹¬ä½¿ç”¨æ‰‹å·¥è®¾è®¡çš„éšæœºå˜æ¢ä¿®æ”¹è®­ç»ƒæ ·æœ¬ï¼Œè¿™äº›å˜æ¢ä¸ä¼šæ”¹å˜å›¾åƒçš„è¯­ä¹‰å†…å®¹ï¼Œä¾‹å¦‚è£å‰ªã€ç¼©æ”¾ã€é•œåƒæˆ–é¢œè‰²å˜åŒ–ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å›¾åƒåˆ†ç±»ï¼ˆImage Classificationï¼‰**ï¼šå›¾åƒåˆ†ç±»æ˜¯å°†è¾“å…¥å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«ä¸­çš„ä»»åŠ¡ã€‚å·ç§¯ç½‘ç»œå’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹æ˜¯å¸¸ç”¨çš„å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚
- **æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰**ï¼šæ•°æ®å¢å¼ºé€šè¿‡å¯¹è®­ç»ƒå›¾åƒè¿›è¡Œéšæœºå˜æ¢ï¼ˆå¦‚è£å‰ªã€ç¼©æ”¾ã€é•œåƒç­‰ï¼‰æ¥å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

### Chapter 6.3 Object Detection

#### 6.3 ç›®æ ‡æ£€æµ‹

A more complex task for image understanding is object detection, in which the objective is, given an input image, to predict the classes and positions of objects of interest.

å›¾åƒç†è§£ä¸­æ›´å¤æ‚çš„ä»»åŠ¡æ˜¯ç›®æ ‡æ£€æµ‹ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨ç»™å®šè¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹æ„Ÿå…´è¶£å¯¹è±¡çš„ç±»åˆ«å’Œä½ç½®ã€‚

An object position is formalized as the four coordinates \((x_1, y_1, x_2, y_2)\) of a rectangular bounding box, and the ground truth associated with each training image is a list of such bounding boxes, each labeled with the class of the object contained therein.

å¯¹è±¡ä½ç½®è¢«å½¢å¼åŒ–ä¸ºçŸ©å½¢è¾¹ç•Œæ¡†çš„å››ä¸ªåæ ‡\((x_1, y_1, x_2, y_2)\)ï¼Œæ¯ä¸ªè®­ç»ƒå›¾åƒçš„ground truthæ˜¯æ­¤ç±»è¾¹ç•Œæ¡†çš„åˆ—è¡¨ï¼Œæ¯ä¸ªè¾¹ç•Œæ¡†éƒ½æ ‡æœ‰å…¶ä¸­åŒ…å«çš„å¯¹è±¡çš„ç±»åˆ«ã€‚

The standard approach to solve this task, for instance, by the Single Shot Detector (SSD) [Liu et al., 2015]), is to use a convolutional neural network that produces a sequence of image representations \(Z_s\) of size \(D_s \times H_s \times W_s\), \(s = 1, \ldots, S\), with decreasing spatial resolution \(H_s \times W_s\) down to \(1 \times 1\) for \(s = S\) (see Figure 6.1). Each of these tensors covers the input image in full, so the \(h, w\) indices correspond to a partitioning of the image lattice into regular squares that gets coarser when \(s\) increases.

è§£å†³æ­¤ä»»åŠ¡çš„æ ‡å‡†æ–¹æ³•ï¼Œä¾‹å¦‚é€šè¿‡Single Shot Detectorï¼ˆSSDï¼‰[Liu et al., 2015]ï¼Œæ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œç”Ÿæˆä¸€ç³»åˆ—å›¾åƒè¡¨ç¤º\(Z_s\)ï¼Œå¤§å°ä¸º\(D_s \times H_s \times W_s\)ï¼Œ\(s = 1, \ldots, S\)ï¼Œç©ºé—´åˆ†è¾¨ç‡\(H_s \times W_s\)éšç€\(s\)çš„å¢åŠ è€Œé™ä½ï¼Œç›´åˆ°\(s = S\)æ—¶ä¸º\(1 \times 1\)ï¼ˆè§å›¾6.1ï¼‰ã€‚è¿™äº›å¼ é‡ä¸­çš„æ¯ä¸€ä¸ªéƒ½å®Œå…¨è¦†ç›–è¾“å…¥å›¾åƒï¼Œå› æ­¤\(h, w\)ç´¢å¼•å¯¹åº”äºå°†å›¾åƒç½‘æ ¼åˆ’åˆ†ä¸ºè§„åˆ™çš„æ­£æ–¹å½¢ï¼Œéšç€\(s\)çš„å¢åŠ ï¼Œè¿™äº›æ­£æ–¹å½¢å˜å¾—æ›´ç²—ç³™ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **ç›®æ ‡æ£€æµ‹ï¼ˆObject Detectionï¼‰**ï¼šç›®æ ‡æ£€æµ‹æ˜¯åœ¨å›¾åƒä¸­å®šä½å¹¶åˆ†ç±»å¤šä¸ªå¯¹è±¡çš„ä»»åŠ¡ã€‚SSDæ˜¯ä¸€ç§å¸¸ç”¨çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡å·ç§¯ç½‘ç»œç”Ÿæˆå¤šå°ºåº¦çš„ç‰¹å¾å›¾æ¥æ£€æµ‹å¯¹è±¡ã€‚
- **è¾¹ç•Œæ¡†ï¼ˆBounding Boxï¼‰**ï¼šè¾¹ç•Œæ¡†ç”¨äºè¡¨ç¤ºå›¾åƒä¸­å¯¹è±¡çš„ä½ç½®ï¼Œé€šå¸¸ç”±å››ä¸ªåæ ‡ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰å®šä¹‰ã€‚

---

### Chapter 6.4 Semantic Segmentation

#### 6.4 è¯­ä¹‰åˆ†å‰²

The finest-grain prediction task for image understanding is semantic segmentation, which consists of predicting, for each pixel, the class of the object to which it belongs. This can be achieved with a standard convolutional neural network that outputs a convolutional map with as many channels as classes, carrying the estimated logits for every pixel.

å›¾åƒç†è§£ä¸­æœ€ç»†ç²’åº¦çš„é¢„æµ‹ä»»åŠ¡æ˜¯è¯­ä¹‰åˆ†å‰²ï¼Œå®ƒæ¶‰åŠä¸ºæ¯ä¸ªåƒç´ é¢„æµ‹å…¶æ‰€å±å¯¹è±¡çš„ç±»åˆ«ã€‚è¿™å¯ä»¥é€šè¿‡æ ‡å‡†çš„å·ç§¯ç¥ç»ç½‘ç»œå®ç°ï¼Œè¯¥ç½‘ç»œè¾“å‡ºå…·æœ‰ä¸ç±»åˆ«æ•°é‡ç›¸åŒé€šé“çš„å·ç§¯å›¾ï¼Œæºå¸¦æ¯ä¸ªåƒç´ çš„ä¼°è®¡logitsã€‚

While a standard residual network, for instance, can generate a dense output of the same resolution as its input, as for object detection, this task requires operating at multiple scales. This is necessary so that any object, or sufficiently informative sub-part, regardless of its size, is captured somewhere in the model by the feature representation at a single tensor position. Hence, standard architectures for this task downscale the image with a series of convolutional layers to increase the receptive field of the activations, and re-upscale it with a series of transposed convolutional layers, or other upscaling methods such as bilinear interpolation, to make the prediction at high resolution.

è™½ç„¶æ ‡å‡†çš„æ®‹å·®ç½‘ç»œå¯ä»¥ç”Ÿæˆä¸è¾“å…¥åˆ†è¾¨ç‡ç›¸åŒçš„å¯†é›†è¾“å‡ºï¼Œå¦‚ç›®æ ‡æ£€æµ‹ï¼Œä½†æ­¤ä»»åŠ¡éœ€è¦åœ¨å¤šä¸ªå°ºåº¦ä¸Šæ“ä½œã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œä»¥ä¾¿ä»»ä½•å¯¹è±¡æˆ–è¶³å¤Ÿä¿¡æ¯çš„å­éƒ¨åˆ†ï¼Œæ— è®ºå…¶å¤§å°å¦‚ä½•ï¼Œéƒ½èƒ½åœ¨æ¨¡å‹ä¸­çš„æŸä¸ªä½ç½®é€šè¿‡å•ä¸ªå¼ é‡ä½ç½®çš„ç‰¹å¾è¡¨ç¤ºæ•è·ã€‚å› æ­¤ï¼Œæ­¤ä»»åŠ¡çš„æ ‡å‡†æ¶æ„é€šè¿‡ä¸€ç³»åˆ—å·ç§¯å±‚ç¼©å°å›¾åƒä»¥å¢åŠ æ¿€æ´»çš„æ„Ÿå—é‡ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—è½¬ç½®å·ç§¯å±‚æˆ–å…¶ä»–ä¸Šé‡‡æ ·æ–¹æ³•ï¼ˆå¦‚åŒçº¿æ€§æ’å€¼ï¼‰é‡æ–°æ”¾å¤§å›¾åƒï¼Œä»¥åœ¨é«˜åˆ†è¾¨ç‡ä¸‹è¿›è¡Œé¢„æµ‹ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è¯­ä¹‰åˆ†å‰²ï¼ˆSemantic Segmentationï¼‰**ï¼šè¯­ä¹‰åˆ†å‰²æ˜¯ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾çš„ä»»åŠ¡ã€‚ä¸ç›®æ ‡æ£€æµ‹ä¸åŒï¼Œè¯­ä¹‰åˆ†å‰²ä¸åŒºåˆ†åŒä¸€ç±»åˆ«çš„ä¸åŒå®ä¾‹ã€‚
- **å¤šå°ºåº¦æ“ä½œï¼ˆMulti-scale Operationï¼‰**ï¼šè¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸éœ€è¦åœ¨å¤šä¸ªå°ºåº¦ä¸Šæ“ä½œï¼Œä»¥æ•æ‰ä¸åŒå¤§å°çš„å¯¹è±¡ã€‚é€šè¿‡ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ“ä½œï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹è¿›è¡Œé¢„æµ‹ã€‚

---

### æ€»ç»“ï¼š
- **å›¾åƒå»å™ª**æ˜¯é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ä»é€€åŒ–å›¾åƒä¸­æ¢å¤åŸå§‹å›¾åƒçš„è¿‡ç¨‹ï¼Œå»å™ªè‡ªç¼–ç å™¨æ˜¯å¸¸ç”¨çš„å»å™ªæ¨¡å‹ã€‚
- **å›¾åƒåˆ†ç±»**æ˜¯å°†è¾“å…¥å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«ä¸­çš„ä»»åŠ¡ï¼Œå·ç§¯ç½‘ç»œå’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹æ˜¯å¸¸ç”¨çš„å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚
- **ç›®æ ‡æ£€æµ‹**æ˜¯åœ¨å›¾åƒä¸­å®šä½å¹¶åˆ†ç±»å¤šä¸ªå¯¹è±¡çš„ä»»åŠ¡ï¼ŒSSDæ˜¯ä¸€ç§å¸¸ç”¨çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚
- **è¯­ä¹‰åˆ†å‰²**æ˜¯ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾çš„ä»»åŠ¡ï¼Œé€šå¸¸éœ€è¦åœ¨å¤šä¸ªå°ºåº¦ä¸Šæ“ä½œä»¥æ•æ‰ä¸åŒå¤§å°çš„å¯¹è±¡ã€‚

è¿™äº›çŸ¥è¯†ç‚¹æ˜¯ç†è§£æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­åº”ç”¨çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬æœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–å›¾åƒå¤„ç†æ¨¡å‹ã€‚

---

å¥½çš„ï¼Œæˆ‘å°†ç»§ç»­ç¿»è¯‘æ¥ä¸‹æ¥çš„ç« èŠ‚ã€‚ä»¥ä¸‹æ˜¯Chapter 6.5åˆ°Chapter 6.7çš„ç¿»è¯‘å’ŒçŸ¥è¯†ç‚¹è®²è§£ï¼š

---

### Chapter 6.5 Speech Recognition

#### 6.5 è¯­éŸ³è¯†åˆ«

Speech recognition consists of converting a sound sample into a sequence of words. There have been plenty of approaches to this problem historically, but a conceptually simple and recent one proposed by Radford et al. [2022] consists of casting it as a sequence-to-sequence translation and then solving it with a standard attention-based Transformer, as described in Â§ 5.3.

è¯­éŸ³è¯†åˆ«æ¶‰åŠå°†å£°éŸ³æ ·æœ¬è½¬æ¢ä¸ºå•è¯åºåˆ—ã€‚å†å²ä¸Šæœ‰å¾ˆå¤šæ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†Radford et al. [2022]æå‡ºçš„ä¸€ä¸ªæ¦‚å¿µä¸Šç®€å•ä¸”æœ€è¿‘çš„æ–¹æ³•æ˜¯å°†å®ƒè§†ä¸ºåºåˆ—åˆ°åºåˆ—çš„ç¿»è¯‘ï¼Œç„¶åç”¨æ ‡å‡†çš„åŸºäºæ³¨æ„åŠ›çš„Transformeræ¥è§£å†³ï¼Œå¦‚Â§5.3æ‰€è¿°ã€‚

Their model first converts the sound signal into a spectrogram, which is a one-dimensional series \( T \times D \), that encodes at every time step a vector of energies in \( D \) frequency bands. The associated text is encoded with the BPE tokenizer (see Â§ 3.2).

ä»–ä»¬çš„æ¨¡å‹é¦–å…ˆå°†å£°éŸ³ä¿¡å·è½¬æ¢ä¸ºé¢‘è°±å›¾ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€ç»´åºåˆ—\( T \times D \)ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç¼–ç \( D \)ä¸ªé¢‘å¸¦ä¸­çš„èƒ½é‡å‘é‡ã€‚ç›¸å…³çš„æ–‡æœ¬ä½¿ç”¨BPEåˆ†è¯å™¨è¿›è¡Œç¼–ç ï¼ˆè§Â§3.2ï¼‰ã€‚

The spectrogram is processed through a few 1D convolutional layers, and the resulting representation is fed into the encoder of the Transformer. The decoder directly generates a discrete sequence of tokens, that correspond to one of the possible tasks considered during training. Multiple objectives are considered: transcription of English or non-English text, translation from any language to English, or detection of non-speech sequences, such as background music or ambient noise.

é¢‘è°±å›¾é€šè¿‡å‡ ä¸ª1Då·ç§¯å±‚è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆçš„è¡¨ç¤ºè¢«è¾“å…¥åˆ°Transformerçš„ç¼–ç å™¨ä¸­ã€‚è§£ç å™¨ç›´æ¥ç”Ÿæˆç¦»æ•£çš„æ ‡è®°åºåˆ—ï¼Œå¯¹åº”äºè®­ç»ƒæœŸé—´è€ƒè™‘çš„å…¶ä¸­ä¸€ä¸ªå¯èƒ½ä»»åŠ¡ã€‚è€ƒè™‘äº†å¤šä¸ªç›®æ ‡ï¼šè‹±è¯­æˆ–éè‹±è¯­æ–‡æœ¬çš„è½¬å½•ã€ä»ä»»ä½•è¯­è¨€åˆ°è‹±è¯­çš„ç¿»è¯‘ï¼Œæˆ–éè¯­éŸ³åºåˆ—çš„æ£€æµ‹ï¼Œå¦‚èƒŒæ™¯éŸ³ä¹æˆ–ç¯å¢ƒå™ªå£°ã€‚

This approach allows leveraging extremely large datasets that combine multiple types of sound sources with diverse ground truths.

è¿™ç§æ–¹æ³•å…è®¸åˆ©ç”¨ç»“åˆäº†å¤šç§å£°éŸ³æºå’Œå¤šæ ·åŒ–ground truthçš„æå¤§æ•°æ®é›†ã€‚

It is noteworthy that even though the ultimate goal of this approach is to produce a translation as deterministic as possible given the input signal, it is formally the sampling of a text distribution conditioned on a sound sample, hence a synthesis process. The decoder is, in fact, extremely similar to the generative model of Â§ 7.1.

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡è¿™ç§æ–¹æ³•çš„æœ€ç»ˆç›®æ ‡æ˜¯å°½å¯èƒ½ç¡®å®šæ€§åœ°ç”Ÿæˆç¿»è¯‘ï¼Œä½†å®ƒåœ¨å½¢å¼ä¸Šæ˜¯åŸºäºå£°éŸ³æ ·æœ¬çš„æ–‡æœ¬åˆ†å¸ƒçš„é‡‡æ ·ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªåˆæˆè¿‡ç¨‹ã€‚è§£ç å™¨å®é™…ä¸Šä¸Â§7.1ä¸­çš„ç”Ÿæˆæ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **è¯­éŸ³è¯†åˆ«ï¼ˆSpeech Recognitionï¼‰**ï¼šè¯­éŸ³è¯†åˆ«æ˜¯å°†å£°éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬çš„ä»»åŠ¡ã€‚Transformeræ¨¡å‹é€šè¿‡å°†å£°éŸ³ä¿¡å·è½¬æ¢ä¸ºé¢‘è°±å›¾ï¼Œå¹¶ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œåºåˆ—åˆ°åºåˆ—çš„ç¿»è¯‘ã€‚
- **é¢‘è°±å›¾ï¼ˆSpectrogramï¼‰**ï¼šé¢‘è°±å›¾æ˜¯å£°éŸ³ä¿¡å·çš„æ—¶é¢‘è¡¨ç¤ºï¼Œé€šå¸¸ç”¨äºè¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­ã€‚
- **BPEåˆ†è¯å™¨ï¼ˆByte Pair Encoding Tokenizerï¼‰**ï¼šBPEåˆ†è¯å™¨æ˜¯ä¸€ç§å°†æ–‡æœ¬åˆ†è§£ä¸ºå­è¯å•å…ƒçš„æ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

---

### Chapter 6.6 Text-Image Representations

#### 6.6 æ–‡æœ¬-å›¾åƒè¡¨ç¤º

A powerful approach to image understanding consists of learning consistent image and text representations, such that an image, or a textual description of it, would be mapped to the same feature vector.

å›¾åƒç†è§£çš„ä¸€ç§å¼ºå¤§æ–¹æ³•æ˜¯å­¦ä¹ ä¸€è‡´çš„å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œä½¿å¾—å›¾åƒæˆ–å…¶æ–‡æœ¬æè¿°è¢«æ˜ å°„åˆ°ç›¸åŒçš„ç‰¹å¾å‘é‡ã€‚

The \textit{Contrastive Language-Image Pre-training (CLIP)} proposed by Radford et al. [2021] combines an image encoder \( f \), which is a ViT, and a text encoder \( g \), which is a GPT. See Â§ 5.3 for both.

Radford et al. [2021]æå‡ºçš„\textit{å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰}ç»“åˆäº†ä¸€ä¸ªå›¾åƒç¼–ç å™¨\( f \)ï¼ˆå³ViTï¼‰å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨\( g \)ï¼ˆå³GPTï¼‰ã€‚å‚è§Â§5.3ã€‚

To repurpose a GPT as a text encoder, instead of a standard autoregressive model, they add an "end of sentence" token to the input sequence, and use the representation of this token in the last layer as the embedding. Its dimension is between 512 and 1024, depending on the configuration.

ä¸ºäº†å°†GPTé‡æ–°ç”¨ä½œæ–‡æœ¬ç¼–ç å™¨ï¼Œè€Œä¸æ˜¯æ ‡å‡†çš„è‡ªå›å½’æ¨¡å‹ï¼Œä»–ä»¬åœ¨è¾“å…¥åºåˆ—ä¸­æ·»åŠ äº†ä¸€ä¸ªâ€œå¥å­ç»“æŸâ€æ ‡è®°ï¼Œå¹¶ä½¿ç”¨æœ€åä¸€å±‚ä¸­è¯¥æ ‡è®°çš„è¡¨ç¤ºä½œä¸ºåµŒå…¥ã€‚å…¶ç»´åº¦åœ¨512åˆ°1024ä¹‹é—´ï¼Œå…·ä½“å–å†³äºé…ç½®ã€‚

Those two models are trained from scratch using a dataset of 400 million image-text pairs \((i_k, t_k)\) collected from the internet. The training procedure follows the standard mini-batch stochastic gradient descent approach but relies on a contrastive loss. The embeddings are computed for every image and every text of the \( N \) pairs in the mini-batch, and a cosine similarity measure is computed not only between text and image embeddings from each pair, but also across pairs, resulting in an \( N \times N \) matrix of similarity scores:

è¿™ä¸¤ä¸ªæ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨ä»äº’è”ç½‘æ”¶é›†çš„4äº¿ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹\((i_k, t_k)\)çš„æ•°æ®é›†ã€‚è®­ç»ƒè¿‡ç¨‹éµå¾ªæ ‡å‡†çš„å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œä½†ä¾èµ–äºå¯¹æ¯”æŸå¤±ã€‚ä¸ºå°æ‰¹é‡ä¸­çš„æ¯å¯¹å›¾åƒå’Œæ–‡æœ¬è®¡ç®—åµŒå…¥ï¼Œå¹¶è®¡ç®—æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä¸ä»…åœ¨æ¯ä¸ªå¯¹ä¹‹é—´ï¼Œè¿˜åœ¨å¯¹ä¹‹é—´è®¡ç®—ï¼Œç”Ÿæˆä¸€ä¸ª\( N \times N \)çš„ç›¸ä¼¼åº¦åˆ†æ•°çŸ©é˜µï¼š

\[ l_{m,n} = f(t_m) \cdot g(t_n), \, m = 1, \ldots, N, n = 1, \ldots, N. \]

The model is trained with cross-entropy so that, \(\forall n\) the values \( l_1, n, \ldots, l_N, n \) interpreted as logit scores predict \( n \), and similarly for \( l_{n,1}, \ldots, l_{n,N} \). This means that \(\forall n, m, \, \text{s.t. } n \neq m \) the similarity \( l_{n,n} \) is unambiguously greater than both \( l_{n,m} \) and \( l_{m,n} \).

æ¨¡å‹ä½¿ç”¨äº¤å‰ç†µè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤\(\forall n\)ï¼Œå€¼\( l_1, n, \ldots, l_N, n \)è¢«è§£é‡Šä¸ºlogitåˆ†æ•°ï¼Œé¢„æµ‹\( n \)ï¼ŒåŒæ ·é€‚ç”¨äº\( l_{n,1}, \ldots, l_{n,N} \)ã€‚è¿™æ„å‘³ç€\(\forall n, m, \, \text{s.t. } n \neq m \)ï¼Œç›¸ä¼¼åº¦\( l_{n,n} \)æ˜ç¡®å¤§äº\( l_{n,m} \)å’Œ\( l_{m,n} \)ã€‚

When it has been trained, this model can be used to do zero-shot prediction, that is, classifying a signal in the absence of training examples by defining a series of candidate classes with text descriptions, and computing the similarity of the embedding of an image with the embedding of each of those descriptions (see Figure 6.4).

è®­ç»ƒå®Œæˆåï¼Œè¯¥æ¨¡å‹å¯ä»¥ç”¨äºé›¶æ ·æœ¬é¢„æµ‹ï¼Œå³é€šè¿‡å®šä¹‰ä¸€ç³»åˆ—å¸¦æœ‰æ–‡æœ¬æè¿°çš„å€™é€‰ç±»åˆ«ï¼Œå¹¶è®¡ç®—å›¾åƒåµŒå…¥ä¸æ¯ä¸ªæè¿°åµŒå…¥çš„ç›¸ä¼¼åº¦ï¼Œåœ¨æ²¡æœ‰è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹å¯¹ä¿¡å·è¿›è¡Œåˆ†ç±»ï¼ˆè§å›¾6.4ï¼‰ã€‚

Additionally, since the textual descriptions are often detailed, such a model has to capture a richer representation of images and pick up cues beyond what is necessary for instance for classification. This translates to excellent performance on challenging datasets such as ImageNet Adversarial [Hendrycks et al., 2019] which was specifically designed to degrade or erase cues on which standard predictors rely.

æ­¤å¤–ï¼Œç”±äºæ–‡æœ¬æè¿°é€šå¸¸å¾ˆè¯¦ç»†ï¼Œè¿™æ ·çš„æ¨¡å‹å¿…é¡»æ•æ‰æ›´ä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºï¼Œå¹¶æå–è¶…å‡ºåˆ†ç±»æ‰€éœ€çš„çº¿ç´¢ã€‚è¿™è½¬åŒ–ä¸ºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼ˆå¦‚ImageNet Adversarial [Hendrycks et al., 2019]ï¼‰ä¸Šçš„å‡ºè‰²æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†ä¸“é—¨è®¾è®¡ç”¨äºé™è§£æˆ–æ¶ˆé™¤æ ‡å‡†é¢„æµ‹å™¨ä¾èµ–çš„çº¿ç´¢ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰**ï¼šCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç›¸åŒçš„åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé›¶æ ·æœ¬é¢„æµ‹ã€‚
- **é›¶æ ·æœ¬é¢„æµ‹ï¼ˆZero-shot Predictionï¼‰**ï¼šé›¶æ ·æœ¬é¢„æµ‹æ˜¯æŒ‡åœ¨æ²¡æœ‰ç‰¹å®šç±»åˆ«è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ–‡æœ¬æè¿°å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚

---

### Chapter 6.7 Reinforcement Learning

#### 6.7 å¼ºåŒ–å­¦ä¹ 

Many problems, such as strategy games or robotic control, can be formalized with a discrete-time state process \( S_t \) and reward process \( R_t \) that can be modulated by choosing actions \( A_t \). If \( S_t \) is Markovian, meaning that it carries alone as much information about the future as all the past states until that instant, such an object is a Markovian Decision Process (MDP).

è®¸å¤šé—®é¢˜ï¼Œå¦‚ç­–ç•¥æ¸¸æˆæˆ–æœºå™¨äººæ§åˆ¶ï¼Œå¯ä»¥ç”¨ç¦»æ•£æ—¶é—´çŠ¶æ€è¿‡ç¨‹\( S_t \)å’Œå¥–åŠ±è¿‡ç¨‹\( R_t \)æ¥å½¢å¼åŒ–ï¼Œè¿™äº›è¿‡ç¨‹å¯ä»¥é€šè¿‡é€‰æ‹©åŠ¨ä½œ\( A_t \)æ¥è°ƒèŠ‚ã€‚å¦‚æœ\( S_t \)æ˜¯é©¬å°”å¯å¤«çš„ï¼Œæ„å‘³ç€å®ƒå•ç‹¬æºå¸¦äº†å…³äºæœªæ¥çš„æ‰€æœ‰ä¿¡æ¯ï¼Œç›´åˆ°è¯¥æ—¶åˆ»çš„æ‰€æœ‰è¿‡å»çŠ¶æ€ï¼Œè¿™æ ·çš„å¯¹è±¡å°±æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚

Given an MDP, the objective is classically to find a policy \(\pi\) such that \( A_t = \pi(S_t) \) maximizes the expectation of the return, which is an accumulated discounted reward:

ç»™å®šä¸€ä¸ªMDPï¼Œç»å…¸ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥\(\pi\)ï¼Œä½¿å¾—\( A_t = \pi(S_t) \)æœ€å¤§åŒ–å›æŠ¥çš„æœŸæœ›ï¼Œå³ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±ï¼š

\[\mathbb{E} \left[ \sum_{t \geq 0} \gamma^t R_t \right],\]

for a discount factor \( 0 < \gamma < 1 \).

å…¶ä¸­æŠ˜æ‰£å› å­\( 0 < \gamma < 1 \)ã€‚

This is the standard setup of Reinforcement Learning (RL), and it can be worked out by introducing the optimal state-action value function \( Q(s, a) \) which is the expected return if we execute action \( a \) in state \( s \), and then follow the optimal policy. It provides a means to compute the optimal policy as \(\pi(s) = \arg\max_a Q(s, a)\), and, thanks to the Markovian assumption, it verifies the Bellman equation:

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ ‡å‡†è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å¼•å…¥æœ€ä¼˜çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°\( Q(s, a) \)æ¥è§£å†³ï¼Œè¯¥å‡½æ•°è¡¨ç¤ºåœ¨çŠ¶æ€\( s \)ä¸­æ‰§è¡ŒåŠ¨ä½œ\( a \)ç„¶åéµå¾ªæœ€ä¼˜ç­–ç•¥çš„æœŸæœ›å›æŠ¥ã€‚å®ƒæä¾›äº†ä¸€ç§è®¡ç®—æœ€ä¼˜ç­–ç•¥çš„æ–¹æ³•ï¼Œå³\(\pi(s) = \arg\max_a Q(s, a)\)ï¼Œå¹¶ä¸”ç”±äºé©¬å°”å¯å¤«å‡è®¾ï¼Œå®ƒæ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼š

\[Q(s, a) = \tag{6.1}\]

\[\mathbb{E} \left[ R_t + \gamma \max_{a'} Q(S_{t+1}, a') \right] S_t = s, A_t = a \]

from which we can design a procedure to train a parametric model \( Q(\cdot, \cdot; w) \).

ä»ä¸­æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªè®­ç»ƒå‚æ•°æ¨¡å‹\( Q(\cdot, \cdot; w) \)çš„è¿‡ç¨‹ã€‚

To apply this framework to play classical Atari video games, Mnih et al. [2015] use for \( S_t \) the concatenation of the frame at time \( t \) and the three that precede, so that the Markovian assumption is reasonable, and use for \( Q \) a model dubbed the Deep Q-Network (DQN), composed of two convolutional layers and one fully connected layer with one output value per action, following the classical structure of a LeNet (see Â§ 5.2).

ä¸ºäº†å°†è¿™ä¸ªæ¡†æ¶åº”ç”¨äºç©ç»å…¸çš„Atariè§†é¢‘æ¸¸æˆï¼ŒMnih et al. [2015]ä½¿ç”¨\( S_t \)ä½œä¸ºæ—¶é—´\( t \)çš„å¸§å’Œå‰ä¸‰ä¸ªå¸§çš„è¿æ¥ï¼Œä½¿å¾—é©¬å°”å¯å¤«å‡è®¾åˆç†ï¼Œå¹¶ä½¿ç”¨\( Q \)çš„æ¨¡å‹ç§°ä¸ºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ï¼Œç”±ä¸¤ä¸ªå·ç§¯å±‚å’Œä¸€ä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œæ¯ä¸ªåŠ¨ä½œæœ‰ä¸€ä¸ªè¾“å‡ºå€¼ï¼Œéµå¾ªLeNetçš„ç»å…¸ç»“æ„ï¼ˆè§Â§5.2ï¼‰ã€‚

Training is achieved by alternatively playing and recording episodes, and building mini-batches of tuples \((s_n, a_n, r_n, s'_n) \sim (S_t, A_t, R_t, S_{t+1})\) taken across stored episodes and time steps, and minimizing

è®­ç»ƒé€šè¿‡äº¤æ›¿ç©æ¸¸æˆå’Œè®°å½•å‰§é›†ï¼Œå¹¶æ„å»ºä»å­˜å‚¨çš„å‰§é›†å’Œæ—¶é—´æ­¥éª¤ä¸­æå–çš„å…ƒç»„\((s_n, a_n, r_n, s'_n) \sim (S_t, A_t, R_t, S_{t+1})\)çš„å°æ‰¹é‡ï¼Œå¹¶æœ€å°åŒ–

\[\mathcal{L}(w) = \frac{1}{N} \sum_{n=1}^{N} \left( Q(s_n, a_n; w) - y_n \right)^2 \tag{6.2}\]

with one iteration of SGD, where \( y_n = r_n \) if this tuple is the end of the episode, and \( y_n = r_n + \gamma \max_a Q(s'_n, a; w) \) otherwise.

é€šè¿‡ä¸€æ¬¡SGDè¿­ä»£ï¼Œå…¶ä¸­å¦‚æœè¯¥å…ƒç»„æ˜¯å‰§é›†çš„ç»“æŸï¼Œåˆ™\( y_n = r_n \)ï¼Œå¦åˆ™\( y_n = r_n + \gamma \max_a Q(s'_n, a; w) \)ã€‚

#### çŸ¥è¯†ç‚¹è®²è§£ï¼š
- **å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰**ï¼šå¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚
- **æ·±åº¦Qç½‘ç»œï¼ˆDeep Q-Network, DQNï¼‰**ï¼šDQNæ˜¯ä¸€ç§åŸºäºQå­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å·ç§¯ç½‘ç»œå¤„ç†å›¾åƒè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼ã€‚

---

### æ€»ç»“ï¼š
- **è¯­éŸ³è¯†åˆ«**æ˜¯å°†å£°éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬çš„ä»»åŠ¡ï¼ŒTransformeræ¨¡å‹é€šè¿‡å°†å£°éŸ³ä¿¡å·è½¬æ¢ä¸ºé¢‘è°±å›¾ï¼Œå¹¶ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œåºåˆ—åˆ°åºåˆ—çš„ç¿»è¯‘ã€‚
- **å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰**é€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç›¸åŒçš„åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé›¶æ ·æœ¬é¢„æµ‹ã€‚
- **å¼ºåŒ–å­¦ä¹ **æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå¤„ç†å›¾åƒè¾“å…¥çš„ä»»åŠ¡ã€‚

è¿™äº›çŸ¥è¯†ç‚¹æ˜¯ç†è§£æ·±åº¦å­¦ä¹ åœ¨è¯­éŸ³è¯†åˆ«ã€å›¾åƒ-æ–‡æœ¬è¡¨ç¤ºå’Œå¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬æœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–ç›¸å…³æ¨¡å‹ã€‚

---











































### Chapter 6.8: The Compute Schism

#### 6.8.1: Prompt Engineering

**English:**
The simplest strategy to specialize or improve a Large Language Model with a limited computational budget is to use prompt engineering, that is, to carefully craft the beginning of the text sequence to bias the autoregressive process [Sahoo et al., 2024]. This approach moves a part of the information traditionally encoded in the modelâ€™s parameters to the input.

**Chinese:**
åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¸“é—¨åŒ–æˆ–æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€ç®€å•ç­–ç•¥æ˜¯ä½¿ç”¨æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰ï¼Œå³ç²¾å¿ƒè®¾è®¡æ–‡æœ¬åºåˆ—çš„å¼€å¤´ï¼Œä»¥å¼•å¯¼è‡ªå›å½’è¿‡ç¨‹ [Sahoo et al., 2024]ã€‚è¿™ç§æ–¹æ³•å°†ä¼ ç»Ÿä¸Šç¼–ç åœ¨æ¨¡å‹å‚æ•°ä¸­çš„éƒ¨åˆ†ä¿¡æ¯è½¬ç§»åˆ°è¾“å…¥ä¸­ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Prompt Engineeringï¼ˆæç¤ºå·¥ç¨‹ï¼‰**: è¿™æ˜¯ä¸€ç§é€šè¿‡è®¾è®¡è¾“å…¥æç¤ºï¼ˆpromptï¼‰æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šè¾“å‡ºçš„æŠ€æœ¯ã€‚æç¤ºå·¥ç¨‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¾“å…¥ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆé¢„æœŸçš„ç»“æœï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
- **Autoregressive Processï¼ˆè‡ªå›å½’è¿‡ç¨‹ï¼‰**: è‡ªå›å½’æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ­¥ç”Ÿæˆåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ æ¥ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚æ¯ä¸ªå…ƒç´ çš„ç”Ÿæˆä¾èµ–äºä¹‹å‰ç”Ÿæˆçš„å…ƒç´ ã€‚

**English:**
We saw in Â§ 7.1 a simple example of few-shot prediction, to use an LLM for a text classification task without fine-tuning. A long and sophisticated prompt allows generalizing this strategy to complex tasks.

**Chinese:**
æˆ‘ä»¬åœ¨Â§7.1ä¸­çœ‹åˆ°äº†ä¸€ä¸ªç®€å•çš„å°‘æ ·æœ¬é¢„æµ‹ï¼ˆfew-shot predictionï¼‰ç¤ºä¾‹ï¼Œå³åœ¨ä¸éœ€è¦å¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ä¸€ä¸ªé•¿è€Œå¤æ‚çš„æç¤ºå¯ä»¥å°†è¿™ç§ç­–ç•¥æ¨å¹¿åˆ°æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Few-shot Predictionï¼ˆå°‘æ ·æœ¬é¢„æµ‹ï¼‰**: å°‘æ ·æœ¬é¢„æµ‹æ˜¯æŒ‡æ¨¡å‹åœ¨åªæœ‰å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰å¤§é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®Œæˆä»»åŠ¡ã€‚
- **Fine-tuningï¼ˆå¾®è°ƒï¼‰**: å¾®è°ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ã€‚

**English:**
Since the promptâ€™s role is to leverage the â€œgoodâ€ biases that were present in the training set, it benefits from surprising strategies such as stating that the response is generated by a skilled professional [Xu et al., 2023].

**Chinese:**
ç”±äºæç¤ºçš„ä½œç”¨æ˜¯åˆ©ç”¨è®­ç»ƒé›†ä¸­å­˜åœ¨çš„â€œè‰¯å¥½â€åå·®ï¼Œå› æ­¤å®ƒå¯ä»¥ä»ä¸€äº›å‡ºäººæ„æ–™çš„ç­–ç•¥ä¸­å—ç›Šï¼Œä¾‹å¦‚å£°æ˜å“åº”æ˜¯ç”±ç†Ÿç»ƒçš„ä¸“ä¸šäººå£«ç”Ÿæˆçš„ [Xu et al., 2023]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Biasï¼ˆåå·®ï¼‰**: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåå·®æŒ‡çš„æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„åå¥½æˆ–å€¾å‘ã€‚æç¤ºå·¥ç¨‹é€šè¿‡åˆ©ç”¨è¿™äº›åå·®æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆé¢„æœŸçš„è¾“å‡ºã€‚

**English:**
The context size of a language model, that is, the number of tokens it can operate on, directly modulates the quantity of information that can be provided in the prompt. This is mostly constrained by the computational cost of standard attention models, which is quadratic with the context size (see Â§ 4.8).

**Chinese:**
è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œå³å®ƒå¯ä»¥å¤„ç†çš„ä»¤ç‰Œæ•°é‡ï¼Œç›´æ¥å†³å®šäº†æç¤ºä¸­å¯ä»¥æä¾›çš„ä¿¡æ¯é‡ã€‚è¿™ä¸»è¦å—åˆ°æ ‡å‡†æ³¨æ„åŠ›æ¨¡å‹è®¡ç®—æˆæœ¬çš„é™åˆ¶ï¼Œè¯¥æˆæœ¬ä¸ä¸Šä¸‹æ–‡å¤§å°æˆäºŒæ¬¡æ–¹å…³ç³»ï¼ˆå‚è§Â§4.8ï¼‰ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Context Sizeï¼ˆä¸Šä¸‹æ–‡å¤§å°ï¼‰**: ä¸Šä¸‹æ–‡å¤§å°æŒ‡çš„æ˜¯æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ—¶èƒ½å¤Ÿè€ƒè™‘çš„ä»¤ç‰Œæ•°é‡ã€‚è¾ƒå¤§çš„ä¸Šä¸‹æ–‡å¤§å°å…è®¸æ¨¡å‹å¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼Œä½†ä¹Ÿä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚
- **Attention Modelsï¼ˆæ³¨æ„åŠ›æ¨¡å‹ï¼‰**: æ³¨æ„åŠ›æ¨¡å‹æ˜¯ä¸€ç§ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡è®¡ç®—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„é‡è¦æ€§æ¥ç”Ÿæˆè¾“å‡ºã€‚æ ‡å‡†çš„æ³¨æ„åŠ›æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ä¸è¾“å…¥åºåˆ—é•¿åº¦çš„å¹³æ–¹æˆæ­£æ¯”ã€‚

**English:**
A remarkable type of prompting aims at making the model generate intermediate steps before generating the response itself.

**Chinese:**
ä¸€ç§æ˜¾è‘—çš„æç¤ºç±»å‹æ—¨åœ¨è®©æ¨¡å‹åœ¨ç”Ÿæˆå“åº”ä¹‹å‰ç”Ÿæˆä¸­é—´æ­¥éª¤ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Intermediate Stepsï¼ˆä¸­é—´æ­¥éª¤ï¼‰**: ä¸­é—´æ­¥éª¤æ˜¯æŒ‡æ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆè¾“å‡ºä¹‹å‰ç”Ÿæˆçš„ä¸­é—´ç»“æœã€‚é€šè¿‡ç”Ÿæˆä¸­é—´æ­¥éª¤ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°åˆ†è§£å¤æ‚ä»»åŠ¡ï¼Œä»è€Œæé«˜ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§ã€‚

**English:**
Such a chain-of-thought is composed of successive steps that are simpler, hence have been better modeled during training, and are predicted more deterministically [Wei et al., 2022; Kojima et al., 2022]. See Figure 8.1 for an example.

**Chinese:**
è¿™ç§æ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰ç”±ä¸€ç³»åˆ—æ›´ç®€å•çš„æ­¥éª¤ç»„æˆï¼Œå› æ­¤åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾—åˆ°äº†æ›´å¥½çš„å»ºæ¨¡ï¼Œå¹¶ä¸”å¯ä»¥æ›´ç¡®å®šæ€§åœ°é¢„æµ‹ [Wei et al., 2022; Kojima et al., 2022]ã€‚å‚è§å›¾8.1ä¸­çš„ç¤ºä¾‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Chain-of-Thoughtï¼ˆæ€ç»´é“¾ï¼‰**: æ€ç»´é“¾æ˜¯ä¸€ç§æç¤ºæŠ€æœ¯ï¼Œé€šè¿‡è®©æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ã€‚

**English:**
Prompt engineering can also be put to use to connect a language model to an external knowledge base. It plays the role of a smart interface that allows the end user to formulate questions in natural language and get back a response that combines information that is not encoded in the modelâ€™s parameters [Lewis et al., 2020].

**Chinese:**
æç¤ºå·¥ç¨‹è¿˜å¯ä»¥ç”¨äºå°†è¯­è¨€æ¨¡å‹è¿æ¥åˆ°å¤–éƒ¨çŸ¥è¯†åº“ã€‚å®ƒå……å½“ä¸€ä¸ªæ™ºèƒ½æ¥å£ï¼Œå…è®¸æœ€ç»ˆç”¨æˆ·ä»¥è‡ªç„¶è¯­è¨€æå‡ºé—®é¢˜ï¼Œå¹¶è¿”å›ä¸€ä¸ªç»“åˆäº†æœªç¼–ç åœ¨æ¨¡å‹å‚æ•°ä¸­çš„ä¿¡æ¯çš„å“åº” [Lewis et al., 2020]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **External Knowledge Baseï¼ˆå¤–éƒ¨çŸ¥è¯†åº“ï¼‰**: å¤–éƒ¨çŸ¥è¯†åº“æ˜¯æŒ‡æ¨¡å‹å¤–éƒ¨çš„ç»“æ„åŒ–æ•°æ®æºï¼Œå¦‚æ•°æ®åº“æˆ–çŸ¥è¯†å›¾è°±ã€‚é€šè¿‡æç¤ºå·¥ç¨‹ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨è¿™äº›å¤–éƒ¨çŸ¥è¯†æ¥ç”Ÿæˆæ›´å‡†ç¡®çš„å“åº”ã€‚

**English:**
For such Retrieval-Augmented Generation (RAG), an embedding model is used to retrieve documents whose embedding is correlated to that of the userâ€™s query. Then, a prompt is constructed by joining these retrieved documents with instructions to combine them, and the generative model produces the response to the userâ€™s query.

**Chinese:**
å¯¹äºè¿™ç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰ï¼Œä½¿ç”¨åµŒå…¥æ¨¡å‹æ¥æ£€ç´¢ä¸ç”¨æˆ·æŸ¥è¯¢åµŒå…¥ç›¸å…³çš„æ–‡æ¡£ã€‚ç„¶åï¼Œé€šè¿‡å°†è¿™äº›æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç»„åˆæŒ‡ä»¤ç»“åˆæ¥æ„å»ºæç¤ºï¼Œç”Ÿæˆæ¨¡å‹ç”Ÿæˆå¯¹ç”¨æˆ·æŸ¥è¯¢çš„å“åº”ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Retrieval-Augmented Generation (RAG)ï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**: RAGæ˜¯ä¸€ç§ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å“åº”ã€‚
- **Embedding Modelï¼ˆåµŒå…¥æ¨¡å‹ï¼‰**: åµŒå…¥æ¨¡å‹æ˜¯ä¸€ç§å°†æ–‡æœ¬æˆ–å…¶ä»–æ•°æ®è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºçš„æ¨¡å‹ã€‚è¿™äº›å‘é‡è¡¨ç¤ºå¯ä»¥ç”¨äºè®¡ç®—æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚

#### 6.8.2: Quantization

**English:**
Although training or generating multiple streams can benefit from high-end parallel computing devices, deployment of a Large Language Model for individual use requires generally single-stream inference, which is bounded by memory size and speed far more than by computation.

**Chinese:**
å°½ç®¡è®­ç»ƒæˆ–ç”Ÿæˆå¤šä¸ªæµå¯ä»¥ä»é«˜ç«¯å¹¶è¡Œè®¡ç®—è®¾å¤‡ä¸­å—ç›Šï¼Œä½†ä¸ºä¸ªäººä½¿ç”¨éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦å•æµæ¨ç†ï¼Œè¿™æ›´å¤šåœ°å—åˆ°å†…å­˜å¤§å°å’Œé€Ÿåº¦çš„é™åˆ¶ï¼Œè€Œä¸æ˜¯è®¡ç®—èƒ½åŠ›çš„é™åˆ¶ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Single-stream Inferenceï¼ˆå•æµæ¨ç†ï¼‰**: å•æµæ¨ç†æ˜¯æŒ‡æ¨¡å‹åœ¨å•ä¸ªè®¡ç®—æµä¸Šè¿è¡Œï¼Œé€šå¸¸ç”¨äºä¸ªäººè®¾å¤‡æˆ–èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚ä¸å¤šæµæ¨ç†ç›¸æ¯”ï¼Œå•æµæ¨ç†æ›´å—å†…å­˜å’Œé€Ÿåº¦çš„é™åˆ¶ã€‚

**English:**
As stated in Â§ 2.1, parameters, activations, and gradients are usually encoded with 32 or 16 bits. The precision it provides is necessary for training, to allow gradual changes to accumulate.

**Chinese:**
å¦‚Â§2.1æ‰€è¿°ï¼Œå‚æ•°ã€æ¿€æ´»å€¼å’Œæ¢¯åº¦é€šå¸¸ç”¨32ä½æˆ–16ä½ç¼–ç ã€‚è¿™ç§ç²¾åº¦å¯¹äºè®­ç»ƒæ˜¯å¿…è¦çš„ï¼Œä»¥ä¾¿å…è®¸é€æ¸çš„å˜åŒ–ç§¯ç´¯ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Precisionï¼ˆç²¾åº¦ï¼‰**: ç²¾åº¦æŒ‡çš„æ˜¯æ•°å€¼è¡¨ç¤ºçš„ä½æ•°ï¼Œé€šå¸¸ç”¨32ä½æˆ–16ä½æµ®ç‚¹æ•°è¡¨ç¤ºã€‚é«˜ç²¾åº¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¿…è¦çš„ï¼Œä»¥ç¡®ä¿æ¢¯åº¦ä¸‹é™çš„ç¨³å®šæ€§ã€‚

**English:**
However, since activations are the sums of many terms, quantization during inference is mitigated by an averaging effect. This is even more true with large architectures, and models quantized down to 6 or 4 bits per parameter exhibit remarkable performance. Additionally to reducing the memory footprint, quantization also improves inference speed significantly.

**Chinese:**
ç„¶è€Œï¼Œç”±äºæ¿€æ´»å€¼æ˜¯è®¸å¤šé¡¹çš„æ€»å’Œï¼Œæ¨ç†è¿‡ç¨‹ä¸­çš„é‡åŒ–é€šè¿‡å¹³å‡æ•ˆåº”å¾—åˆ°äº†ç¼“è§£ã€‚å¯¹äºå¤§å‹æ¶æ„æ¥è¯´ï¼Œè¿™ä¸€ç‚¹å°¤å…¶æ˜æ˜¾ï¼Œé‡åŒ–åˆ°æ¯ä¸ªå‚æ•°6æˆ–4ä½çš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ã€‚é™¤äº†å‡å°‘å†…å­˜å ç”¨å¤–ï¼Œé‡åŒ–è¿˜æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Quantizationï¼ˆé‡åŒ–ï¼‰**: é‡åŒ–æ˜¯ä¸€ç§å°†é«˜ç²¾åº¦æ•°å€¼è½¬æ¢ä¸ºä½ç²¾åº¦æ•°å€¼çš„æŠ€æœ¯ï¼Œé€šå¸¸ç”¨äºå‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚é‡åŒ–å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾è‘—æé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚

**English:**
This has motivated the development of software to quantize existing models with Post-Training Quantization, and run them in single-stream inference on consumer hardware, such as llama.cpp [Llama.cpp, 2023]. This framework implements multiple formats, that apply specific quantization levels for the different weight matrices of a language model. For instance the quantization may use more bits for the \( W^v \) weights of the attention blocks, and for the weights of the feed-forward blocks.

**Chinese:**
è¿™æ¨åŠ¨äº†å¼€å‘ç”¨äºé‡åŒ–ç°æœ‰æ¨¡å‹çš„è½¯ä»¶ï¼Œå¦‚llama.cpp [Llama.cpp, 2023]ï¼Œå¹¶åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šä»¥å•æµæ¨ç†è¿è¡Œè¿™äº›æ¨¡å‹ã€‚è¯¥æ¡†æ¶å®ç°äº†å¤šç§æ ¼å¼ï¼Œé’ˆå¯¹è¯­è¨€æ¨¡å‹çš„ä¸åŒæƒé‡çŸ©é˜µåº”ç”¨ç‰¹å®šçš„é‡åŒ–çº§åˆ«ã€‚ä¾‹å¦‚ï¼Œé‡åŒ–å¯èƒ½å¯¹æ³¨æ„åŠ›å—çš„\( W^v \)æƒé‡å’Œå‰é¦ˆå—çš„æƒé‡ä½¿ç”¨æ›´å¤šçš„ä½æ•°ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Post-Training Quantizationï¼ˆè®­ç»ƒåé‡åŒ–ï¼‰**: è®­ç»ƒåé‡åŒ–æ˜¯æŒ‡åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä»¥å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚è¿™ç§æ–¹æ³•é€šå¸¸ç”¨äºåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²æ¨¡å‹ã€‚

**English:**
An example of llama.cppâ€™s quantization is Q4_1. It quantizes individually sub-blocks of 32 entries of the original weight matrix by storing for each a scaling factor \( d \) and a bias \( m \) in the original FP16 encoding, and encoding each entry \( x \) with 4 bits as a value \( q \in \{0, \ldots, 2^4 - 1\} \). The resulting de-quantized value being \( \bar{x} = dq + m \).

**Chinese:**
llama.cppçš„é‡åŒ–ç¤ºä¾‹æ˜¯Q4_1ã€‚å®ƒé€šè¿‡å¯¹åŸå§‹æƒé‡çŸ©é˜µçš„æ¯ä¸ª32ä¸ªæ¡ç›®çš„å­å—è¿›è¡Œå•ç‹¬é‡åŒ–ï¼Œä¸ºæ¯ä¸ªå­å—å­˜å‚¨ä¸€ä¸ªç¼©æ”¾å› å­\( d \)å’Œä¸€ä¸ªåç½®\( m \)ï¼Œå¹¶ä½¿ç”¨4ä½å°†æ¯ä¸ªæ¡ç›®\( x \)ç¼–ç ä¸º\( q \in \{0, \ldots, 2^4 - 1\} \)ã€‚åé‡åŒ–åçš„å€¼ä¸º\( \bar{x} = dq + m \)ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Scaling Factor and Biasï¼ˆç¼©æ”¾å› å­å’Œåç½®ï¼‰**: åœ¨é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œç¼©æ”¾å› å­å’Œåç½®ç”¨äºå°†ä½ç²¾åº¦çš„é‡åŒ–å€¼è½¬æ¢å›é«˜ç²¾åº¦çš„æ•°å€¼ã€‚ç¼©æ”¾å› å­ç”¨äºè°ƒæ•´é‡åŒ–å€¼çš„èŒƒå›´ï¼Œè€Œåç½®ç”¨äºè°ƒæ•´é‡åŒ–å€¼çš„åç§»ã€‚

**English:**
Such a block was encoded originally as 32 values in FP16, hence 64 bytes, while the quantized version needs 4 bytes for \( q \) and \( m \) and \( 32 \cdot 4 \) bits = 16 bytes for the entries, hence a total of 20 bytes.

**Chinese:**
è¿™æ ·çš„å—åŸæœ¬ç¼–ç ä¸º32ä¸ªFP16å€¼ï¼Œå› æ­¤éœ€è¦64å­—èŠ‚ï¼Œè€Œé‡åŒ–ç‰ˆæœ¬éœ€è¦4å­—èŠ‚ç”¨äº\( q \)å’Œ\( m \)ï¼Œä»¥åŠ\( 32 \cdot 4 \)ä½=16å­—èŠ‚ç”¨äºæ¡ç›®ï¼Œå› æ­¤æ€»å…±éœ€è¦20å­—èŠ‚ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Memory Footprintï¼ˆå†…å­˜å ç”¨ï¼‰**: å†…å­˜å ç”¨æŒ‡çš„æ˜¯æ¨¡å‹åœ¨è¿è¡Œæ—¶æ‰€éœ€çš„å†…å­˜å¤§å°ã€‚é‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œã€‚

**English:**
Such an aggressive quantization surprisingly degrades only marginally the performance of the models, as illustrated on Figure 8.2.

**Chinese:**
å¦‚æ­¤æ¿€è¿›çš„é‡åŒ–ä»¤äººæƒŠè®¶åœ°ä»…ç•¥å¾®é™ä½äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¦‚å›¾8.2æ‰€ç¤ºã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Aggressive Quantizationï¼ˆæ¿€è¿›é‡åŒ–ï¼‰**: æ¿€è¿›é‡åŒ–æ˜¯æŒ‡å°†æ¨¡å‹å‚æ•°å‹ç¼©åˆ°æä½ç²¾åº¦çš„é‡åŒ–æ–¹æ³•ã€‚å°½ç®¡é‡åŒ–ç²¾åº¦è¾ƒä½ï¼Œä½†é€šè¿‡é€‚å½“çš„ç¼©æ”¾å› å­å’Œåç½®è°ƒæ•´ï¼Œæ¨¡å‹çš„æ€§èƒ½æŸå¤±å¯ä»¥æ§åˆ¶åœ¨å¯æ¥å—çš„èŒƒå›´å†…ã€‚

**English:**
An alternative to Post-Training Quantization is Quantization-Aware Training that applies quantization during the forward pass but keeps high-precision encoding of parameters and gradients, and propagates the gradients during the backward pass as if there was no quantization [Ma et al., 2024].

**Chinese:**
è®­ç»ƒåé‡åŒ–çš„å¦ä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQuantization-Aware Trainingï¼‰ï¼Œå®ƒåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åº”ç”¨é‡åŒ–ï¼Œä½†ä¿æŒå‚æ•°å’Œæ¢¯åº¦çš„é«˜ç²¾åº¦ç¼–ç ï¼Œå¹¶åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¼ æ’­æ¢¯åº¦ï¼Œå°±åƒæ²¡æœ‰é‡åŒ–ä¸€æ · [Ma et al., 2024]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Quantization-Aware Trainingï¼ˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰**: é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–çš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨é‡åŒ–æ¥ä½¿æ¨¡å‹é€‚åº”ä½ç²¾åº¦çš„è®¡ç®—ç¯å¢ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»¥å‡å°‘é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

#### 6.8.3: Adapters

**English:**
As we saw in Â§ 3.6, fine-tuning is a key strategy to reuse pre-trained models. Since it aims at making only minor changes to an existing model, techniques have been developed that add components with few parameters, referred to as adapters, to the pre-trained architecture, and freeze all the original parameters [Houlsby et al., 2019].

**Chinese:**
æ­£å¦‚æˆ‘ä»¬åœ¨Â§3.6ä¸­çœ‹åˆ°çš„ï¼Œå¾®è°ƒæ˜¯é‡ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…³é”®ç­–ç•¥ã€‚ç”±äºå®ƒæ—¨åœ¨å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå› æ­¤å¼€å‘äº†ä¸€äº›æŠ€æœ¯ï¼Œå‘é¢„è®­ç»ƒæ¶æ„ä¸­æ·»åŠ å°‘é‡å‚æ•°çš„ç»„ä»¶ï¼Œç§°ä¸ºé€‚é…å™¨ï¼ˆadaptersï¼‰ï¼Œå¹¶å†»ç»“æ‰€æœ‰åŸå§‹å‚æ•° [Houlsby et al., 2019]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Adaptersï¼ˆé€‚é…å™¨ï¼‰**: é€‚é…å™¨æ˜¯ä¸€ç§åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°çš„æŠ€æœ¯ï¼Œç”¨äºåœ¨ä¸æ”¹å˜åŸå§‹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹é€‚åº”æ–°ä»»åŠ¡ã€‚é€‚é…å™¨é€šå¸¸æ’å…¥åˆ°æ¨¡å‹çš„æŸäº›å±‚ä¸­ï¼Œä»¥å¾®è°ƒæ¨¡å‹çš„è¡Œä¸ºã€‚

**English:**
The current dominant method is the Low-Rank Adaptation (LoRA), which adds low-rank corrections to some of the modelâ€™s weight matrices [Hu et al., 2021].

**Chinese:**
å½“å‰çš„ä¸»æµæ–¹æ³•æ˜¯ä½ç§©é€‚åº”ï¼ˆLow-Rank Adaptation, LoRAï¼‰ï¼Œå®ƒå‘æ¨¡å‹çš„æŸäº›æƒé‡çŸ©é˜µæ·»åŠ ä½ç§©æ ¡æ­£ [Hu et al., 2021]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Low-Rank Adaptation (LoRA)ï¼ˆä½ç§©é€‚åº”ï¼‰**: LoRAæ˜¯ä¸€ç§é€šè¿‡å‘æ¨¡å‹çš„æƒé‡çŸ©é˜µæ·»åŠ ä½ç§©çŸ©é˜µæ¥è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥å°‘é‡é¢å¤–çš„å‚æ•°æ¥è°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚

**English:**
Formally, given a linear operation of the form \(XW^T\), where \(X\) is a \(N \times D\) tensor of activations for a batch of \(N\) samples, and \(W\) is a \(C \times D\) weight matrix, the LoRA adapter replaces this operation with \(X(W + BA)^T\), where \(A\) and \(B\) are two trainable matrices of size \(R \times D\) and \(C \times R\) respectively, with \(R \ll \min(C, D)\), and the matrix \(W\) is removed from the trainable parameters. The matrix \(A\) is initialized with random Gaussian values, and \(B\) is set to zero, so that the fine-tuning starts with a model that computes an output identical to that of the original one.

**Chinese:**
å½¢å¼ä¸Šï¼Œç»™å®šå½¢å¼ä¸º\(XW^T\)çš„çº¿æ€§æ“ä½œï¼Œå…¶ä¸­\(X\)æ˜¯å¤§å°ä¸º\(N \times D\)çš„æ¿€æ´»å¼ é‡ï¼Œè¡¨ç¤ºä¸€æ‰¹\(N\)ä¸ªæ ·æœ¬ï¼Œ\(W\)æ˜¯å¤§å°ä¸º\(C \times D\)çš„æƒé‡çŸ©é˜µï¼ŒLoRAé€‚é…å™¨å°†æ­¤æ“ä½œæ›¿æ¢ä¸º\(X(W + BA)^T\)ï¼Œå…¶ä¸­\(A\)å’Œ\(B\)åˆ†åˆ«æ˜¯å¤§å°ä¸º\(R \times D\)å’Œ\(C \times R\)çš„ä¸¤ä¸ªå¯è®­ç»ƒçŸ©é˜µï¼Œä¸”\(R \ll \min(C, D)\)ï¼ŒçŸ©é˜µ\(W\)ä»å¯è®­ç»ƒå‚æ•°ä¸­ç§»é™¤ã€‚çŸ©é˜µ\(A\)ç”¨éšæœºé«˜æ–¯å€¼åˆå§‹åŒ–ï¼Œ\(B\)è®¾ç½®ä¸ºé›¶ï¼Œå› æ­¤å¾®è°ƒå¼€å§‹æ—¶æ¨¡å‹è®¡ç®—çš„è¾“å‡ºä¸åŸå§‹æ¨¡å‹ç›¸åŒã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Low-Rank Matrixï¼ˆä½ç§©çŸ©é˜µï¼‰**: ä½ç§©çŸ©é˜µæ˜¯æŒ‡ç§©è¿œå°äºå…¶è¡Œæ•°å’Œåˆ—æ•°çš„çŸ©é˜µã€‚åœ¨LoRAä¸­ï¼Œä½ç§©çŸ©é˜µç”¨äºè¡¨ç¤ºå¯¹åŸå§‹æƒé‡çŸ©é˜µçš„å¾®å°è°ƒæ•´ï¼Œä»è€Œå‡å°‘å¾®è°ƒæ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚

**English:**
The total number of parameters to optimize with this approach is generally a few percent of the number of parameters in the original model.

**Chinese:**
ä½¿ç”¨è¿™ç§æ–¹æ³•ä¼˜åŒ–çš„å‚æ•°æ€»æ•°é€šå¸¸æ˜¯åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡çš„ç™¾åˆ†ä¹‹å‡ ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Parameter Efficiencyï¼ˆå‚æ•°æ•ˆç‡ï¼‰**: å‚æ•°æ•ˆç‡æŒ‡çš„æ˜¯åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥çš„é¢å¤–å‚æ•°æ•°é‡ã€‚LoRAé€šè¿‡å¼•å…¥å°‘é‡ä½ç§©çŸ©é˜µæ¥ä¿æŒè¾ƒé«˜çš„å‚æ•°æ•ˆç‡ï¼Œä»è€Œå‡å°‘å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚

**English:**
The standard procedure to fine-tune a transformer with such adapters is to change only the weight matrices in the attention blocks, and to keep the MLP of the feed-forward blocks unchanged. The same strategy has been used successfully to tune diffusion denoising models by fine-tuning the attention blocks responsible for the text-based conditioning.

**Chinese:**
ä½¿ç”¨è¿™ç§é€‚é…å™¨å¾®è°ƒTransformerçš„æ ‡å‡†ç¨‹åºæ˜¯ä»…æ›´æ”¹æ³¨æ„åŠ›å—ä¸­çš„æƒé‡çŸ©é˜µï¼Œå¹¶ä¿æŒå‰é¦ˆå—çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä¸å˜ã€‚åŒæ ·çš„ç­–ç•¥å·²æˆåŠŸç”¨äºé€šè¿‡å¾®è°ƒè´Ÿè´£åŸºäºæ–‡æœ¬æ¡ä»¶çš„æ³¨æ„åŠ›å—æ¥è°ƒæ•´æ‰©æ•£å»å™ªæ¨¡å‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Attention Blocksï¼ˆæ³¨æ„åŠ›å—ï¼‰**: æ³¨æ„åŠ›å—æ˜¯Transformeræ¨¡å‹ä¸­çš„å…³é”®ç»„ä»¶ï¼Œè´Ÿè´£è®¡ç®—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„é‡è¦æ€§ã€‚é€šè¿‡å¾®è°ƒæ³¨æ„åŠ›å—ï¼Œå¯ä»¥è°ƒæ•´æ¨¡å‹å¯¹ä¸åŒè¾“å…¥çš„å…³æ³¨ç¨‹åº¦ã€‚

**English:**
Since fine-tuning with LoRA adapters drastically reduces the number of trainable parameters, it reduces the memory footprint required by optimizers such as Adam, which generally store two running average per parameter to optimize. Also, it reduces slightly the computation during the backward pass.

**Chinese:**
ç”±äºä½¿ç”¨LoRAé€‚é…å™¨è¿›è¡Œå¾®è°ƒå¤§å¤§å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œå› æ­¤å®ƒå‡å°‘äº†ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰æ‰€éœ€çš„å†…å­˜å ç”¨ï¼Œä¼˜åŒ–å™¨é€šå¸¸ä¸ºæ¯ä¸ªå‚æ•°å­˜å‚¨ä¸¤ä¸ªè¿è¡Œå¹³å‡å€¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç•¥å¾®å‡å°‘äº†åå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„è®¡ç®—é‡ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Optimizer Memory Footprintï¼ˆä¼˜åŒ–å™¨å†…å­˜å ç”¨ï¼‰**: ä¼˜åŒ–å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å­˜å‚¨æ¯ä¸ªå‚æ•°çš„è¿è¡Œå¹³å‡å€¼ï¼Œä»¥è®¡ç®—æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼ŒLoRAå¯ä»¥æ˜¾è‘—é™ä½ä¼˜åŒ–å™¨çš„å†…å­˜éœ€æ±‚ã€‚

**English:**
For commercial applications that require a large number of fine-tuned models, the \(AB\) pairs can be stored separately from the original model, which has to be stored only once. And finally, contrary to other type of adapters, the modifications can be integrated into the original architecture, simply by adding \(AB\) to \(W\), resulting in an architecture and parameter count for inference identical to that of the original model.

**Chinese:**
å¯¹äºéœ€è¦å¤§é‡å¾®è°ƒæ¨¡å‹çš„å•†ä¸šåº”ç”¨ï¼Œ\(AB\)å¯¹å¯ä»¥ä¸åŸå§‹æ¨¡å‹åˆ†å¼€å­˜å‚¨ï¼ŒåŸå§‹æ¨¡å‹åªéœ€å­˜å‚¨ä¸€æ¬¡ã€‚æœ€åï¼Œä¸å…¶ä»–ç±»å‹çš„é€‚é…å™¨ä¸åŒï¼Œè¿™äº›ä¿®æ”¹å¯ä»¥é€šè¿‡ç®€å•åœ°å°†\(AB\)æ·»åŠ åˆ°\(W\)ä¸­é›†æˆåˆ°åŸå§‹æ¶æ„ä¸­ï¼Œä»è€Œåœ¨æ¨ç†æ—¶ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸åŒçš„æ¶æ„å’Œå‚æ•°æ•°é‡ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Model Integrationï¼ˆæ¨¡å‹é›†æˆï¼‰**: æ¨¡å‹é›†æˆæ˜¯æŒ‡å°†å¾®è°ƒåçš„å‚æ•°ä¸åŸå§‹æ¨¡å‹å‚æ•°ç»“åˆï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„æ¨ç†æ¨¡å‹ã€‚LoRAé€šè¿‡ç®€å•çš„çŸ©é˜µåŠ æ³•å®ç°æ¨¡å‹é›†æˆï¼Œä»è€Œä¿æŒæ¨ç†æ—¶çš„æ¨¡å‹ç»“æ„ä¸å˜ã€‚

**English:**
We saw that quantization degrade modelsâ€™ accuracy only marginally. However, gradient descent requires high precision in both the gradient and the trained parameters, to allow the accumulation of small changes. The QLoRA approach combines a quantized base model and unquantized Low-Rank Adaptation to reduce the memory requirement even more [Dettmers et al., 2023].

**Chinese:**
æˆ‘ä»¬çœ‹åˆ°é‡åŒ–ä»…ç•¥å¾®é™ä½äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæ¢¯åº¦ä¸‹é™éœ€è¦åœ¨æ¢¯åº¦å’Œè®­ç»ƒå‚æ•°ä¸­ä¿æŒé«˜ç²¾åº¦ï¼Œä»¥å…è®¸å°å˜åŒ–çš„ç§¯ç´¯ã€‚QLoRAæ–¹æ³•ç»“åˆäº†é‡åŒ–åŸºç¡€æ¨¡å‹å’Œæœªé‡åŒ–çš„ä½ç§©é€‚åº”ï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜éœ€æ±‚ [Dettmers et al., 2023]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **QLoRAï¼ˆé‡åŒ–ä½ç§©é€‚åº”ï¼‰**: QLoRAæ˜¯ä¸€ç§ç»“åˆäº†é‡åŒ–å’Œä½ç§©é€‚åº”çš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨é‡åŒ–åŸºç¡€æ¨¡å‹ä¸Šæ·»åŠ æœªé‡åŒ–çš„ä½ç§©çŸ©é˜µæ¥è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥å‡å°‘äº†å†…å­˜éœ€æ±‚ã€‚

#### 6.8.4: Model Merging

**English:**
An alternative to the fine-tuning and prompting methods seen in the previous sections consists of combining multiple models with diverse capabilities into a single one, without additional training.

**Chinese:**
ä¸å‰é¢ç« èŠ‚ä¸­çœ‹åˆ°çš„å¾®è°ƒå’Œæç¤ºæ–¹æ³•ä¸åŒï¼Œå¦ä¸€ç§æ–¹æ³•æ˜¯å°†å…·æœ‰ä¸åŒèƒ½åŠ›çš„å¤šä¸ªæ¨¡å‹ç»„åˆæˆä¸€ä¸ªæ¨¡å‹ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Model Mergingï¼ˆæ¨¡å‹åˆå¹¶ï¼‰**: æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§å°†å¤šä¸ªæ¨¡å‹çš„å‚æ•°ç»„åˆæˆä¸€ä¸ªæ¨¡å‹çš„æŠ€æœ¯ï¼Œé€šå¸¸ç”¨äºç»“åˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**English:**
Model merging relies on the compatibility between multiple fine-tuned versions of a base model.

**Chinese:**
æ¨¡å‹åˆå¹¶ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„å¤šä¸ªå¾®è°ƒç‰ˆæœ¬ä¹‹é—´çš„å…¼å®¹æ€§ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Compatibilityï¼ˆå…¼å®¹æ€§ï¼‰**: å…¼å®¹æ€§æŒ‡çš„æ˜¯ä¸åŒæ¨¡å‹å‚æ•°ä¹‹é—´çš„ç›¸ä¼¼æ€§æˆ–ä¸€è‡´æ€§ã€‚æ¨¡å‹åˆå¹¶è¦æ±‚ä¸åŒå¾®è°ƒç‰ˆæœ¬çš„æ¨¡å‹å‚æ•°åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯å…¼å®¹çš„ï¼Œä»¥ä¾¿èƒ½å¤Ÿæœ‰æ•ˆåœ°ç»„åˆã€‚

**English:**
Ilharco et al. [2022] showed that models obtained by fine-tuning a CLIP base model on several image classification data-sets can be combined in the parameter space, where they exhibit Task Arithmetic properties.

**Chinese:**
Ilharcoç­‰äºº[2022]è¡¨æ˜ï¼Œé€šè¿‡åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šå¾®è°ƒCLIPåŸºç¡€æ¨¡å‹è·å¾—çš„æ¨¡å‹å¯ä»¥åœ¨å‚æ•°ç©ºé—´ä¸­ç»„åˆï¼Œè¿™äº›æ¨¡å‹è¡¨ç°å‡ºä»»åŠ¡ç®—æœ¯ï¼ˆTask Arithmeticï¼‰ç‰¹æ€§ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Task Arithmeticï¼ˆä»»åŠ¡ç®—æœ¯ï¼‰**: ä»»åŠ¡ç®—æœ¯æ˜¯ä¸€ç§æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œé€šè¿‡åœ¨å‚æ•°ç©ºé—´ä¸­ç»„åˆä¸åŒä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹ï¼Œç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šä¸ªä»»åŠ¡çš„å•ä¸€æ¨¡å‹ã€‚

**English:**
Formally, let \(\theta\) be the parameter vector of a pretrained model, and for \(t = 1, \ldots, T\), let \(\theta_t\) and \(\tau_t = \theta_t - \theta\) be respectively the parameters after fine-tuning on task \(t\) and the corresponding residual. Experiments show that the model with parameters \(\theta + \tau_1 + \cdots + \tau_T\) exhibits multi-task capabilities. Similarly, subtracting a \(\tau_t\) degrades the performance on the corresponding task.

**Chinese:**
å½¢å¼ä¸Šï¼Œè®¾\(\theta\)ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å‘é‡ï¼Œå¯¹äº\(t = 1, \ldots, T\)ï¼Œè®¾\(\theta_t\)å’Œ\(\tau_t = \theta_t - \theta\)åˆ†åˆ«ä¸ºåœ¨ä»»åŠ¡\(t\)ä¸Šå¾®è°ƒåçš„å‚æ•°å’Œç›¸åº”çš„æ®‹å·®ã€‚å®éªŒè¡¨æ˜ï¼Œå…·æœ‰å‚æ•°\(\theta + \tau_1 + \cdots + \tau_T\)çš„æ¨¡å‹è¡¨ç°å‡ºå¤šä»»åŠ¡èƒ½åŠ›ã€‚ç±»ä¼¼åœ°ï¼Œå‡å»\(\tau_t\)ä¼šé™ä½ç›¸åº”ä»»åŠ¡çš„æ€§èƒ½ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Residual Parametersï¼ˆæ®‹å·®å‚æ•°ï¼‰**: æ®‹å·®å‚æ•°æ˜¯æŒ‡å¾®è°ƒåçš„æ¨¡å‹å‚æ•°ä¸åŸå§‹æ¨¡å‹å‚æ•°ä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡ç»„åˆè¿™äº›æ®‹å·®å‚æ•°ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šä¸ªä»»åŠ¡çš„å•ä¸€æ¨¡å‹ã€‚

**English:**
Methods have been developed to reduce the interference between the different residuals and improve the performance when the number of tasks is large.

**Chinese:**
å·²ç»å¼€å‘äº†ä¸€äº›æ–¹æ³•æ¥å‡å°‘ä¸åŒæ®‹å·®ä¹‹é—´çš„å¹²æ‰°ï¼Œå¹¶åœ¨ä»»åŠ¡æ•°é‡è¾ƒå¤šæ—¶æé«˜æ€§èƒ½ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Interference Reductionï¼ˆå¹²æ‰°å‡å°‘ï¼‰**: å¹²æ‰°å‡å°‘æ˜¯æŒ‡é€šè¿‡è°ƒæ•´æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œå‡å°‘ä¸åŒä»»åŠ¡æ®‹å·®ä¹‹é—´çš„å†²çªï¼Œä»è€Œæé«˜å¤šä»»åŠ¡æ¨¡å‹çš„æ€§èƒ½ã€‚

**English:**
An alternative to merging models in parameter space is to recombine their layers. Akiba et al. [2024] combine merging the parameters and re-combining layers, and rely on a stochastic optimization to deal with the combinatorial explosion. Experiments with three fine-tuned versions of Mistral-7B [Jiang et al., 2023] show that combining these two merging strategies outperforms both of them.

**Chinese:**
åœ¨å‚æ•°ç©ºé—´ä¸­åˆå¹¶æ¨¡å‹çš„å¦ä¸€ç§æ–¹æ³•æ˜¯é‡æ–°ç»„åˆå®ƒä»¬çš„å±‚ã€‚Akibaç­‰äºº[2024]ç»“åˆäº†å‚æ•°åˆå¹¶å’Œå±‚é‡æ–°ç»„åˆï¼Œå¹¶ä¾èµ–éšæœºä¼˜åŒ–æ¥å¤„ç†ç»„åˆçˆ†ç‚¸ã€‚ä½¿ç”¨Mistral-7B [Jiang et al., 2023]çš„ä¸‰ä¸ªå¾®è°ƒç‰ˆæœ¬è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç»“åˆè¿™ä¸¤ç§åˆå¹¶ç­–ç•¥ä¼˜äºå•ç‹¬ä½¿ç”¨å…¶ä¸­ä»»ä½•ä¸€ç§ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Layer Recombinationï¼ˆå±‚é‡æ–°ç»„åˆï¼‰**: å±‚é‡æ–°ç»„åˆæ˜¯ä¸€ç§æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œé€šè¿‡é‡æ–°ç»„åˆä¸åŒæ¨¡å‹çš„å±‚æ¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç»“åˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç”Ÿæˆæ›´å¼ºå¤§çš„å¤šä»»åŠ¡æ¨¡å‹ã€‚
- **Stochastic Optimizationï¼ˆéšæœºä¼˜åŒ–ï¼‰**: éšæœºä¼˜åŒ–æ˜¯ä¸€ç§é€šè¿‡éšæœºæœç´¢æ¥å¯»æ‰¾æœ€ä¼˜è§£çš„æ–¹æ³•ã€‚åœ¨æ¨¡å‹åˆå¹¶ä¸­ï¼Œéšæœºä¼˜åŒ–å¯ä»¥ç”¨äºå¤„ç†ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼Œæ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹ç»„åˆç­–ç•¥ã€‚

### æ€»ç»“

æœ¬ç« ä»‹ç»äº†åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹ã€é‡åŒ–å’Œé€‚é…å™¨ç­‰æŠ€æœ¯æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå¾®è°ƒã€‚æç¤ºå·¥ç¨‹é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¾“å…¥æç¤ºå¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šè¾“å‡ºï¼›é‡åŒ–é€šè¿‡å‡å°‘æ¨¡å‹å‚æ•°çš„ç²¾åº¦æ¥é™ä½å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ï¼›é€‚é…å™¨åˆ™é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°æ¥é€‚åº”æ–°ä»»åŠ¡ã€‚è¿™äº›æŠ€æœ¯ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚

### Chapter 6.8.4: Model Merging (Continued)

#### 6.8.4.1: Model Merging in Parameter Space

**English:**
Model merging in parameter space involves combining the parameters of multiple models trained on different tasks to create a single model capable of performing multiple tasks. This approach leverages the idea that the differences between models (residuals) can be additive, allowing for the combination of task-specific knowledge without retraining.

**Chinese:**
å‚æ•°ç©ºé—´ä¸­çš„æ¨¡å‹åˆå¹¶æ¶‰åŠå°†å¤šä¸ªåœ¨ä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„å‚æ•°ç»„åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿæ‰§è¡Œå¤šä¸ªä»»åŠ¡çš„å•ä¸€æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼ˆæ®‹å·®ï¼‰å¯ä»¥ç›¸åŠ çš„æ€æƒ³ï¼Œä»è€Œåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ç»“åˆç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Parameter Spaceï¼ˆå‚æ•°ç©ºé—´ï¼‰**: å‚æ•°ç©ºé—´æ˜¯æŒ‡æ¨¡å‹å‚æ•°çš„æ‰€æœ‰å¯èƒ½ç»„åˆã€‚é€šè¿‡åœ¨è¿™ä¸ªç©ºé—´ä¸­ç»„åˆä¸åŒæ¨¡å‹çš„å‚æ•°ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šä¸ªä»»åŠ¡çš„å•ä¸€æ¨¡å‹ã€‚
- **Residualsï¼ˆæ®‹å·®ï¼‰**: æ®‹å·®æ˜¯æŒ‡å¾®è°ƒåçš„æ¨¡å‹å‚æ•°ä¸åŸå§‹æ¨¡å‹å‚æ•°ä¹‹é—´çš„å·®å¼‚ã€‚è¿™äº›å·®å¼‚å¯ä»¥ç”¨äºç»„åˆä¸åŒä»»åŠ¡çš„çŸ¥è¯†ã€‚

**English:**
The key insight is that the residuals from fine-tuning on different tasks can be combined linearly to create a model that performs well on all tasks. This is particularly useful when the tasks are related, as the residuals will share some common structure.

**Chinese:**
å…³é”®è§è§£æ˜¯ï¼Œåœ¨ä¸åŒä»»åŠ¡ä¸Šå¾®è°ƒçš„æ®‹å·®å¯ä»¥çº¿æ€§ç»„åˆï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚è¿™åœ¨ä»»åŠ¡ç›¸å…³æ—¶å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºæ®‹å·®å°†å…±äº«ä¸€äº›å…±åŒçš„ç»“æ„ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Linear Combinationï¼ˆçº¿æ€§ç»„åˆï¼‰**: çº¿æ€§ç»„åˆæ˜¯æŒ‡å°†å¤šä¸ªå‘é‡æˆ–çŸ©é˜µé€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼ç»„åˆèµ·æ¥ã€‚åœ¨æ¨¡å‹åˆå¹¶ä¸­ï¼Œçº¿æ€§ç»„åˆç”¨äºå°†ä¸åŒä»»åŠ¡çš„æ®‹å·®ç»“åˆèµ·æ¥ã€‚
- **Task Relatednessï¼ˆä»»åŠ¡ç›¸å…³æ€§ï¼‰**: ä»»åŠ¡ç›¸å…³æ€§æŒ‡çš„æ˜¯ä¸åŒä»»åŠ¡ä¹‹é—´çš„ç›¸ä¼¼æ€§æˆ–å…³è”æ€§ã€‚ä»»åŠ¡ç›¸å…³æ€§è¶Šé«˜ï¼Œæ¨¡å‹åˆå¹¶çš„æ•ˆæœé€šå¸¸è¶Šå¥½ã€‚

#### 6.8.4.2: Layer Recombination

**English:**
Layer recombination is an alternative approach to model merging that involves recombining the layers of different models to create a new model. This method is particularly useful when the models have different architectures or when the tasks require different levels of abstraction.

**Chinese:**
å±‚é‡æ–°ç»„åˆæ˜¯æ¨¡å‹åˆå¹¶çš„å¦ä¸€ç§æ–¹æ³•ï¼Œæ¶‰åŠé‡æ–°ç»„åˆä¸åŒæ¨¡å‹çš„å±‚ä»¥åˆ›å»ºä¸€ä¸ªæ–°æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨æ¨¡å‹å…·æœ‰ä¸åŒæ¶æ„æˆ–ä»»åŠ¡éœ€è¦ä¸åŒæŠ½è±¡çº§åˆ«æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Layer Recombinationï¼ˆå±‚é‡æ–°ç»„åˆï¼‰**: å±‚é‡æ–°ç»„åˆæ˜¯æŒ‡å°†ä¸åŒæ¨¡å‹çš„å±‚é‡æ–°ç»„åˆä»¥ç”Ÿæˆä¸€ä¸ªæ–°æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç»“åˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç”Ÿæˆæ›´å¼ºå¤§çš„å¤šä»»åŠ¡æ¨¡å‹ã€‚
- **Abstraction Levelsï¼ˆæŠ½è±¡çº§åˆ«ï¼‰**: æŠ½è±¡çº§åˆ«æŒ‡çš„æ˜¯æ¨¡å‹åœ¨å¤„ç†æ•°æ®æ—¶æ‰€ä½¿ç”¨çš„æŠ½è±¡ç¨‹åº¦ã€‚ä¸åŒä»»åŠ¡å¯èƒ½éœ€è¦ä¸åŒæŠ½è±¡çº§åˆ«çš„æ¨¡å‹å±‚ã€‚

**English:**
For example, one might combine the lower layers of a model trained on image classification with the higher layers of a model trained on object detection to create a model that can perform both tasks effectively.

**Chinese:**
ä¾‹å¦‚ï¼Œå¯ä»¥å°†ä¸€ä¸ªåœ¨å›¾åƒåˆ†ç±»ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„è¾ƒä½å±‚ä¸ä¸€ä¸ªåœ¨ç›®æ ‡æ£€æµ‹ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„è¾ƒé«˜å±‚ç»“åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œè¿™ä¸¤ä¸ªä»»åŠ¡çš„æ¨¡å‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Lower Layersï¼ˆè¾ƒä½å±‚ï¼‰**: è¾ƒä½å±‚é€šå¸¸è´Ÿè´£æå–è¾“å…¥æ•°æ®çš„åŸºæœ¬ç‰¹å¾ï¼Œå¦‚è¾¹ç¼˜å’Œçº¹ç†ã€‚
- **Higher Layersï¼ˆè¾ƒé«˜å±‚ï¼‰**: è¾ƒé«˜å±‚é€šå¸¸è´Ÿè´£å¤„ç†æ›´æŠ½è±¡çš„ç‰¹å¾ï¼Œå¦‚ç‰©ä½“çš„å½¢çŠ¶å’Œç±»åˆ«ã€‚

#### 6.8.4.3: Stochastic Optimization for Model Merging

**English:**
Stochastic optimization is a technique used to handle the combinatorial explosion that arises when merging multiple models. By randomly sampling different combinations of model parameters or layers, one can find a combination that performs well across multiple tasks.

**Chinese:**
éšæœºä¼˜åŒ–æ˜¯ä¸€ç§ç”¨äºå¤„ç†åˆå¹¶å¤šä¸ªæ¨¡å‹æ—¶å‡ºç°çš„ç»„åˆçˆ†ç‚¸é—®é¢˜çš„æŠ€æœ¯ã€‚é€šè¿‡éšæœºé‡‡æ ·æ¨¡å‹å‚æ•°æˆ–å±‚çš„ä¸åŒç»„åˆï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„ç»„åˆã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Combinatorial Explosionï¼ˆç»„åˆçˆ†ç‚¸ï¼‰**: ç»„åˆçˆ†ç‚¸æŒ‡çš„æ˜¯å½“ç»„åˆæ•°é‡éšç€å…ƒç´ æ•°é‡çš„å¢åŠ è€Œæ€¥å‰§å¢åŠ çš„ç°è±¡ã€‚åœ¨æ¨¡å‹åˆå¹¶ä¸­ï¼Œç»„åˆçˆ†ç‚¸ä½¿å¾—å¯»æ‰¾æœ€ä½³ç»„åˆå˜å¾—å›°éš¾ã€‚
- **Random Samplingï¼ˆéšæœºé‡‡æ ·ï¼‰**: éšæœºé‡‡æ ·æ˜¯æŒ‡ä»æ‰€æœ‰å¯èƒ½çš„ç»„åˆä¸­éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†è¿›è¡Œæµ‹è¯•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶æ‰¾åˆ°è¾ƒå¥½çš„ç»„åˆã€‚

**English:**
Akiba et al. [2024] demonstrated that combining parameter merging and layer recombination with stochastic optimization can lead to better performance than using either method alone. Their experiments with Mistral-7B showed significant improvements in multi-task performance.

**Chinese:**
Akibaç­‰äºº[2024]è¯æ˜äº†å°†å‚æ•°åˆå¹¶å’Œå±‚é‡æ–°ç»„åˆä¸éšæœºä¼˜åŒ–ç»“åˆèµ·æ¥ï¼Œå¯ä»¥æ¯”å•ç‹¬ä½¿ç”¨ä»»ä½•ä¸€ç§æ–¹æ³•è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä»–ä»¬åœ¨Mistral-7Bä¸Šçš„å®éªŒæ˜¾ç¤ºäº†å¤šä»»åŠ¡æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Multi-task Performanceï¼ˆå¤šä»»åŠ¡æ€§èƒ½ï¼‰**: å¤šä»»åŠ¡æ€§èƒ½æŒ‡çš„æ˜¯æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ç»“åˆå‚æ•°åˆå¹¶å’Œå±‚é‡æ–°ç»„åˆï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚

### Chapter 6.9: The Missing Bits

#### 6.9.1: Recurrent Neural Networks

**English:**
Before attention models showed greater performance, Recurrent Neural Networks (RNN) were the standard approach for dealing with temporal sequences such as text or sound samples. These architectures possess an internal hidden state that gets updated each time a component of the sequence is processed. Their main components are layers such as LSTM [Hochreiter and Schmidhuber, 1997] or GRU [Cho et al., 2014].

**Chinese:**
åœ¨æ³¨æ„åŠ›æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜æ€§èƒ½ä¹‹å‰ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ˜¯å¤„ç†æ–‡æœ¬æˆ–å£°éŸ³æ ·æœ¬ç­‰æ—¶é—´åºåˆ—çš„æ ‡å‡†æ–¹æ³•ã€‚è¿™äº›æ¶æ„å…·æœ‰ä¸€ä¸ªå†…éƒ¨éšè—çŠ¶æ€ï¼Œæ¯æ¬¡å¤„ç†åºåˆ—çš„ä¸€ä¸ªç»„ä»¶æ—¶éƒ½ä¼šæ›´æ–°ã€‚å®ƒä»¬çš„ä¸»è¦ç»„ä»¶æ˜¯LSTM [Hochreiter and Schmidhuber, 1997] æˆ– GRU [Cho et al., 2014] ç­‰å±‚ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Recurrent Neural Networks (RNN)ï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰**: RNNæ˜¯ä¸€ç§ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå…·æœ‰ä¸€ä¸ªå†…éƒ¨éšè—çŠ¶æ€ï¼Œå¯ä»¥åœ¨å¤„ç†åºåˆ—æ—¶ä¿æŒä¿¡æ¯ã€‚
- **LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰**: LSTMæ˜¯ä¸€ç§ç‰¹æ®Šçš„RNNï¼Œé€šè¿‡å¼•å…¥é—¨æ§æœºåˆ¶æ¥è§£å†³é•¿åºåˆ—ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- **GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰**: GRUæ˜¯å¦ä¸€ç§RNNå˜ä½“ï¼Œé€šè¿‡ç®€åŒ–LSTMçš„ç»“æ„æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

**English:**
Training a recurrent architecture amounts to unfolding it in time, which results in a long composition of operators. This has historically prompted the design of key techniques now used for deep architectures such as rectifiers and gating, a form of skip connections which are modulated by the input.

**Chinese:**
è®­ç»ƒå¾ªç¯æ¶æ„ç›¸å½“äºåœ¨æ—¶é—´ä¸Šå±•å¼€å®ƒï¼Œè¿™ä¼šå¯¼è‡´ä¸€ç³»åˆ—é•¿æ“ä½œç¬¦çš„ç»„åˆã€‚è¿™åœ¨å†å²ä¸Šä¿ƒä½¿äº†ç°åœ¨ç”¨äºæ·±åº¦æ¶æ„çš„å…³é”®æŠ€æœ¯çš„è®¾è®¡ï¼Œå¦‚æ•´æµå™¨å’Œé—¨æ§æœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”±è¾“å…¥è°ƒåˆ¶çš„è·³è·ƒè¿æ¥å½¢å¼ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Unfolding in Timeï¼ˆæ—¶é—´å±•å¼€ï¼‰**: æ—¶é—´å±•å¼€æ˜¯æŒ‡å°†å¾ªç¯ç¥ç»ç½‘ç»œåœ¨æ—¶é—´æ­¥éª¤ä¸Šå±•å¼€ï¼Œå½¢æˆä¸€ä¸ªå‰é¦ˆç½‘ç»œã€‚è¿™ç§æ–¹æ³•ä½¿å¾—RNNå¯ä»¥é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒã€‚
- **Rectifiersï¼ˆæ•´æµå™¨ï¼‰**: æ•´æµå™¨æ˜¯ä¸€ç§æ¿€æ´»å‡½æ•°ï¼Œå¦‚ReLUï¼Œç”¨äºåœ¨ç¥ç»ç½‘ç»œä¸­å¼•å…¥éçº¿æ€§ã€‚
- **Gatingï¼ˆé—¨æ§æœºåˆ¶ï¼‰**: é—¨æ§æœºåˆ¶æ˜¯ä¸€ç§é€šè¿‡è¾“å…¥ä¿¡å·æ§åˆ¶ä¿¡æ¯æµåŠ¨çš„æŠ€æœ¯ï¼Œå¸¸ç”¨äºLSTMå’ŒGRUä¸­ã€‚

**English:**
One of the key drawbacks of traditional recurrent architectures is that the structure of the computation \( x_{t+1} = f(x_t) \) imposes to process the input sequence serially, which takes a time proportional to \( T \). In contrast, transformers, for instance, can take advantage of parallel computation, resulting in a constant time if enough computing units are available.

**Chinese:**
ä¼ ç»Ÿå¾ªç¯æ¶æ„çš„ä¸€ä¸ªå…³é”®ç¼ºç‚¹æ˜¯è®¡ç®—ç»“æ„ \( x_{t+1} = f(x_t) \) è¦æ±‚æŒ‰é¡ºåºå¤„ç†è¾“å…¥åºåˆ—ï¼Œè¿™éœ€è¦ä¸ \( T \) æˆæ­£æ¯”çš„æ—¶é—´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¾‹å¦‚Transformerå¯ä»¥åˆ©ç”¨å¹¶è¡Œè®¡ç®—ï¼Œå¦‚æœæœ‰è¶³å¤Ÿçš„è®¡ç®—å•å…ƒï¼Œåˆ™å¯ä»¥åœ¨æ’å®šæ—¶é—´å†…å®Œæˆã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Serial Processingï¼ˆé¡ºåºå¤„ç†ï¼‰**: é¡ºåºå¤„ç†æ˜¯æŒ‡æŒ‰é¡ºåºé€ä¸ªå¤„ç†è¾“å…¥åºåˆ—ä¸­çš„å…ƒç´ ã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†é•¿åºåˆ—æ—¶æ•ˆç‡è¾ƒä½ã€‚
- **Parallel Computationï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰**: å¹¶è¡Œè®¡ç®—æ˜¯æŒ‡åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥å…ƒç´ ã€‚Transformeré€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œæé«˜äº†å¤„ç†æ•ˆç‡ã€‚

**English:**
This is addressed by architectures such as QRNN [Bradbury et al., 2016], S4 [Gu et al., 2021], or Mamba [Gu and Dao, 2023], whose recurrent operations are affine so that the \( f^t \) themselves, and consequently the \( x_t = f^t(x_0) \), can be computed in parallel, resulting in a constant time if \( f \) does not depend on \( t \) and \(\log T\) otherwise, again if enough parallel computing units are available.

**Chinese:**
è¿™ä¸ªé—®é¢˜é€šè¿‡QRNN [Bradbury et al., 2016]ã€S4 [Gu et al., 2021] æˆ– Mamba [Gu and Dao, 2023] ç­‰æ¶æ„å¾—åˆ°è§£å†³ï¼Œè¿™äº›æ¶æ„çš„å¾ªç¯æ“ä½œæ˜¯ä»¿å°„çš„ï¼Œå› æ­¤ \( f^t \) æœ¬èº«ä»¥åŠ \( x_t = f^t(x_0) \) å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œå¦‚æœ \( f \) ä¸ä¾èµ–äº \( t \)ï¼Œåˆ™å¯ä»¥åœ¨æ’å®šæ—¶é—´å†…å®Œæˆï¼Œå¦åˆ™åœ¨ \(\log T\) æ—¶é—´å†…å®Œæˆï¼Œå‰ææ˜¯æœ‰è¶³å¤Ÿçš„å¹¶è¡Œè®¡ç®—å•å…ƒã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Affine Operationsï¼ˆä»¿å°„æ“ä½œï¼‰**: ä»¿å°„æ“ä½œæ˜¯æŒ‡çº¿æ€§å˜æ¢åŠ ä¸Šä¸€ä¸ªåç½®é¡¹ã€‚ä»¿å°„æ“ä½œå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚
- **Parallel Computing Unitsï¼ˆå¹¶è¡Œè®¡ç®—å•å…ƒï¼‰**: å¹¶è¡Œè®¡ç®—å•å…ƒæ˜¯æŒ‡èƒ½å¤ŸåŒæ—¶æ‰§è¡Œå¤šä¸ªè®¡ç®—ä»»åŠ¡çš„ç¡¬ä»¶èµ„æºï¼Œå¦‚GPUæˆ–TPUã€‚

#### 6.9.2: Autoencoder

**English:**
An autoencoder is a model that maps an input signal, possibly of high dimension, to a low-dimension latent representation, and then maps it back to the original signal, ensuring that information has been preserved. We saw it in Â§ 6.1 for denoising, but it can also be used to automatically discover a meaningful low-dimension representation of the data.

**Chinese:**
è‡ªç¼–ç å™¨æ˜¯ä¸€ç§å°†è¾“å…¥ä¿¡å·ï¼ˆå¯èƒ½æ˜¯é«˜ç»´çš„ï¼‰æ˜ å°„åˆ°ä½ç»´æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åå†å°†å…¶æ˜ å°„å›åŸå§‹ä¿¡å·çš„æ¨¡å‹ï¼Œç¡®ä¿ä¿¡æ¯å¾—ä»¥ä¿ç•™ã€‚æˆ‘ä»¬åœ¨Â§6.1ä¸­çœ‹åˆ°äº†å®ƒç”¨äºå»å™ªï¼Œä½†å®ƒä¹Ÿå¯ä»¥ç”¨äºè‡ªåŠ¨å‘ç°æ•°æ®çš„æœ‰æ„ä¹‰çš„ä½ç»´è¡¨ç¤ºã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Autoencoderï¼ˆè‡ªç¼–ç å™¨ï¼‰**: è‡ªç¼–ç å™¨æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å°†è¾“å…¥æ•°æ®å‹ç¼©åˆ°ä½ç»´è¡¨ç¤ºå¹¶é‡å»ºè¾“å…¥æ•°æ®æ¥å­¦ä¹ æ•°æ®çš„ç‰¹å¾ã€‚
- **Latent Representationï¼ˆæ½œåœ¨è¡¨ç¤ºï¼‰**: æ½œåœ¨è¡¨ç¤ºæ˜¯æŒ‡è‡ªç¼–ç å™¨å°†è¾“å…¥æ•°æ®å‹ç¼©åˆ°çš„ä½ç»´è¡¨ç¤ºã€‚è¿™ç§è¡¨ç¤ºé€šå¸¸åŒ…å«æ•°æ®çš„å…³é”®ç‰¹å¾ã€‚

**English:**
The Variational Autoencoder (VAE) proposed by Kingma and Welling [2013] is a generative model with a similar structure. It imposes, through the loss, a pre-defined distribution on the latent representation. This allows, after training, the generation of new samples by sampling the latent representation according to this imposed distribution and then mapping back through the decoder.

**Chinese:**
Kingmaå’ŒWelling [2013] æå‡ºçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ˜¯ä¸€ç§å…·æœ‰ç±»ä¼¼ç»“æ„çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒé€šè¿‡æŸå¤±å‡½æ•°åœ¨æ½œåœ¨è¡¨ç¤ºä¸Šæ–½åŠ é¢„å®šä¹‰çš„åˆ†å¸ƒã€‚è¿™ä½¿å¾—åœ¨è®­ç»ƒåï¼Œå¯ä»¥é€šè¿‡æŒ‰ç…§è¿™ç§æ–½åŠ çš„åˆ†å¸ƒå¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œé‡‡æ ·ï¼Œç„¶åé€šè¿‡è§£ç å™¨æ˜ å°„å›æ¥ç”Ÿæˆæ–°æ ·æœ¬ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Variational Autoencoder (VAE)ï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰**: VAEæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡åœ¨æ½œåœ¨è¡¨ç¤ºä¸Šæ–½åŠ æ¦‚ç‡åˆ†å¸ƒæ¥ç”Ÿæˆæ–°æ ·æœ¬ã€‚
- **Pre-defined Distributionï¼ˆé¢„å®šä¹‰åˆ†å¸ƒï¼‰**: é¢„å®šä¹‰åˆ†å¸ƒæ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ–½åŠ åœ¨æ½œåœ¨è¡¨ç¤ºä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€šå¸¸é€‰æ‹©é«˜æ–¯åˆ†å¸ƒã€‚

#### 6.9.3: Generative Adversarial Networks

**English:**
Another approach to density modeling is the Generative Adversarial Networks (GAN) introduced by Goodfellow et al. [2014]. This method combines a generator, which takes a random input following a fixed distribution as input and produces a structured signal such as an image, and a discriminator, which takes a sample as input and predicts whether it comes from the training set or if it was generated by the generator.

**Chinese:**
å¦ä¸€ç§å¯†åº¦å»ºæ¨¡æ–¹æ³•æ˜¯Goodfellowç­‰äºº[2014] æå‡ºçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒæ¥å—ä¸€ä¸ªéµå¾ªå›ºå®šåˆ†å¸ƒçš„éšæœºè¾“å…¥å¹¶ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–ä¿¡å·ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œä»¥åŠä¸€ä¸ªåˆ¤åˆ«å™¨ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ ·æœ¬ä½œä¸ºè¾“å…¥å¹¶é¢„æµ‹å®ƒæ˜¯æ¥è‡ªè®­ç»ƒé›†è¿˜æ˜¯ç”±ç”Ÿæˆå™¨ç”Ÿæˆçš„ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Generative Adversarial Networks (GAN)ï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰**: GANæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—è®­ç»ƒæ¥ç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ã€‚
- **Generatorï¼ˆç”Ÿæˆå™¨ï¼‰**: ç”Ÿæˆå™¨æ˜¯GANçš„ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£ç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ã€‚
- **Discriminatorï¼ˆåˆ¤åˆ«å™¨ï¼‰**: åˆ¤åˆ«å™¨æ˜¯GANçš„å¦ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£åŒºåˆ†çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ã€‚

**English:**
Training optimizes the discriminator to minimize a standard cross-entropy loss, and the generator to maximize the discriminatorâ€™s loss. It results in a generator that produces samples that are indistinguishable from real data.

**Chinese:**
è®­ç»ƒä¼˜åŒ–åˆ¤åˆ«å™¨ä»¥æœ€å°åŒ–æ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼Œå¹¶ä¼˜åŒ–ç”Ÿæˆå™¨ä»¥æœ€å¤§åŒ–åˆ¤åˆ«å™¨çš„æŸå¤±ã€‚è¿™å¯¼è‡´ç”Ÿæˆå™¨ç”Ÿæˆä¸çœŸå®æ•°æ®æ— æ³•åŒºåˆ†çš„æ ·æœ¬ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Cross-entropy Lossï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰**: äº¤å‰ç†µæŸå¤±æ˜¯ä¸€ç§ç”¨äºè¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŸå¤±å‡½æ•°ã€‚åœ¨GANä¸­ï¼Œåˆ¤åˆ«å™¨ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¥åŒºåˆ†çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ã€‚
- **Adversarial Trainingï¼ˆå¯¹æŠ—è®­ç»ƒï¼‰**: å¯¹æŠ—è®­ç»ƒæ˜¯æŒ‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—è¿‡ç¨‹ï¼Œç”Ÿæˆå™¨è¯•å›¾ç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ï¼Œè€Œåˆ¤åˆ«å™¨è¯•å›¾åŒºåˆ†çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ã€‚

#### 6.9.4: Graph Neural Networks

**English:**
Many applications require processing signals which are not organized regularly on a grid. For instance, proteins, 3D meshes, geographic locations, or social interactions are more naturally structured as graphs. Standard convolutional networks or even attention models are poorly adapted to process such data, and the tool of choice for such a task is Graph Neural Networks (GNN) [Scarselli et al., 2009].

**Chinese:**
è®¸å¤šåº”ç”¨éœ€è¦å¤„ç†æœªåœ¨ç½‘æ ¼ä¸Šè§„åˆ™ç»„ç»‡çš„ä¿¡å·ã€‚ä¾‹å¦‚ï¼Œè›‹ç™½è´¨ã€3Dç½‘æ ¼ã€åœ°ç†ä½ç½®æˆ–ç¤¾äº¤äº’åŠ¨æ›´è‡ªç„¶åœ°ç»“æ„åŒ–ä¸ºå›¾ã€‚æ ‡å‡†å·ç§¯ç½‘ç»œç”šè‡³æ³¨æ„åŠ›æ¨¡å‹éƒ½ä¸å¤ªé€‚åˆå¤„ç†æ­¤ç±»æ•°æ®ï¼Œè€Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰[Scarselli et al., 2009] æ˜¯å¤„ç†æ­¤ç±»ä»»åŠ¡çš„é¦–é€‰å·¥å…·ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Graph Neural Networks (GNN)ï¼ˆå›¾ç¥ç»ç½‘ç»œï¼‰**: GNNæ˜¯ä¸€ç§ç”¨äºå¤„ç†å›¾ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œé€šè¿‡åœ¨å›¾ä¸­çš„èŠ‚ç‚¹ä¹‹é—´ä¼ é€’ä¿¡æ¯æ¥å­¦ä¹ å›¾çš„ç‰¹å¾ã€‚
- **Graph Structureï¼ˆå›¾ç»“æ„ï¼‰**: å›¾ç»“æ„æ˜¯æŒ‡ç”±èŠ‚ç‚¹å’Œè¾¹ç»„æˆçš„æ•°æ®ç»“æ„ï¼Œå¸¸ç”¨äºè¡¨ç¤ºå¤æ‚çš„å…³ç³»å’Œäº¤äº’ã€‚

**English:**
These models are composed of layers that compute activations at each vertex by combining linearly the activations located at its immediate neighboring vertices. This operation is very similar to a standard convolution, except that the data structure does not reflect any geometrical information associated with the feature vectors they carry.

**Chinese:**
è¿™äº›æ¨¡å‹ç”±å±‚ç»„æˆï¼Œè¿™äº›å±‚é€šè¿‡çº¿æ€§ç»„åˆä½äºå…¶ç›´æ¥ç›¸é‚»é¡¶ç‚¹çš„æ¿€æ´»æ¥è®¡ç®—æ¯ä¸ªé¡¶ç‚¹çš„æ¿€æ´»ã€‚è¿™ç§æ“ä½œä¸æ ‡å‡†å·ç§¯éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯æ•°æ®ç»“æ„ä¸åæ˜ å®ƒä»¬æ‰€æºå¸¦çš„ç‰¹å¾å‘é‡çš„ä»»ä½•å‡ ä½•ä¿¡æ¯ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Vertex Activationï¼ˆé¡¶ç‚¹æ¿€æ´»ï¼‰**: é¡¶ç‚¹æ¿€æ´»æ˜¯æŒ‡å›¾ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æ¿€æ´»å€¼ï¼Œé€šè¿‡ä¸å…¶é‚»å±…èŠ‚ç‚¹çš„æ¿€æ´»å€¼çº¿æ€§ç»„åˆæ¥è®¡ç®—ã€‚
- **Geometrical Informationï¼ˆå‡ ä½•ä¿¡æ¯ï¼‰**: å‡ ä½•ä¿¡æ¯æ˜¯æŒ‡æ•°æ®åœ¨ç©ºé—´ä¸­çš„ä½ç½®å’Œå½¢çŠ¶ä¿¡æ¯ã€‚åœ¨å›¾ç¥ç»ç½‘ç»œä¸­ï¼Œæ•°æ®ç»“æ„ä¸åæ˜ å‡ ä½•ä¿¡æ¯ï¼Œè€Œæ˜¯åæ˜ èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚

#### 6.9.5: Self-Supervised Learning

**English:**
As stated in Â§ 7.1, even though they are trained only to predict the next word, Large Language Models trained on large unlabeled datasets such as GPT (see Â§ 5.3) are able to solve various tasks, such as identifying the grammatical role of a word, answering questions, or even translating from one language to another [Radford et al., 2019].

**Chinese:**
å¦‚Â§7.1æ‰€è¿°ï¼Œå°½ç®¡å®ƒä»¬ä»…è¢«è®­ç»ƒæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œä½†åœ¨å¤§å‹æœªæ ‡è®°æ•°æ®é›†ï¼ˆå¦‚GPTï¼Œå‚è§Â§5.3ï¼‰ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè§£å†³å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚è¯†åˆ«å•è¯çš„è¯­æ³•è§’è‰²ã€å›ç­”é—®é¢˜ï¼Œç”šè‡³ä»ä¸€ç§è¯­è¨€ç¿»è¯‘åˆ°å¦ä¸€ç§è¯­è¨€ [Radford et al., 2019]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Self-Supervised Learningï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰**: è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨çš„æ ‡ç­¾ã€‚
- **Large Language Modelsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰**: å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯æŒ‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå’Œç†è§£è‡ªç„¶è¯­è¨€ã€‚

**English:**
Such models constitute one category of a larger class of methods that fall under the name of self-supervised learning, and try to take advantage of unlabeled datasets [Balestriero et al., 2023].

**Chinese:**
è¿™äº›æ¨¡å‹æ„æˆäº†è‡ªç›‘ç£å­¦ä¹ è¿™ä¸€å¤§ç±»æ–¹æ³•ä¸­çš„ä¸€ç±»ï¼Œè¯•å›¾åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®é›† [Balestriero et al., 2023]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Unlabeled Datasetsï¼ˆæœªæ ‡è®°æ•°æ®é›†ï¼‰**: æœªæ ‡è®°æ•°æ®é›†æ˜¯æŒ‡æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡æ¥åˆ©ç”¨è¿™äº›æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

**English:**
The key principle of these methods is to define a task that does not require labels but necessitates feature representations which are useful for the real task of interest, for which a small labeled dataset exists. In computer vision, for instance, image features can be optimized so that they are invariant to data transformations that do not change the semantic content of the image, while being statistically uncorrelated [Zbontar et al., 2021].

**Chinese:**
è¿™äº›æ–¹æ³•çš„å…³é”®åŸåˆ™æ˜¯å®šä¹‰ä¸€ä¸ªä¸éœ€è¦æ ‡ç­¾ä½†éœ€è¦ç‰¹å¾è¡¨ç¤ºçš„ä»»åŠ¡ï¼Œè¿™äº›ç‰¹å¾è¡¨ç¤ºå¯¹äºæ„Ÿå…´è¶£çš„çœŸå®ä»»åŠ¡æ˜¯æœ‰ç”¨çš„ï¼Œè€ŒçœŸå®ä»»åŠ¡å­˜åœ¨ä¸€ä¸ªå°å‹æ ‡è®°æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå¯ä»¥ä¼˜åŒ–å›¾åƒç‰¹å¾ï¼Œä½¿å…¶å¯¹ä¸æ”¹å˜å›¾åƒè¯­ä¹‰å†…å®¹çš„æ•°æ®å˜æ¢ä¿æŒä¸å˜ï¼ŒåŒæ—¶åœ¨ç»Ÿè®¡ä¸Šä¸ç›¸å…³ [Zbontar et al., 2021]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Feature Representationsï¼ˆç‰¹å¾è¡¨ç¤ºï¼‰**: ç‰¹å¾è¡¨ç¤ºæ˜¯æŒ‡æ•°æ®åœ¨æ¨¡å‹ä¸­çš„è¡¨ç¤ºå½¢å¼ã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **Data Transformationsï¼ˆæ•°æ®å˜æ¢ï¼‰**: æ•°æ®å˜æ¢æ˜¯æŒ‡å¯¹æ•°æ®è¿›è¡Œæ—‹è½¬ã€ç¼©æ”¾ã€è£å‰ªç­‰æ“ä½œã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ä½¿ç‰¹å¾è¡¨ç¤ºå¯¹è¿™äº›å˜æ¢ä¿æŒä¸å˜æ¥å­¦ä¹ é²æ£’çš„ç‰¹å¾ã€‚

**English:**
In both NLP and computer vision, a powerful generic strategy is to train a model to recover a masked part of the input, or to predict the relative position of patches, or to reconstruct the input from a corrupted version [Devlin et al., 2018; Zhou et al., 2021].

**Chinese:**
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­ï¼Œä¸€ä¸ªå¼ºå¤§çš„é€šç”¨ç­–ç•¥æ˜¯è®­ç»ƒæ¨¡å‹æ¥æ¢å¤è¾“å…¥çš„æ©ç éƒ¨åˆ†ï¼Œæˆ–é¢„æµ‹è¡¥ä¸çš„ç›¸å¯¹ä½ç½®ï¼Œæˆ–ä»æŸåçš„ç‰ˆæœ¬ä¸­é‡å»ºè¾“å…¥ [Devlin et al., 2018; Zhou et al., 2021]ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Masked Partï¼ˆæ©ç éƒ¨åˆ†ï¼‰**: æ©ç éƒ¨åˆ†æ˜¯æŒ‡è¾“å…¥ä¸­è¢«éšè—æˆ–é®æŒ¡çš„éƒ¨åˆ†ã€‚é€šè¿‡æ¢å¤æ©ç éƒ¨åˆ†ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°è¾“å…¥æ•°æ®çš„å®Œæ•´è¡¨ç¤ºã€‚
- **Relative Positionï¼ˆç›¸å¯¹ä½ç½®ï¼‰**: ç›¸å¯¹ä½ç½®æ˜¯æŒ‡è¾“å…¥ä¸­ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„ä½ç½®å…³ç³»ã€‚é€šè¿‡é¢„æµ‹ç›¸å¯¹ä½ç½®ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°è¾“å…¥æ•°æ®çš„ç©ºé—´ç»“æ„ã€‚
- **Reconstructionï¼ˆé‡å»ºï¼‰**: é‡å»ºæ˜¯æŒ‡ä»æŸåçš„è¾“å…¥ä¸­æ¢å¤åŸå§‹è¾“å…¥ã€‚é€šè¿‡é‡å»ºä»»åŠ¡ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°è¾“å…¥æ•°æ®çš„å…³é”®ç‰¹å¾ã€‚

### æ€»ç»“

æœ¬ç« ä»‹ç»äº†åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹ã€é‡åŒ–å’Œé€‚é…å™¨ç­‰æŠ€æœ¯æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå¾®è°ƒã€‚æç¤ºå·¥ç¨‹é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¾“å…¥æç¤ºå¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šè¾“å‡ºï¼›é‡åŒ–é€šè¿‡å‡å°‘æ¨¡å‹å‚æ•°çš„ç²¾åº¦æ¥é™ä½å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ï¼›é€‚é…å™¨åˆ™é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°æ¥é€‚åº”æ–°ä»»åŠ¡ã€‚è¿™äº›æŠ€æœ¯ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚


### Chapter 6.9.5: Self-Supervised Learning (Continued)

#### 6.9.5.1: Masked Language Modeling

**English:**
Masked Language Modeling (MLM) is a common self-supervised learning task in natural language processing. In this task, certain words in a sentence are masked, and the model is trained to predict the masked words based on the surrounding context. This approach allows the model to learn rich representations of language without requiring labeled data.

**Chinese:**
æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked Language Modeling, MLMï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¸¸è§çš„è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œå¥å­ä¸­çš„æŸäº›è¯è¢«æ©ç ï¼Œæ¨¡å‹è¢«è®­ç»ƒä¸ºæ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¢«æ©ç çš„è¯ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ ä¸°å¯Œçš„è¯­è¨€è¡¨ç¤ºã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Masked Language Modeling (MLM)ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼‰**: MLMæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡é¢„æµ‹è¢«æ©ç çš„è¯æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¹¿æ³›åº”ç”¨äºBERTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ã€‚
- **Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰**: ä¸Šä¸‹æ–‡æ˜¯æŒ‡å¥å­ä¸­å›´ç»•æŸä¸ªè¯çš„è¯è¯­æˆ–å¥å­ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£è¯­è¨€çš„å«ä¹‰ã€‚

**English:**
For example, in the sentence "The cat sat on the [MASK]", the model might be trained to predict that the masked word is "mat" based on the context provided by the other words in the sentence.

**Chinese:**
ä¾‹å¦‚ï¼Œåœ¨å¥å­â€œThe cat sat on the [MASK]â€ä¸­ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¢«è®­ç»ƒä¸ºæ ¹æ®å¥å­ä¸­å…¶ä»–è¯æä¾›çš„ä¸Šä¸‹æ–‡é¢„æµ‹è¢«æ©ç çš„è¯æ˜¯â€œmatâ€ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Predictionï¼ˆé¢„æµ‹ï¼‰**: é¢„æµ‹æ˜¯æŒ‡æ¨¡å‹æ ¹æ®è¾“å…¥æ•°æ®æ¨æ–­å‡ºç¼ºå¤±æˆ–æœªæ¥çš„ä¿¡æ¯ã€‚åœ¨MLMä¸­ï¼Œæ¨¡å‹é€šè¿‡é¢„æµ‹è¢«æ©ç çš„è¯æ¥å­¦ä¹ è¯­è¨€è¡¨ç¤ºã€‚

#### 6.9.5.2: Contrastive Learning

**English:**
Contrastive learning is another self-supervised learning technique that involves training a model to distinguish between similar and dissimilar pairs of data points. This is often done by maximizing the similarity between positive pairs (e.g., two different views of the same image) and minimizing the similarity between negative pairs (e.g., views of different images).

**Chinese:**
å¯¹æ¯”å­¦ä¹ æ˜¯å¦ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œæ¶‰åŠè®­ç»ƒæ¨¡å‹ä»¥åŒºåˆ†ç›¸ä¼¼å’Œä¸ç›¸ä¼¼çš„æ•°æ®ç‚¹å¯¹ã€‚è¿™é€šå¸¸é€šè¿‡æœ€å¤§åŒ–æ­£å¯¹ï¼ˆä¾‹å¦‚ï¼ŒåŒä¸€å›¾åƒçš„ä¸¤ä¸ªä¸åŒè§†å›¾ï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶æœ€å°åŒ–è´Ÿå¯¹ï¼ˆä¾‹å¦‚ï¼Œä¸åŒå›¾åƒçš„è§†å›¾ï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥å®ç°ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Contrastive Learningï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰**: å¯¹æ¯”å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡æ¯”è¾ƒæ•°æ®ç‚¹å¯¹æ¥å­¦ä¹ ç‰¹å¾è¡¨ç¤ºçš„æŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
- **Positive Pairsï¼ˆæ­£å¯¹ï¼‰**: æ­£å¯¹æ˜¯æŒ‡æ¥è‡ªåŒä¸€æ•°æ®æºçš„ä¸åŒè§†å›¾æˆ–æ ·æœ¬ã€‚é€šè¿‡æœ€å¤§åŒ–æ­£å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ•°æ®çš„å…±åŒç‰¹å¾ã€‚
- **Negative Pairsï¼ˆè´Ÿå¯¹ï¼‰**: è´Ÿå¯¹æ˜¯æŒ‡æ¥è‡ªä¸åŒæ•°æ®æºçš„æ ·æœ¬ã€‚é€šè¿‡æœ€å°åŒ–è´Ÿå¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ•°æ®çš„åŒºåˆ†æ€§ç‰¹å¾ã€‚

**English:**
In computer vision, contrastive learning has been used to train models on large unlabeled datasets by creating positive pairs through data augmentation techniques such as cropping, rotating, or color jittering.

**Chinese:**
åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå¯¹æ¯”å­¦ä¹ å·²é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆå¦‚è£å‰ªã€æ—‹è½¬æˆ–é¢œè‰²æŠ–åŠ¨ï¼‰åˆ›å»ºæ­£å¯¹ï¼Œä»è€Œåœ¨å¤§å‹æœªæ ‡è®°æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Data Augmentationï¼ˆæ•°æ®å¢å¼ºï¼‰**: æ•°æ®å¢å¼ºæ˜¯æŒ‡é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢ï¼ˆå¦‚è£å‰ªã€æ—‹è½¬ã€é¢œè‰²æŠ–åŠ¨ç­‰ï¼‰æ¥ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### 6.9.5.3: Reconstruction-Based Methods

**English:**
Reconstruction-based methods involve training a model to reconstruct the input data from a corrupted or partial version of it. This can be done using autoencoders, where the model learns to encode the input into a lower-dimensional representation and then decode it back to the original input.

**Chinese:**
åŸºäºé‡å»ºçš„æ–¹æ³•æ¶‰åŠè®­ç»ƒæ¨¡å‹ä»æŸåæˆ–éƒ¨åˆ†ç‰ˆæœ¬çš„è¾“å…¥æ•°æ®ä¸­é‡å»ºåŸå§‹è¾“å…¥ã€‚è¿™å¯ä»¥ä½¿ç”¨è‡ªç¼–ç å™¨æ¥å®ç°ï¼Œæ¨¡å‹å­¦ä¹ å°†è¾“å…¥ç¼–ç ä¸ºä½ç»´è¡¨ç¤ºï¼Œç„¶åå°†å…¶è§£ç å›åŸå§‹è¾“å…¥ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Reconstruction-Based Methodsï¼ˆåŸºäºé‡å»ºçš„æ–¹æ³•ï¼‰**: åŸºäºé‡å»ºçš„æ–¹æ³•æ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡é‡å»ºè¾“å…¥æ•°æ®æ¥å­¦ä¹ ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¸¸ç”¨äºå›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„ç‰¹å¾å­¦ä¹ ã€‚
- **Autoencodersï¼ˆè‡ªç¼–ç å™¨ï¼‰**: è‡ªç¼–ç å™¨æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å°†è¾“å…¥æ•°æ®å‹ç¼©åˆ°ä½ç»´è¡¨ç¤ºå¹¶é‡å»ºè¾“å…¥æ•°æ®æ¥å­¦ä¹ ç‰¹å¾ã€‚

**English:**
For example, in image denoising, the model is trained to reconstruct a clean image from a noisy version of it. This forces the model to learn robust features that are invariant to noise.

**Chinese:**
ä¾‹å¦‚ï¼Œåœ¨å›¾åƒå»å™ªä¸­ï¼Œæ¨¡å‹è¢«è®­ç»ƒä¸ºä»å™ªå£°ç‰ˆæœ¬ä¸­é‡å»ºå¹²å‡€çš„å›¾åƒã€‚è¿™è¿«ä½¿æ¨¡å‹å­¦ä¹ å¯¹å™ªå£°ä¸å˜çš„é²æ£’ç‰¹å¾ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Image Denoisingï¼ˆå›¾åƒå»å™ªï¼‰**: å›¾åƒå»å™ªæ˜¯æŒ‡ä»å™ªå£°å›¾åƒä¸­æ¢å¤åŸå§‹å›¾åƒçš„è¿‡ç¨‹ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹è¿›è¡Œå›¾åƒå»å™ªï¼Œå¯ä»¥å­¦ä¹ åˆ°å¯¹å™ªå£°ä¸å˜çš„å›¾åƒç‰¹å¾ã€‚

### Chapter 6.10: Conclusion

**English:**
Self-supervised learning has emerged as a powerful paradigm for training models on large unlabeled datasets. By designing tasks that do not require labeled data, such as masked language modeling, contrastive learning, and reconstruction-based methods, researchers can leverage the vast amounts of unlabeled data available to train more robust and generalizable models.

**Chinese:**
è‡ªç›‘ç£å­¦ä¹ å·²æˆä¸ºåœ¨å¤§å‹æœªæ ‡è®°æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„æœ‰åŠ›èŒƒå¼ã€‚é€šè¿‡è®¾è®¡ä¸éœ€è¦æ ‡æ³¨æ•°æ®çš„ä»»åŠ¡ï¼Œå¦‚æ©ç è¯­è¨€å»ºæ¨¡ã€å¯¹æ¯”å­¦ä¹ å’ŒåŸºäºé‡å»ºçš„æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®æ¥è®­ç»ƒæ›´é²æ£’å’Œå¯æ³›åŒ–çš„æ¨¡å‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Self-Supervised Learningï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰**: è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§åˆ©ç”¨æœªæ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **Robust Modelsï¼ˆé²æ£’æ¨¡å‹ï¼‰**: é²æ£’æ¨¡å‹æ˜¯æŒ‡å¯¹è¾“å…¥æ•°æ®ä¸­çš„å™ªå£°å’Œå˜åŒ–å…·æœ‰è¾ƒå¼ºé€‚åº”èƒ½åŠ›çš„æ¨¡å‹ã€‚è‡ªç›‘ç£å­¦ä¹ å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°å¯¹å™ªå£°å’Œå˜åŒ–ä¸å˜çš„ç‰¹å¾ã€‚
- **Generalizable Modelsï¼ˆå¯æ³›åŒ–æ¨¡å‹ï¼‰**: å¯æ³›åŒ–æ¨¡å‹æ˜¯æŒ‡åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**English:**
These techniques have been particularly successful in natural language processing and computer vision, where large amounts of unlabeled data are readily available. As the field continues to evolve, we can expect to see even more innovative applications of self-supervised learning in other domains.

**Chinese:**
è¿™äº›æŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­ç‰¹åˆ«æˆåŠŸï¼Œå› ä¸ºè¿™äº›é¢†åŸŸä¸­æœ‰å¤§é‡æœªæ ‡è®°æ•°æ®å¯ç”¨ã€‚éšç€è¯¥é¢†åŸŸçš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æœŸåœ¨å…¶ä»–é¢†åŸŸçœ‹åˆ°æ›´å¤šè‡ªç›‘ç£å­¦ä¹ çš„åˆ›æ–°åº”ç”¨ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Natural Language Processingï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰**: è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯æŒ‡åˆ©ç”¨è®¡ç®—æœºå¤„ç†å’Œç†è§£äººç±»è¯­è¨€çš„æŠ€æœ¯ã€‚è‡ªç›‘ç£å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚
- **Computer Visionï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰**: è®¡ç®—æœºè§†è§‰æ˜¯æŒ‡åˆ©ç”¨è®¡ç®—æœºå¤„ç†å’Œç†è§£å›¾åƒå’Œè§†é¢‘çš„æŠ€æœ¯ã€‚è‡ªç›‘ç£å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚

### æ€»ç»“

æœ¬ç« ä»‹ç»äº†è‡ªç›‘ç£å­¦ä¹ çš„å‡ ç§ä¸»è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ©ç è¯­è¨€å»ºæ¨¡ã€å¯¹æ¯”å­¦ä¹ å’ŒåŸºäºé‡å»ºçš„æ–¹æ³•ã€‚è¿™äº›æŠ€æœ¯é€šè¿‡è®¾è®¡ä¸éœ€è¦æ ‡æ³¨æ•°æ®çš„ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è‡ªç›‘ç£å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¹¶æœ‰æœ›åœ¨å…¶ä»–é¢†åŸŸå¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

#### 6.10.1: Future Directions in Self-Supervised Learning

**English:**
As self-supervised learning continues to advance, several promising directions are emerging. One area of focus is the development of more sophisticated pretext tasks that can better capture the underlying structure of the data. Another direction is the integration of self-supervised learning with other learning paradigms, such as reinforcement learning and meta-learning, to create more versatile and adaptive models.

**Chinese:**
éšç€è‡ªç›‘ç£å­¦ä¹ çš„ä¸æ–­å‘å±•ï¼Œä¸€äº›æœ‰å‰æ™¯çš„æ–¹å‘æ­£åœ¨æ¶Œç°ã€‚ä¸€ä¸ªé‡ç‚¹é¢†åŸŸæ˜¯å¼€å‘æ›´å¤æ‚çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ•°æ®çš„åº•å±‚ç»“æ„ã€‚å¦ä¸€ä¸ªæ–¹å‘æ˜¯å°†è‡ªç›‘ç£å­¦ä¹ ä¸å…¶ä»–å­¦ä¹ èŒƒå¼ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ å’Œå…ƒå­¦ä¹ ï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥åˆ›å»ºæ›´é€šç”¨å’Œè‡ªé€‚åº”çš„æ¨¡å‹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Pretext Tasksï¼ˆé¢„è®­ç»ƒä»»åŠ¡ï¼‰**: é¢„è®­ç»ƒä»»åŠ¡æ˜¯æŒ‡è‡ªç›‘ç£å­¦ä¹ ä¸­è®¾è®¡çš„ä»»åŠ¡ï¼Œç”¨äºåœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒæ¨¡å‹ã€‚æ›´å¤æ‚çš„é¢„è®­ç»ƒä»»åŠ¡å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£æ•°æ®çš„ç»“æ„ã€‚
- **Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰**: å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ ç­–ç•¥çš„å­¦ä¹ èŒƒå¼ã€‚å°†è‡ªç›‘ç£å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚
- **Meta-Learningï¼ˆå…ƒå­¦ä¹ ï¼‰**: å…ƒå­¦ä¹ æ˜¯æŒ‡å­¦ä¹ å¦‚ä½•å­¦ä¹ çš„æŠ€æœ¯ã€‚é€šè¿‡å°†è‡ªç›‘ç£å­¦ä¹ ä¸å…ƒå­¦ä¹ ç»“åˆï¼Œå¯ä»¥åˆ›å»ºèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„æ¨¡å‹ã€‚

**English:**
Additionally, there is growing interest in applying self-supervised learning to multimodal data, where the goal is to learn representations that are consistent across different types of data, such as text, images, and audio. This can enable more powerful and flexible models that can understand and generate content across multiple modalities.

**Chinese:**
æ­¤å¤–ï¼Œå°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºå¤šæ¨¡æ€æ•°æ®çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œå…¶ç›®æ ‡æ˜¯å­¦ä¹ è·¨ä¸åŒç±»å‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ï¼‰çš„ä¸€è‡´è¡¨ç¤ºã€‚è¿™å¯ä»¥åˆ›å»ºæ›´å¼ºå¤§å’Œçµæ´»çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè·¨å¤šç§æ¨¡æ€çš„å†…å®¹ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Multimodal Dataï¼ˆå¤šæ¨¡æ€æ•°æ®ï¼‰**: å¤šæ¨¡æ€æ•°æ®æ˜¯æŒ‡åŒ…å«å¤šç§ç±»å‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰çš„æ•°æ®é›†ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œå¯ä»¥å­¦ä¹ åˆ°è·¨æ¨¡æ€çš„ä¸€è‡´è¡¨ç¤ºã€‚
- **Consistent Representationsï¼ˆä¸€è‡´è¡¨ç¤ºï¼‰**: ä¸€è‡´è¡¨ç¤ºæ˜¯æŒ‡åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å…±äº«çš„ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å­¦ä¹ ä¸€è‡´è¡¨ç¤ºï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆè·¨æ¨¡æ€çš„å†…å®¹ã€‚

#### 6.10.2: Challenges and Limitations

**English:**
Despite its successes, self-supervised learning still faces several challenges. One major challenge is the design of pretext tasks that can effectively capture the complexity of real-world data. Another challenge is the scalability of self-supervised learning methods, particularly when dealing with extremely large datasets and models.

**Chinese:**
å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œè‡ªç›‘ç£å­¦ä¹ ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç°å®ä¸–ç•Œæ•°æ®å¤æ‚æ€§çš„é¢„è®­ç»ƒä»»åŠ¡ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æå¤§æ•°æ®é›†å’Œæ¨¡å‹æ—¶ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Complexity of Real-World Dataï¼ˆç°å®ä¸–ç•Œæ•°æ®çš„å¤æ‚æ€§ï¼‰**: ç°å®ä¸–ç•Œæ•°æ®é€šå¸¸å…·æœ‰å¤æ‚çš„ç»“æ„å’Œå™ªå£°ï¼Œè®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¿™äº›å¤æ‚æ€§çš„é¢„è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚
- **Scalabilityï¼ˆå¯æ‰©å±•æ€§ï¼‰**: å¯æ‰©å±•æ€§æ˜¯æŒ‡æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æå¤§æ•°æ®é›†å’Œæ¨¡å‹æ—¶ï¼Œéœ€è¦è§£å†³è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬çš„é—®é¢˜ã€‚

**English:**
Furthermore, while self-supervised learning can reduce the need for labeled data, it does not eliminate it entirely. In many cases, fine-tuning on a small amount of labeled data is still necessary to achieve optimal performance on specific tasks.

**Chinese:**
æ­¤å¤–ï¼Œå°½ç®¡è‡ªç›‘ç£å­¦ä¹ å¯ä»¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œä½†å®ƒå¹¶ä¸èƒ½å®Œå…¨æ¶ˆé™¤è¿™ç§éœ€æ±‚ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä»ç„¶éœ€è¦åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Fine-Tuningï¼ˆå¾®è°ƒï¼‰**: å¾®è°ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ã€‚å°½ç®¡è‡ªç›‘ç£å­¦ä¹ å¯ä»¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œå¾®è°ƒä»ç„¶æ˜¯å¿…è¦çš„ã€‚

#### 6.10.3: Ethical Considerations

**English:**
As with any powerful technology, self-supervised learning raises important ethical considerations. One concern is the potential for bias in the learned representations, particularly when the training data contains biases. Ensuring that self-supervised learning models are fair and unbiased is an ongoing area of research.

**Chinese:**
ä¸ä»»ä½•å¼ºå¤§çš„æŠ€æœ¯ä¸€æ ·ï¼Œè‡ªç›‘ç£å­¦ä¹ ä¹Ÿå¼•å‘äº†é‡è¦çš„ä¼¦ç†è€ƒè™‘ã€‚ä¸€ä¸ªæ‹…å¿§æ˜¯å­¦ä¹ åˆ°çš„è¡¨ç¤ºå¯èƒ½å­˜åœ¨åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®åŒ…å«åè§çš„æƒ…å†µä¸‹ã€‚ç¡®ä¿è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å…¬å¹³ä¸”æ— åè§æ˜¯ä¸€ä¸ªæŒç»­çš„ç ”ç©¶é¢†åŸŸã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Bias in Representationsï¼ˆè¡¨ç¤ºä¸­çš„åè§ï¼‰**: è¡¨ç¤ºä¸­çš„åè§æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯èƒ½ç»§æ‰¿è®­ç»ƒæ•°æ®ä¸­çš„åè§ï¼Œå¯¼è‡´ä¸å…¬å¹³çš„ç»“æœã€‚æ¶ˆé™¤è¡¨ç¤ºä¸­çš„åè§æ˜¯è‡ªç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚
- **Fairnessï¼ˆå…¬å¹³æ€§ï¼‰**: å…¬å¹³æ€§æ˜¯æŒ‡æ¨¡å‹åœ¨ä¸åŒç¾¤ä½“ä¹‹é—´è¡¨ç°ä¸€è‡´ï¼Œä¸å› æ€§åˆ«ã€ç§æ—ç­‰å› ç´ è€Œäº§ç”Ÿåè§ã€‚ç¡®ä¿è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„å…¬å¹³æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„ä¼¦ç†é—®é¢˜ã€‚

**English:**
Another ethical consideration is the potential for misuse of self-supervised learning models, such as in the creation of deepfakes or other forms of synthetic media. It is important for researchers and practitioners to consider the societal impact of their work and to develop guidelines for responsible use.

**Chinese:**
å¦ä¸€ä¸ªä¼¦ç†è€ƒè™‘æ˜¯è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å¯èƒ½è¢«æ»¥ç”¨çš„é£é™©ï¼Œä¾‹å¦‚ç”¨äºåˆ›å»ºæ·±åº¦ä¼ªé€ æˆ–å…¶ä»–å½¢å¼çš„åˆæˆåª’ä½“ã€‚ç ”ç©¶äººå‘˜å’Œå®è·µè€…éœ€è¦è€ƒè™‘å…¶å·¥ä½œçš„ç¤¾ä¼šå½±å“ï¼Œå¹¶åˆ¶å®šè´Ÿè´£ä»»ä½¿ç”¨çš„æŒ‡å—ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Misuse of Modelsï¼ˆæ¨¡å‹çš„æ»¥ç”¨ï¼‰**: æ¨¡å‹çš„æ»¥ç”¨æ˜¯æŒ‡å°†æ¨¡å‹ç”¨äºä¸é“å¾·æˆ–éæ³•çš„ç›®çš„ï¼Œå¦‚ç”Ÿæˆè™šå‡ä¿¡æ¯æˆ–ä¾µçŠ¯éšç§ã€‚é˜²æ­¢æ¨¡å‹æ»¥ç”¨æ˜¯è‡ªç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦ä¼¦ç†é—®é¢˜ã€‚
- **Responsible Useï¼ˆè´Ÿè´£ä»»çš„ä½¿ç”¨ï¼‰**: è´Ÿè´£ä»»çš„ä½¿ç”¨æ˜¯æŒ‡åœ¨ä½¿ç”¨æŠ€æœ¯æ—¶è€ƒè™‘å…¶æ½œåœ¨çš„ç¤¾ä¼šå½±å“ï¼Œå¹¶é‡‡å–æªæ–½é˜²æ­¢æ»¥ç”¨ã€‚åˆ¶å®šè´Ÿè´£ä»»ä½¿ç”¨çš„æŒ‡å—æ˜¯ç¡®ä¿æŠ€æœ¯é€ ç¦ç¤¾ä¼šçš„é‡è¦æ­¥éª¤ã€‚

### æ€»ç»“

æœ¬ç« æ€»ç»“äº†è‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥æ–¹å‘ã€æŒ‘æˆ˜å’Œä¼¦ç†è€ƒè™‘ã€‚éšç€è‡ªç›‘ç£å­¦ä¹ çš„ä¸æ–­å‘å±•ï¼Œå¼€å‘æ›´å¤æ‚çš„é¢„è®­ç»ƒä»»åŠ¡ã€ç»“åˆå…¶ä»–å­¦ä¹ èŒƒå¼ä»¥åŠåº”ç”¨äºå¤šæ¨¡æ€æ•°æ®æ˜¯æœªæ¥çš„é‡è¦æ–¹å‘ã€‚ç„¶è€Œï¼Œè‡ªç›‘ç£å­¦ä¹ ä»ç„¶é¢ä¸´è®¾è®¡æœ‰æ•ˆé¢„è®­ç»ƒä»»åŠ¡ã€æé«˜å¯æ‰©å±•æ€§ä»¥åŠç¡®ä¿å…¬å¹³æ€§å’Œé˜²æ­¢æ»¥ç”¨ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜å’Œå®è·µè€…éœ€è¦åœ¨è¿™äº›é¢†åŸŸç»§ç»­åŠªåŠ›ï¼Œä»¥ç¡®ä¿è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯çš„å¥åº·å‘å±•å’Œç¤¾ä¼šæ•ˆç›Šã€‚

### Chapter 6.11: Final Thoughts

#### 6.11.1: The Impact of Self-Supervised Learning

**English:**
Self-supervised learning has already had a profound impact on the field of machine learning, enabling the training of powerful models on vast amounts of unlabeled data. This has led to significant advancements in natural language processing, computer vision, and other domains, where labeled data is often scarce or expensive to obtain.

**Chinese:**
è‡ªç›‘ç£å­¦ä¹ å·²ç»å¯¹æœºå™¨å­¦ä¹ é¢†åŸŸäº§ç”Ÿäº†æ·±è¿œçš„å½±å“ï¼Œä½¿å¾—èƒ½å¤Ÿåœ¨å¤§é‡æœªæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒå¼ºå¤§çš„æ¨¡å‹ã€‚è¿™å¯¼è‡´äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå…¶ä»–é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ ‡æ³¨æ•°æ®é€šå¸¸ç¨€ç¼ºæˆ–è·å–æˆæœ¬é«˜æ˜‚ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Impact on Machine Learningï¼ˆå¯¹æœºå™¨å­¦ä¹ çš„å½±å“ï¼‰**: è‡ªç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œæ˜¾è‘—æ‰©å±•äº†æœºå™¨å­¦ä¹ çš„åº”ç”¨èŒƒå›´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨æˆæœ¬é«˜çš„é¢†åŸŸã€‚
- **Advancements in NLP and CVï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰çš„è¿›æ­¥ï¼‰**: è‡ªç›‘ç£å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼Œå¦‚BERTå’Œå¯¹æ¯”å­¦ä¹ ï¼Œæå¤§åœ°æ¨åŠ¨äº†è¿™äº›é¢†åŸŸçš„å‘å±•ã€‚

**English:**
As the field continues to evolve, self-supervised learning is likely to play an increasingly important role in the development of artificial intelligence. By reducing the reliance on labeled data, it opens up new possibilities for training models in domains where data annotation is challenging or impractical.

**Chinese:**
éšç€è¯¥é¢†åŸŸçš„ä¸æ–­å‘å±•ï¼Œè‡ªç›‘ç£å­¦ä¹ å¾ˆå¯èƒ½åœ¨äººå·¥æ™ºèƒ½çš„å‘å±•ä¸­æ‰®æ¼”è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚é€šè¿‡å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå®ƒä¸ºåœ¨æ•°æ®æ ‡æ³¨å…·æœ‰æŒ‘æˆ˜æ€§æˆ–ä¸å¯è¡Œçš„é¢†åŸŸä¸­è®­ç»ƒæ¨¡å‹å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Reduced Reliance on Labeled Dataï¼ˆå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼‰**: è‡ªç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œå‡å°‘äº†å¯¹æ˜‚è´µä¸”è€—æ—¶çš„æ•°æ®æ ‡æ³¨çš„ä¾èµ–ï¼Œä»è€Œæ‰©å±•äº†æœºå™¨å­¦ä¹ çš„åº”ç”¨èŒƒå›´ã€‚
- **New Possibilitiesï¼ˆæ–°çš„å¯èƒ½æ€§ï¼‰**: è‡ªç›‘ç£å­¦ä¹ ä¸ºåœ¨æ•°æ®æ ‡æ³¨å›°éš¾çš„é¢†åŸŸï¼ˆå¦‚åŒ»å­¦å½±åƒåˆ†æï¼‰ä¸­è®­ç»ƒæ¨¡å‹æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

#### 6.11.2: The Role of the Research Community

**English:**
The research community plays a crucial role in advancing self-supervised learning. By developing new techniques, sharing datasets, and fostering collaboration, researchers can accelerate progress and ensure that the benefits of self-supervised learning are widely accessible.

**Chinese:**
ç ”ç©¶ç¤¾åŒºåœ¨æ¨åŠ¨è‡ªç›‘ç£å­¦ä¹ æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚é€šè¿‡å¼€å‘æ–°æŠ€æœ¯ã€å…±äº«æ•°æ®é›†å’Œä¿ƒè¿›åˆä½œï¼Œç ”ç©¶äººå‘˜å¯ä»¥åŠ é€Ÿè¿›å±•ï¼Œå¹¶ç¡®ä¿è‡ªç›‘ç£å­¦ä¹ çš„å¥½å¤„èƒ½å¤Ÿå¹¿æ³›æ™®åŠã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Research Communityï¼ˆç ”ç©¶ç¤¾åŒºï¼‰**: ç ”ç©¶ç¤¾åŒºæ˜¯æŒ‡ä»äº‹ç§‘å­¦ç ”ç©¶çš„äººå‘˜å’Œç»„ç»‡ï¼Œé€šè¿‡åˆä½œå’ŒçŸ¥è¯†å…±äº«æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚
- **Collaborationï¼ˆåˆä½œï¼‰**: åˆä½œæ˜¯æŒ‡ç ”ç©¶äººå‘˜ä¹‹é—´çš„ååŒå·¥ä½œï¼Œé€šè¿‡å…±äº«èµ„æºå’ŒçŸ¥è¯†ï¼ŒåŠ é€ŸæŠ€æœ¯çš„å‘å±•å’Œæ™®åŠã€‚

**English:**
Open-source initiatives and public datasets have been instrumental in the rapid adoption of self-supervised learning techniques. Continued support for these initiatives will be essential for maintaining the momentum of progress in the field.

**Chinese:**
å¼€æºé¡¹ç›®å’Œå…¬å…±æ•°æ®é›†åœ¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿé‡‡ç”¨ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç»§ç»­æ”¯æŒè¿™äº›é¡¹ç›®å¯¹äºä¿æŒè¯¥é¢†åŸŸçš„è¿›å±•åŠ¿å¤´è‡³å…³é‡è¦ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Open-Source Initiativesï¼ˆå¼€æºé¡¹ç›®ï¼‰**: å¼€æºé¡¹ç›®æ˜¯æŒ‡å…¬å¼€æºä»£ç çš„è½¯ä»¶é¡¹ç›®ï¼Œå…è®¸ä»»ä½•äººä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ã€‚å¼€æºé¡¹ç›®åœ¨æ¨åŠ¨æŠ€æœ¯æ™®åŠå’Œåˆ›æ–°æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚
- **Public Datasetsï¼ˆå…¬å…±æ•°æ®é›†ï¼‰**: å…¬å…±æ•°æ®é›†æ˜¯æŒ‡å…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚å…¬å…±æ•°æ®é›†ä¸ºè‡ªç›‘ç£å­¦ä¹ çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦çš„èµ„æºã€‚

#### 6.11.3: The Future of Self-Supervised Learning

**English:**
Looking ahead, the future of self-supervised learning is bright. As techniques continue to improve and new applications are discovered, self-supervised learning will likely become a cornerstone of artificial intelligence, enabling the development of more intelligent, adaptable, and efficient models.

**Chinese:**
å±•æœ›æœªæ¥ï¼Œè‡ªç›‘ç£å­¦ä¹ çš„å‰æ™¯å…‰æ˜ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­æ”¹è¿›å’Œæ–°åº”ç”¨çš„å‘ç°ï¼Œè‡ªç›‘ç£å­¦ä¹ å¾ˆå¯èƒ½æˆä¸ºäººå·¥æ™ºèƒ½çš„åŸºçŸ³ï¼Œæ¨åŠ¨æ›´æ™ºèƒ½ã€æ›´é€‚åº”æ€§å¼ºå’Œæ›´é«˜æ•ˆçš„æ¨¡å‹çš„å‘å±•ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Future of Self-Supervised Learningï¼ˆè‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥ï¼‰**: è‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥å……æ»¡å¸Œæœ›ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œå®ƒå°†åœ¨äººå·¥æ™ºèƒ½ä¸­å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚
- **Intelligent and Adaptable Modelsï¼ˆæ™ºèƒ½å’Œé€‚åº”æ€§å¼ºçš„æ¨¡å‹ï¼‰**: è‡ªç›‘ç£å­¦ä¹ æœ‰åŠ©äºå¼€å‘æ›´æ™ºèƒ½å’Œé€‚åº”æ€§å¼ºçš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„æ•°æ®ã€‚

**English:**
Ultimately, the goal of self-supervised learning is to create models that can learn from the world in a way that is more akin to human learningâ€”by observing, interacting, and making sense of the environment without the need for explicit supervision. Achieving this goal will bring us closer to realizing the full potential of artificial intelligence.

**Chinese:**
æœ€ç»ˆï¼Œè‡ªç›‘ç£å­¦ä¹ çš„ç›®æ ‡æ˜¯åˆ›å»ºèƒ½å¤Ÿä»¥æ›´ç±»ä¼¼äºäººç±»å­¦ä¹ çš„æ–¹å¼ä»ä¸–ç•Œä¸­å­¦ä¹ çš„æ¨¡å‹â€”â€”é€šè¿‡è§‚å¯Ÿã€äº’åŠ¨å’Œç†è§£ç¯å¢ƒï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç›‘ç£ã€‚å®ç°è¿™ä¸€ç›®æ ‡å°†ä½¿æˆ‘ä»¬æ›´æ¥è¿‘å®ç°äººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Human-Like Learningï¼ˆç±»äººå­¦ä¹ ï¼‰**: ç±»äººå­¦ä¹ æ˜¯æŒ‡æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·é€šè¿‡è§‚å¯Ÿå’Œäº’åŠ¨æ¥å­¦ä¹ ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„æŒ‡å¯¼ã€‚è‡ªç›‘ç£å­¦ä¹ æ˜¯å®ç°ç±»äººå­¦ä¹ çš„é‡è¦ä¸€æ­¥ã€‚
- **Full Potential of AIï¼ˆäººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›ï¼‰**: äººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›æ˜¯æŒ‡AIèƒ½å¤Ÿåœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸äººç±»ç›¸å½“ç”šè‡³è¶…è¶Šäººç±»çš„èƒ½åŠ›ã€‚è‡ªç›‘ç£å­¦ä¹ æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚

### æ€»ç»“

æœ¬ç« æ€»ç»“äº†è‡ªç›‘ç£å­¦ä¹ çš„å½±å“ã€ç ”ç©¶ç¤¾åŒºçš„ä½œç”¨ä»¥åŠæœªæ¥çš„å‘å±•æ–¹å‘ã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸçš„è¿›æ­¥ã€‚ç ”ç©¶ç¤¾åŒºé€šè¿‡å¼€å‘æ–°æŠ€æœ¯ã€å…±äº«èµ„æºå’Œä¿ƒè¿›åˆä½œï¼ŒåŠ é€Ÿäº†è‡ªç›‘ç£å­¦ä¹ çš„å‘å±•ã€‚æœªæ¥ï¼Œè‡ªç›‘ç£å­¦ä¹ æœ‰æœ›æˆä¸ºäººå·¥æ™ºèƒ½çš„åŸºçŸ³ï¼Œæ¨åŠ¨æ›´æ™ºèƒ½ã€æ›´é€‚åº”æ€§å¼ºçš„æ¨¡å‹çš„å‘å±•ï¼Œæœ€ç»ˆå®ç°ç±»äººå­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›ã€‚

### Chapter 6.12: Final Words

#### 6.12.1: The Journey Ahead

**English:**
The journey of self-supervised learning is still in its early stages, and there is much to explore and discover. As researchers continue to push the boundaries of what is possible, we can expect to see even more innovative applications and breakthroughs in the coming years.

**Chinese:**
è‡ªç›‘ç£å­¦ä¹ çš„æ—…ç¨‹ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œè¿˜æœ‰è®¸å¤šéœ€è¦æ¢ç´¢å’Œå‘ç°çš„åœ°æ–¹ã€‚éšç€ç ”ç©¶äººå‘˜ä¸æ–­çªç ´å¯èƒ½çš„ç•Œé™ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æœŸåœ¨æœªæ¥å‡ å¹´å†…çœ‹åˆ°æ›´å¤šåˆ›æ–°çš„åº”ç”¨å’Œçªç ´ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Early Stagesï¼ˆæ—©æœŸé˜¶æ®µï¼‰**: è‡ªç›‘ç£å­¦ä¹ ä½œä¸ºä¸€ä¸ªç ”ç©¶é¢†åŸŸï¼Œä»å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µï¼Œæœªæ¥æœ‰è®¸å¤šæ½œåœ¨çš„ç ”ç©¶æ–¹å‘å’Œåº”ç”¨åœºæ™¯ã€‚
- **Innovative Applicationsï¼ˆåˆ›æ–°åº”ç”¨ï¼‰**: è‡ªç›‘ç£å­¦ä¹ çš„åˆ›æ–°åº”ç”¨åŒ…æ‹¬è·¨æ¨¡æ€å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ç­‰ï¼Œè¿™äº›åº”ç”¨å°†è¿›ä¸€æ­¥æ‰©å±•äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ã€‚

**English:**
The potential of self-supervised learning to transform industries and improve our daily lives is immense. From healthcare to education, from autonomous vehicles to personalized recommendations, the possibilities are endless.

**Chinese:**
è‡ªç›‘ç£å­¦ä¹ åœ¨æ”¹å˜è¡Œä¸šå’Œæ”¹å–„æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»æ–¹é¢çš„æ½œåŠ›æ˜¯å·¨å¤§çš„ã€‚ä»åŒ»ç–—ä¿å¥åˆ°æ•™è‚²ï¼Œä»è‡ªåŠ¨é©¾é©¶åˆ°ä¸ªæ€§åŒ–æ¨èï¼Œå¯èƒ½æ€§æ˜¯æ— é™çš„ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Transform Industriesï¼ˆæ”¹å˜è¡Œä¸šï¼‰**: è‡ªç›‘ç£å­¦ä¹ å¯ä»¥é€šè¿‡æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œæ¨åŠ¨å„ä¸ªè¡Œä¸šçš„æ•°å­—åŒ–è½¬å‹ã€‚
- **Improve Daily Livesï¼ˆæ”¹å–„æ—¥å¸¸ç”Ÿæ´»ï¼‰**: è‡ªç›‘ç£å­¦ä¹ å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€ä¸ªæ€§åŒ–æ¨èç­‰é¢†åŸŸï¼Œç›´æ¥æ”¹å–„äººä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä½“éªŒã€‚

#### 6.12.2: A Call to Action

**English:**
As we look to the future, it is important for researchers, practitioners, and policymakers to work together to ensure that the benefits of self-supervised learning are realized in a way that is ethical, fair, and beneficial to all. This includes addressing challenges such as bias, scalability, and misuse, while also fostering innovation and collaboration.

**Chinese:**
å±•æœ›æœªæ¥ï¼Œç ”ç©¶äººå‘˜ã€ä»ä¸šè€…å’Œæ”¿ç­–åˆ¶å®šè€…éœ€è¦å…±åŒåŠªåŠ›ï¼Œç¡®ä¿è‡ªç›‘ç£å­¦ä¹ çš„å¥½å¤„èƒ½å¤Ÿä»¥é“å¾·ã€å…¬å¹³å’Œæœ‰ç›Šäºæ‰€æœ‰äººçš„æ–¹å¼å®ç°ã€‚è¿™åŒ…æ‹¬è§£å†³åè§ã€å¯æ‰©å±•æ€§å’Œæ»¥ç”¨ç­‰æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿ƒè¿›åˆ›æ–°å’Œåˆä½œã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Ethical and Fairï¼ˆé“å¾·å’Œå…¬å¹³ï¼‰**: åœ¨å¼€å‘å’Œåº”ç”¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯æ—¶ï¼Œå¿…é¡»ç¡®ä¿å…¶ç¬¦åˆé“å¾·æ ‡å‡†ï¼Œå¹¶ä¸”å¯¹æ‰€æœ‰ç¾¤ä½“å…¬å¹³ã€‚
- **Addressing Challengesï¼ˆè§£å†³æŒ‘æˆ˜ï¼‰**: è§£å†³è‡ªç›‘ç£å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®åè§ã€æ¨¡å‹å¯æ‰©å±•æ€§å’ŒæŠ€æœ¯æ»¥ç”¨ï¼Œæ˜¯å®ç°å…¶æ½œåŠ›çš„å…³é”®ã€‚

**English:**
By embracing the principles of openness, transparency, and inclusivity, we can create a future where self-supervised learning not only advances technology but also contributes to a more equitable and sustainable world.

**Chinese:**
é€šè¿‡æ‹¥æŠ±å¼€æ”¾ã€é€æ˜å’ŒåŒ…å®¹çš„åŸåˆ™ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›é€ ä¸€ä¸ªæœªæ¥ï¼Œåœ¨è¿™ä¸ªæœªæ¥ä¸­ï¼Œè‡ªç›‘ç£å­¦ä¹ ä¸ä»…æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ï¼Œè¿˜ä¸ºæ›´å…¬å¹³å’Œå¯æŒç»­çš„ä¸–ç•Œåšå‡ºè´¡çŒ®ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Openness and Transparencyï¼ˆå¼€æ”¾å’Œé€æ˜ï¼‰**: å¼€æ”¾å’Œé€æ˜æ˜¯æŒ‡ç ”ç©¶å’ŒæŠ€æœ¯å¼€å‘è¿‡ç¨‹ä¸­çš„ä¿¡æ¯å…¬å¼€å’Œå…±äº«ï¼Œè¿™æœ‰åŠ©äºä¿ƒè¿›åˆä½œå’Œåˆ›æ–°ã€‚
- **Inclusivityï¼ˆåŒ…å®¹æ€§ï¼‰**: åŒ…å®¹æ€§æ˜¯æŒ‡ç¡®ä¿æŠ€æœ¯å¼€å‘å’Œåº”ç”¨çš„å—ç›Šè€…åŒ…æ‹¬æ‰€æœ‰ç¾¤ä½“ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¼ ç»Ÿä¸Šè¢«è¾¹ç¼˜åŒ–çš„ç¾¤ä½“ã€‚

#### 6.12.3: Final Thoughts

**English:**
Self-supervised learning represents a significant step forward in the quest to create machines that can learn and reason like humans. While there are still many challenges to overcome, the progress made so far is a testament to the power of innovation and collaboration.

**Chinese:**
è‡ªç›‘ç£å­¦ä¹ ä»£è¡¨äº†åœ¨åˆ›é€ èƒ½å¤Ÿåƒäººç±»ä¸€æ ·å­¦ä¹ å’Œæ¨ç†çš„æœºå™¨çš„é“è·¯ä¸Šè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚å°½ç®¡ä»æœ‰è®¸å¤šæŒ‘æˆ˜éœ€è¦å…‹æœï¼Œä½†è¿„ä»Šä¸ºæ­¢çš„è¿›å±•è¯æ˜äº†åˆ›æ–°å’Œåˆä½œçš„åŠ›é‡ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Human-Like Learning and Reasoningï¼ˆç±»äººå­¦ä¹ å’Œæ¨ç†ï¼‰**: è‡ªç›‘ç£å­¦ä¹ æ˜¯å®ç°ç±»äººå­¦ä¹ å’Œæ¨ç†çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿æœºå™¨èƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªä¸»å­¦ä¹ å’Œç†è§£ã€‚
- **Power of Innovation and Collaborationï¼ˆåˆ›æ–°å’Œåˆä½œçš„åŠ›é‡ï¼‰**: åˆ›æ–°å’Œåˆä½œæ˜¯æ¨åŠ¨æŠ€æœ¯è¿›æ­¥çš„æ ¸å¿ƒåŠ¨åŠ›ï¼Œè‡ªç›‘ç£å­¦ä¹ çš„æˆåŠŸç¦»ä¸å¼€ç ”ç©¶ç¤¾åŒºçš„å…±åŒåŠªåŠ›ã€‚

**English:**
As we continue to explore the possibilities of self-supervised learning, let us remain committed to the principles of ethical research and responsible innovation. Together, we can unlock the full potential of artificial intelligence and create a better future for all.

**Chinese:**
åœ¨æˆ‘ä»¬ç»§ç»­æ¢ç´¢è‡ªç›‘ç£å­¦ä¹ çš„å¯èƒ½æ€§æ—¶ï¼Œè®©æˆ‘ä»¬å§‹ç»ˆåšæŒé“å¾·ç ”ç©¶å’Œè´Ÿè´£ä»»åˆ›æ–°çš„åŸåˆ™ã€‚å…±åŒåŠªåŠ›ï¼Œæˆ‘ä»¬å¯ä»¥é‡Šæ”¾äººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›ï¼Œä¸ºæ‰€æœ‰äººåˆ›é€ ä¸€ä¸ªæ›´ç¾å¥½çš„æœªæ¥ã€‚

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Ethical Researchï¼ˆé“å¾·ç ”ç©¶ï¼‰**: é“å¾·ç ”ç©¶æ˜¯æŒ‡åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­éµå¾ªé“å¾·è§„èŒƒï¼Œç¡®ä¿ç ”ç©¶ç»“æœå¯¹ç¤¾ä¼šæœ‰ç›Šã€‚
- **Responsible Innovationï¼ˆè´Ÿè´£ä»»åˆ›æ–°ï¼‰**: è´Ÿè´£ä»»åˆ›æ–°æ˜¯æŒ‡åœ¨æŠ€æœ¯å¼€å‘å’Œåº”ç”¨è¿‡ç¨‹ä¸­è€ƒè™‘å…¶ç¤¾ä¼šå½±å“ï¼Œç¡®ä¿æŠ€æœ¯é€ ç¦å…¨äººç±»ã€‚

### æ€»ç»“

æœ¬ç« æ€»ç»“äº†è‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥å‰æ™¯ã€ç ”ç©¶ç¤¾åŒºçš„ä½œç”¨ä»¥åŠå®ç°å…¶æ½œåŠ›çš„å…³é”®åŸåˆ™ã€‚è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸå±•ç°äº†å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜ã€ä»ä¸šè€…å’Œæ”¿ç­–åˆ¶å®šè€…éœ€è¦å…±åŒåŠªåŠ›ï¼Œè§£å†³æŠ€æœ¯æŒ‘æˆ˜ï¼Œç¡®ä¿è‡ªç›‘ç£å­¦ä¹ çš„å‘å±•ç¬¦åˆé“å¾·å’Œå…¬å¹³åŸåˆ™ã€‚é€šè¿‡å¼€æ”¾ã€é€æ˜å’ŒåŒ…å®¹çš„åˆ›æ–°ï¼Œæˆ‘ä»¬å¯ä»¥é‡Šæ”¾äººå·¥æ™ºèƒ½çš„å…¨éƒ¨æ½œåŠ›ï¼Œä¸ºæ‰€æœ‰äººåˆ›é€ ä¸€ä¸ªæ›´ç¾å¥½çš„æœªæ¥ã€‚




### Chapter 7: Synthesis

#### 7.1 Text Generation

**Text Generation (æ–‡æœ¬ç”Ÿæˆ)** is the process of creating coherent and contextually relevant text using machine learning models. The standard approach to text synthesis is to use an **attention-based, autoregressive model (åŸºäºæ³¨æ„åŠ›çš„è‡ªå›å½’æ¨¡å‹)**. A very successful model proposed by Radford et al. [2018] is the **GPT (Generative Pre-trained Transformer, ç”Ÿæˆå¼é¢„è®­ç»ƒå˜å‹å™¨)**, which we described in Â§ 5.3.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Autoregressive Model (è‡ªå›å½’æ¨¡å‹):** è‡ªå›å½’æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ­¥ç”Ÿæˆåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ æ¥ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®å‰é¢ç”Ÿæˆçš„è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚
- **Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶):** æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶å…³æ³¨åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚

This architecture has been used for very large models, such as OpenAIâ€™s 175-billion-parameter **GPT-3** [Brown et al., 2020]. It is composed of 96 **self-attention blocks (è‡ªæ³¨æ„åŠ›å—)**, each with 96 heads, and processes tokens of dimension 12,288, with a hidden dimension of 49,512 in the **MLPs (å¤šå±‚æ„ŸçŸ¥å™¨)** of the attention blocks.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Self-Attention Block (è‡ªæ³¨æ„åŠ›å—):** è‡ªæ³¨æ„åŠ›å—æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡è®¡ç®—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ ä¸å…¶ä»–å…ƒç´ çš„ç›¸å…³æ€§æ¥æ•æ‰åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚
- **MLP (å¤šå±‚æ„ŸçŸ¥å™¨):** å¤šå±‚æ„ŸçŸ¥å™¨æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼Œé€šå¸¸ç”¨äºå¤„ç†éçº¿æ€§å…³ç³»ã€‚

When such a model is trained on a very large dataset, it results in a **Large Language Model (LLM, å¤§è¯­è¨€æ¨¡å‹)**, which exhibits extremely powerful properties. Besides the syntactic and grammatical structure of the language, it has to integrate very diverse knowledge, e.g., to predict the word following â€œThe capital of Japan isâ€, â€œif water is heated to 100 Celsius degrees it turns intoâ€, or â€œbecause her puppy was sick, Jane wasâ€.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Large Language Model (å¤§è¯­è¨€æ¨¡å‹):** å¤§è¯­è¨€æ¨¡å‹æ˜¯é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„æ–‡æœ¬å¹¶æ‰§è¡Œå¤šç§è¯­è¨€ä»»åŠ¡ã€‚

This results in particular in the ability to solve **few-shot prediction (å°‘æ ·æœ¬é¢„æµ‹)**, where only a handful of training examples are available, as illustrated in Figure 7.1. More surprisingly, when given a carefully crafted **prompt (æç¤º)**, it can exhibit abilities for **question answering (é—®ç­”)**, **problem solving (é—®é¢˜è§£å†³)**, and **chain-of-thought (æ€ç»´é“¾)** that appear eerily close to high-level reasoning [Chowdhery et al., 2022; Bubeck et al., 2023].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Few-Shot Prediction (å°‘æ ·æœ¬é¢„æµ‹):** å°‘æ ·æœ¬é¢„æµ‹æ˜¯æŒ‡æ¨¡å‹åœ¨åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„é¢„æµ‹ã€‚
- **Chain-of-Thought (æ€ç»´é“¾):** æ€ç»´é“¾æ˜¯ä¸€ç§æç¤ºæ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹ç”Ÿæˆä¸­é—´æ­¥éª¤æ¥å¼•å¯¼å…¶ç”Ÿæˆæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚

Due to these remarkable capabilities, these models are sometimes called **foundation models (åŸºç¡€æ¨¡å‹)** [Bommasani et al., 2021].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Foundation Models (åŸºç¡€æ¨¡å‹):** åŸºç¡€æ¨¡å‹æ˜¯æŒ‡åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚

However, even though it integrates a very large body of knowledge, such a model may be inadequate for interacting with human users. In many situations, one needs responses that follow the statistics of a helpful dialog with an assistant. This differs from the statistics of available large training sets, which combine novels, encyclopedias, forum messages, and blog posts.

This discrepancy is addressed by **fine-tuning (å¾®è°ƒ)** such a language model (see Â§ 3.6). The current dominant strategy is **Reinforcement Learning from Human Feedback (RLHF, åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )** [Ouyang et al., 2022], which consists of creating small labeled training sets by asking users to either write responses or provide ratings of generated responses. The former can be used as-is to fine-tune the language model, and the latter can be used to train a **reward network (å¥–åŠ±ç½‘ç»œ)** that predicts the rating and use it as a target to fine-tune the language model with a standard Reinforcement Learning approach.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Fine-Tuning (å¾®è°ƒ):** å¾®è°ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚
- **Reinforcement Learning from Human Feedback (RLHF, åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ):** RLHFæ˜¯ä¸€ç§é€šè¿‡äººç±»åé¦ˆæ¥æŒ‡å¯¼æ¨¡å‹è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºç”Ÿæˆæ›´ç¬¦åˆäººç±»æœŸæœ›çš„æ–‡æœ¬ã€‚

#### 7.2 Image Generation

**Image Generation (å›¾åƒç”Ÿæˆ)** is the process of creating new images using machine learning models. Multiple deep methods have been developed to model and sample from a high-dimensional density. A powerful approach for image synthesis relies on inverting a **diffusion process (æ‰©æ•£è¿‡ç¨‹)**. Such a generative model is referred to, somewhat incorrectly, as a **diffusion model (æ‰©æ•£æ¨¡å‹)**.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Diffusion Model (æ‰©æ•£æ¨¡å‹):** æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°æ¥ç ´åæ•°æ®ï¼Œç„¶åå­¦ä¹ å¦‚ä½•é€†è½¬è¿™ä¸ªè¿‡ç¨‹æ¥ç”Ÿæˆæ–°çš„æ•°æ®ã€‚

The principle consists of defining analytically a process that gradually degrades any sample, and consequently transforms the complex and unknown density of the data into a simple and well-known density such as a normal, and training a deep architecture to invert this degradation process [Ho et al., 2020].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Degradation Process (é€€åŒ–è¿‡ç¨‹):** é€€åŒ–è¿‡ç¨‹æ˜¯æŒ‡é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°æˆ–å…¶ä»–å½¢å¼çš„å¹²æ‰°æ¥ç ´åæ•°æ®çš„è¿‡ç¨‹ã€‚
- **Inversion (é€†è½¬):** é€†è½¬æ˜¯æŒ‡é€šè¿‡å­¦ä¹ å¦‚ä½•ä»å™ªå£°æ•°æ®ä¸­æ¢å¤åŸå§‹æ•°æ®çš„è¿‡ç¨‹ã€‚

Given a fixed \( T \), the diffusion process defines a probability distribution over series of \( T + 1 \) images as follows: sample \( x_0 \) uniformly from the dataset, and then sequentially sample \( x_{t+1} \sim p(x_{t+1} | x_t) \), \( t = 0, \ldots, T - 1 \), where the conditional distribution \( p \) is defined analytically and such that it gradually erases the structure that was in \( x_0 \). The setup should degrade the signal so much that the distribution \( p(x_T) \) has a known analytical form which can be sampled.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Conditional Distribution (æ¡ä»¶åˆ†å¸ƒ):** æ¡ä»¶åˆ†å¸ƒæ˜¯æŒ‡åœ¨ç»™å®šæŸäº›æ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡çš„åˆ†å¸ƒã€‚

For instance, Ho et al. [2020] normalize the data to have a mean of 0 and a variance of 1, and their diffusion process consists of adding a bit of white noise and re-normalizing the variance to 1. This process exponentially reduces the importance of \( x_0 \), and \( x_t \)â€™s density can rapidly be approximated with a normal.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **White Noise (ç™½å™ªå£°):** ç™½å™ªå£°æ˜¯ä¸€ç§å…·æœ‰å‡åŒ€åŠŸç‡è°±çš„éšæœºä¿¡å·ï¼Œé€šå¸¸ç”¨äºæ¨¡æ‹Ÿéšæœºå¹²æ‰°ã€‚

The **denoiser (å»å™ªå™¨)** \( f \) is a deep architecture that should model and allow sampling from \( f(x_{t-1}, x_t, t; w) \simeq p(x_{t-1} | x_t) \). It can be shown, thanks to a **variational bound (å˜åˆ†ç•Œ)**, that if this one-step reverse process is accurate enough, sampling \( x_T \sim p(x_T) \) and denoising \( T \) steps with \( f \) results in \( x_0 \) that follows \( p(x_0) \).

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Denoiser (å»å™ªå™¨):** å»å™ªå™¨æ˜¯æŒ‡é€šè¿‡å­¦ä¹ å¦‚ä½•ä»å™ªå£°æ•°æ®ä¸­æ¢å¤åŸå§‹æ•°æ®çš„æ¨¡å‹ã€‚
- **Variational Bound (å˜åˆ†ç•Œ):** å˜åˆ†ç•Œæ˜¯ä¸€ç§æ•°å­¦å·¥å…·ï¼Œç”¨äºä¼°è®¡å¤æ‚åˆ†å¸ƒçš„æ€§è´¨ã€‚

Training \( f \) can be achieved by generating a large number of sequences \( x_0^{(n)}, \ldots, x_T^{(n)} \), picking a \( t_n \) in each, and maximizing

\[
\sum_n \log f \left( x_{t_n-1}^{(n)}, x_{t_n}^{(n)}, t_n; w \right).
\]

Given their diffusion process, Ho et al. [2020] have a denoising of the form:

\[
x_{t-1} | x_t \sim \mathcal{N}(x_t + f(x_t, t; w); \sigma_t), \tag{7.1}
\]

where \( \sigma_t \) is defined analytically.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Normal Distribution (æ­£æ€åˆ†å¸ƒ):** æ­£æ€åˆ†å¸ƒæ˜¯ä¸€ç§å¸¸è§çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œé€šå¸¸ç”¨äºæ¨¡æ‹Ÿéšæœºå˜é‡çš„åˆ†å¸ƒã€‚

In practice, such a model initially hallucinates structures by pure luck in the random noise, and then gradually refines the image by reinforcing the most likely continuation of the image obtained thus far.

This approach can be extended to **text-conditioned synthesis (æ–‡æœ¬æ¡ä»¶åˆæˆ)**, to generate images that match a description. For instance, Nichol et al. [2021] add to the mean of the denoising distribution of Equation 7.1 a bias that goes in the direction of increasing the **CLIP matching score (CLIPåŒ¹é…åˆ†æ•°)** (see Â§ 6.6) between the produced image and the conditioning text description.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Text-Conditioned Synthesis (æ–‡æœ¬æ¡ä»¶åˆæˆ):** æ–‡æœ¬æ¡ä»¶åˆæˆæ˜¯æŒ‡æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆä¸ä¹‹åŒ¹é…çš„å›¾åƒã€‚
- **CLIP Matching Score (CLIPåŒ¹é…åˆ†æ•°):** CLIPåŒ¹é…åˆ†æ•°æ˜¯æŒ‡é€šè¿‡CLIPæ¨¡å‹è®¡ç®—çš„å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

### Chapter 8: The Compute Schism

#### 8.1 Prompt Engineering

**Prompt Engineering (æç¤ºå·¥ç¨‹)** is the process of carefully crafting the input to a machine learning model to elicit the desired output. The simplest strategy to specialize or improve a **Large Language Model (LLM, å¤§è¯­è¨€æ¨¡å‹)** with a limited computational budget is to use prompt engineering, that is, to carefully craft the beginning of the text sequence to bias the autoregressive process [Sahoo et al., 2024]. This approach moves a part of the information traditionally encoded in the modelâ€™s parameters to the input.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Prompt Engineering (æç¤ºå·¥ç¨‹):** æç¤ºå·¥ç¨‹æ˜¯æŒ‡é€šè¿‡è®¾è®¡ç‰¹å®šçš„è¾“å…¥æç¤ºæ¥å¼•å¯¼æ¨¡å‹ç”ŸæˆæœŸæœ›çš„è¾“å‡ºã€‚
- **Autoregressive Process (è‡ªå›å½’è¿‡ç¨‹):** è‡ªå›å½’è¿‡ç¨‹æ˜¯æŒ‡æ¨¡å‹æ ¹æ®å‰é¢ç”Ÿæˆçš„å…ƒç´ æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå…ƒç´ çš„è¿‡ç¨‹ã€‚

We saw in Â§ 7.1 a simple example of **few-shot prediction (å°‘æ ·æœ¬é¢„æµ‹)**, to use an LLM for a text classification task without fine-tuning. A long and sophisticated prompt allows generalizing this strategy to complex tasks.

Since the promptâ€™s role is to leverage the â€œgoodâ€ biases that were present in the training set, it benefits from surprising strategies such as stating that the response is generated by a skilled professional [Xu et al., 2023].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Few-Shot Prediction (å°‘æ ·æœ¬é¢„æµ‹):** å°‘æ ·æœ¬é¢„æµ‹æ˜¯æŒ‡æ¨¡å‹åœ¨åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„é¢„æµ‹ã€‚

The **context size (ä¸Šä¸‹æ–‡å¤§å°)** of a language model, that is, the number of tokens it can operate on, directly modulates the quantity of information that can be provided in the prompt. This is mostly constrained by the computational cost of standard attention models, which is quadratic with the context size (see Â§ 4.8).

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Context Size (ä¸Šä¸‹æ–‡å¤§å°):** ä¸Šä¸‹æ–‡å¤§å°æ˜¯æŒ‡æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚
- **Quadratic Cost (äºŒæ¬¡æˆæœ¬):** äºŒæ¬¡æˆæœ¬æ˜¯æŒ‡è®¡ç®—æˆæœ¬éšç€è¾“å…¥è§„æ¨¡çš„å¹³æ–¹å¢é•¿ã€‚

A remarkable type of prompting aims at making the model generate intermediate steps before generating the response itself.

Such a **chain-of-thought (æ€ç»´é“¾)** is composed of successive steps that are simpler, hence have been better modeled during training, and are predicted more deterministically [Wei et al., 2022; Kojima et al., 2022]. See Figure 8.1 for an example.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Chain-of-Thought (æ€ç»´é“¾):** æ€ç»´é“¾æ˜¯ä¸€ç§æç¤ºæ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹ç”Ÿæˆä¸­é—´æ­¥éª¤æ¥å¼•å¯¼å…¶ç”Ÿæˆæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚

**Retrieval-Augmented Generation (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**

Prompt engineering can also be put to use to connect a language model to an external knowledge base. It plays the role of a smart interface that allows the end user to formulate questions in natural language and get back a response that combines information that is not encoded in the modelâ€™s parameters [Lewis et al., 2020].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Retrieval-Augmented Generation (æ£€ç´¢å¢å¼ºç”Ÿæˆ):** æ£€ç´¢å¢å¼ºç”Ÿæˆæ˜¯æŒ‡é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“æ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚

For such **Retrieval-Augmented Generation (RAG)**, an **embedding model (åµŒå…¥æ¨¡å‹)** is used to retrieve documents whose embedding is correlated to that of the userâ€™s query. Then, a prompt is constructed by joining these retrieved documents with instructions to combine them, and the generative model produces the response to the userâ€™s query.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Embedding Model (åµŒå…¥æ¨¡å‹):** åµŒå…¥æ¨¡å‹æ˜¯æŒ‡å°†æ–‡æœ¬æˆ–å…¶ä»–æ•°æ®æ˜ å°„åˆ°ä½ç»´å‘é‡ç©ºé—´çš„æ¨¡å‹ã€‚

#### 8.2 Quantization

**Quantization (é‡åŒ–)** is the process of reducing the precision of the numbers used in a model to save memory and computational resources. Although training or generating multiple streams can benefit from high-end parallel computing devices, deployment of a **Large Language Model (LLM, å¤§è¯­è¨€æ¨¡å‹)** for individual use requires generally single-stream inference, which is bounded by memory size and speed far more than by computation.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Quantization (é‡åŒ–):** é‡åŒ–æ˜¯æŒ‡å°†æ¨¡å‹ä¸­çš„æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦çš„æ•´æ•°ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚
- **Single-Stream Inference (å•æµæ¨ç†):** å•æµæ¨ç†æ˜¯æŒ‡æ¨¡å‹åœ¨å•ä¸ªè®¡ç®—è®¾å¤‡ä¸Šè¿›è¡Œçš„æ¨ç†è¿‡ç¨‹ã€‚

As stated in Â§ 2.1, parameters, activations, and gradients are usually encoded with 32 or 16 bits. The precision it provides is necessary for training, to allow gradual changes to accumulate.

However, since activations are the sums of many terms, quantization during inference is mitigated by an averaging effect. This is even more true with large architectures, and models quantized down to 6 or 4 bits per parameter exhibit remarkable performance. Additionally to reducing the memory footprint, quantization also improves inference speed significantly.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Memory Footprint (å†…å­˜å ç”¨):** å†…å­˜å ç”¨æ˜¯æŒ‡æ¨¡å‹åœ¨è¿è¡Œæ—¶æ‰€éœ€è¦çš„å†…å­˜ç©ºé—´ã€‚
- **Inference Speed (æ¨ç†é€Ÿåº¦):** æ¨ç†é€Ÿåº¦æ˜¯æŒ‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºæ‰€éœ€çš„æ—¶é—´ã€‚

This has motivated the development of software to quantize existing models with **Post-Training Quantization (è®­ç»ƒåé‡åŒ–)**, and run them in single-stream inference on consumer hardware, such as **llama.cpp** [Llama.cpp, 2023]. This framework implements multiple formats, that apply specific quantization levels for the different weight matrices of a language model. For instance, the quantization may use more bits for the \( W^v \) weights of the attention blocks, and for the weights of the feed-forward blocks.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Post-Training Quantization (è®­ç»ƒåé‡åŒ–):** è®­ç»ƒåé‡åŒ–æ˜¯æŒ‡åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ä»¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚

An example of llama.cppâ€™s quantization is **Q4_1**. It quantizes individually sub-blocks of 32 entries of the original weight matrix by storing for each a scaling factor \( d \) and a bias \( m \) in the original FP16 encoding, and encoding each entry \( x \) with 4 bits as a value \( q \in \{0, \ldots, 2^4 - 1\} \). The resulting de-quantized value being \( \bar{x} = dq + m \).

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Scaling Factor (ç¼©æ”¾å› å­):** ç¼©æ”¾å› å­æ˜¯æŒ‡ç”¨äºè°ƒæ•´é‡åŒ–åæ•°å€¼èŒƒå›´çš„å‚æ•°ã€‚
- **Bias (åç½®):** åç½®æ˜¯æŒ‡ç”¨äºè°ƒæ•´é‡åŒ–åæ•°å€¼çš„åç§»é‡ã€‚

Such a block was encoded originally as 32 values in FP16, hence 64 bytes, while the quantized version needs 4 bytes for \( q \) and \( m \) and \( 32 \cdot 4 \) bits = 16 bytes for the entries, hence a total of 20 bytes.

Such an aggressive quantization surprisingly degrades only marginally the performance of the models, as illustrated on Figure 8.2.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Aggressive Quantization (æ¿€è¿›é‡åŒ–):** æ¿€è¿›é‡åŒ–æ˜¯æŒ‡å°†æ¨¡å‹ä¸­çš„æ•°å€¼ç²¾åº¦å¤§å¹…é™ä½ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚

An alternative to **Post-Training Quantization (è®­ç»ƒåé‡åŒ–)** is **Quantization-Aware Training (é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ)** that applies quantization during the forward pass but keeps high-precision encoding of parameters and gradients, and propagates the gradients during the backward pass as if there was no quantization [Ma et al., 2024].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Quantization-Aware Training (é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ):** é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–çš„æ•ˆæœï¼Œä»¥ä½¿æ¨¡å‹åœ¨é‡åŒ–åä»èƒ½ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚

#### 8.3 Adapters

**Adapters (é€‚é…å™¨)** are small modules added to a pre-trained model to adapt it to a specific task without modifying the original modelâ€™s parameters. As we saw in Â§ 3.6, **fine-tuning (å¾®è°ƒ)** is a key strategy to reuse pre-trained models. Since it aims at making only minor changes to an existing model, techniques have been developed that add components with few parameters, referred to as **adapters (é€‚é…å™¨)**, to the pre-trained architecture, and freeze all the original parameters [Houlsby et al., 2019].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Adapters (é€‚é…å™¨):** é€‚é…å™¨æ˜¯æŒ‡åœ¨ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ·»åŠ å°‘é‡å‚æ•°æ¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„å°æ¨¡å—ã€‚
- **Fine-Tuning (å¾®è°ƒ):** å¾®è°ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

The current dominant method is the **Low-Rank Adaptation (LoRA, ä½ç§©é€‚é…)**, which adds low-rank corrections to some of the modelâ€™s weight matrices [Hu et al., 2021].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Low-Rank Adaptation (LoRA, ä½ç§©é€‚é…):** ä½ç§©é€‚é…æ˜¯ä¸€ç§é€šè¿‡æ·»åŠ ä½ç§©çŸ©é˜µæ¥è°ƒæ•´æ¨¡å‹æƒé‡çš„æ–¹æ³•ï¼Œä»¥å‡å°‘å‚æ•°æ•°é‡ã€‚

Formally, given a linear operation of the form \( XW^T \), where \( X \) is a \( N \times D \) tensor of activations for a batch of \( N \) samples, and \( W \) is a \( C \times D \) weight matrix, the LoRA adapter replaces this operation with \( X(W + BA)^T \), where \( A \) and \( B \) are two trainable matrices of size \( R \times D \) and \( C \times R \) respectively, with \( R \ll \min(C, D) \), and the matrix \( W \) is removed from the trainable parameters. The matrix \( A \) is initialized with random Gaussian values, and \( B \) is set to zero, so that the fine-tuning starts with a model that computes an output identical to that of the original one.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Low-Rank Matrix (ä½ç§©çŸ©é˜µ):** ä½ç§©çŸ©é˜µæ˜¯æŒ‡ç§©è¿œå°äºå…¶è¡Œæ•°å’Œåˆ—æ•°çš„çŸ©é˜µï¼Œé€šå¸¸ç”¨äºè¿‘ä¼¼è¡¨ç¤ºé«˜ç»´æ•°æ®ã€‚

The total number of parameters to optimize with this approach is generally a few percent of the number of parameters in the original model.

The standard procedure to fine-tune a transformer with such adapters is to change only the weight matrices in the attention blocks, and to keep the **MLP (å¤šå±‚æ„ŸçŸ¥å™¨)** of the feed-forward blocks unchanged. The same strategy has been used successfully to tune **diffusion denoising models (æ‰©æ•£å»å™ªæ¨¡å‹)** by fine-tuning the attention blocks responsible for the text-based conditioning.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Attention Blocks (æ³¨æ„åŠ›å—):** æ³¨æ„åŠ›å—æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºæ•æ‰åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚
- **Diffusion Denoising Models (æ‰©æ•£å»å™ªæ¨¡å‹):** æ‰©æ•£å»å™ªæ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ å¦‚ä½•ä»å™ªå£°æ•°æ®ä¸­æ¢å¤åŸå§‹æ•°æ®æ¥ç”Ÿæˆæ–°çš„æ•°æ®ã€‚

Since fine-tuning with LoRA adapters drastically reduces the number of trainable parameters, it reduces the memory footprint required by optimizers such as **Adam**, which generally store two running averages per parameter to optimize. Also, it reduces slightly the computation during the backward pass.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Adam Optimizer (Adamä¼˜åŒ–å™¨):** Adamä¼˜åŒ–å™¨æ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•ï¼Œé€šå¸¸ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

For commercial applications that require a large number of fine-tuned models, the \( AB \) pairs can be stored separately from the original model, which has to be stored only once. And finally, contrary to other types of adapters, the modifications can be integrated into the original architecture, simply by adding \( AB \) to \( W \), resulting in an architecture and parameter count for inference that is identical to the original one.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Commercial Applications (å•†ä¸šåº”ç”¨):** å•†ä¸šåº”ç”¨æ˜¯æŒ‡å°†æ¨¡å‹åº”ç”¨äºå®é™…ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œé€šå¸¸éœ€è¦é«˜æ•ˆçš„æ¨ç†å’Œéƒ¨ç½²ã€‚

We saw that quantization degrades modelsâ€™ accuracy only marginally. However, gradient descent requires high precision in both the gradient and the trained parameters, to allow the accumulation of small changes. The **QLoRA (é‡åŒ–ä½ç§©é€‚é…)** approach combines a quantized base model and unquantized Low-Rank Adaptation to reduce the memory requirement even more [Dettmers et al., 2023].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **QLoRA (é‡åŒ–ä½ç§©é€‚é…):** QLoRAæ˜¯ä¸€ç§ç»“åˆé‡åŒ–å’Œä½ç§©é€‚é…çš„æ–¹æ³•ï¼Œç”¨äºè¿›ä¸€æ­¥å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚

#### 8.4 Model Merging

**Model Merging (æ¨¡å‹åˆå¹¶)** is the process of combining multiple models into a single model to leverage their combined capabilities. An alternative to the fine-tuning and prompting methods seen in the previous sections consists of combining multiple models with diverse capabilities into a single one, without additional training.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Model Merging (æ¨¡å‹åˆå¹¶):** æ¨¡å‹åˆå¹¶æ˜¯æŒ‡å°†å¤šä¸ªæ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªæ¨¡å‹ï¼Œä»¥åˆ©ç”¨å®ƒä»¬çš„ç»¼åˆèƒ½åŠ›ã€‚

Model merging relies on the compatibility between multiple fine-tuned versions of a base model.

Ilharco et al. [2022] showed that models obtained by fine-tuning a **CLIP (Contrastive Language-Image Pre-training, å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ)** base model on several image classification datasets can be combined in the parameter space, where they exhibit **Task Arithmetic (ä»»åŠ¡ç®—æœ¯)** properties.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Task Arithmetic (ä»»åŠ¡ç®—æœ¯):** ä»»åŠ¡ç®—æœ¯æ˜¯æŒ‡é€šè¿‡å°†å¤šä¸ªä»»åŠ¡æ¨¡å‹çš„å‚æ•°è¿›è¡Œç®—æœ¯æ“ä½œæ¥åˆå¹¶æ¨¡å‹çš„æ–¹æ³•ã€‚

Formally, let \( \theta \) be the parameter vector of a pre-trained model, and for \( t = 1, \ldots, T \), let \( \theta_t \) and \( \tau_t = \theta_t - \theta \) be respectively the parameters after fine-tuning on task \( t \) and the corresponding residual. Experiments show that the model with parameters \( \theta + \tau_1 + \cdots + \tau_T \) exhibits multi-task capabilities. Similarly, subtracting a \( \tau_t \) degrades the performance on the corresponding task.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Residual (æ®‹å·®):** æ®‹å·®æ˜¯æŒ‡æ¨¡å‹åœ¨å¾®è°ƒåå‚æ•°ä¸åŸå§‹å‚æ•°ä¹‹é—´çš„å·®å¼‚ã€‚

Methods have been developed to reduce the interference between the different residuals and improve the performance when the number of tasks is large [Yadav et al., 2023].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Interference (å¹²æ‰°):** å¹²æ‰°æ˜¯æŒ‡å¤šä¸ªä»»åŠ¡æ¨¡å‹åœ¨åˆå¹¶æ—¶å¯èƒ½äº§ç”Ÿçš„æ€§èƒ½ä¸‹é™ã€‚

An alternative to merging models in parameter space is to recombine their layers. Akiba et al. [2024] combine merging the parameters and re-combining layers, and rely on a stochastic optimization to deal with the combinatorial explosion. Experiments with three fine-tuned versions of **Mistral-7B** [Jiang et al., 2023] show that combining these two merging strategies outperforms both of them.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Stochastic Optimization (éšæœºä¼˜åŒ–):** éšæœºä¼˜åŒ–æ˜¯æŒ‡é€šè¿‡éšæœºæœç´¢æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°çš„æ–¹æ³•ã€‚
- **Combinatorial Explosion (ç»„åˆçˆ†ç‚¸):** ç»„åˆçˆ†ç‚¸æ˜¯æŒ‡éšç€ä»»åŠ¡æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹åˆå¹¶çš„å¯èƒ½æ€§å‘ˆæŒ‡æ•°å¢é•¿ã€‚

### The Missing Bits

For the sake of concision, this volume skips many important topics, in particular:

#### Recurrent Neural Networks

Before attention models showed greater performance, **Recurrent Neural Networks (RNN, å¾ªç¯ç¥ç»ç½‘ç»œ)** were the standard approach for dealing with temporal sequences such as text or sound samples. These architectures possess an internal **hidden state (éšè—çŠ¶æ€)** that gets updated each time a component of the sequence is processed. Their main components are layers such as **LSTM (Long Short-Term Memory, é•¿çŸ­æœŸè®°å¿†)** [Hochreiter and Schmidhuber, 1997] or **GRU (Gated Recurrent Unit, é—¨æ§å¾ªç¯å•å…ƒ)** [Cho et al., 2014].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Recurrent Neural Networks (RNN, å¾ªç¯ç¥ç»ç½‘ç»œ):** å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå…·æœ‰è®°å¿†èƒ½åŠ›ã€‚
- **Hidden State (éšè—çŠ¶æ€):** éšè—çŠ¶æ€æ˜¯æŒ‡RNNåœ¨å¤„ç†åºåˆ—æ—¶ä¿ç•™çš„å†…éƒ¨çŠ¶æ€ï¼Œç”¨äºæ•æ‰åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚

Training a recurrent architecture amounts to unfolding it in time, which results in a long composition of operators. This has historically prompted the design of key techniques now used for deep architectures such as **rectifiers (æ•´æµå™¨)** and **gating (é—¨æ§)**, a form of skip connections which are modulated by the hidden state.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Rectifiers (æ•´æµå™¨):** æ•´æµå™¨æ˜¯æŒ‡ç”¨äºå¤„ç†éçº¿æ€§æ¿€æ´»å‡½æ•°çš„ç»„ä»¶ï¼Œå¦‚ReLUã€‚
- **Gating (é—¨æ§):** é—¨æ§æ˜¯æŒ‡é€šè¿‡æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨æ¥å¢å¼ºæ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚

One of the key drawbacks of traditional recurrent architectures is that the structure of the computation \( x_{t+1} = f(x_t) \) imposes to process the input sequence serially, which takes a time proportional to \( T \). In contrast, transformers, for instance, can take advantage of parallel computation, resulting in a constant time if enough computing units are available.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Serial Processing (ä¸²è¡Œå¤„ç†):** ä¸²è¡Œå¤„ç†æ˜¯æŒ‡æŒ‰é¡ºåºå¤„ç†è¾“å…¥åºåˆ—ï¼Œé€šå¸¸éœ€è¦è¾ƒé•¿çš„æ—¶é—´ã€‚
- **Parallel Computation (å¹¶è¡Œè®¡ç®—):** å¹¶è¡Œè®¡ç®—æ˜¯æŒ‡åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

This is addressed by architectures such as **QRNN (Quasi-Recurrent Neural Networks, å‡†å¾ªç¯ç¥ç»ç½‘ç»œ)** [Bradbury et al., 2016], **S4 (Structured State Spaces, ç»“æ„åŒ–çŠ¶æ€ç©ºé—´)** [Gu et al., 2021], or **Mamba (Mambaæ¨¡å‹)** [Gu and Dao, 2023], whose recurrent operations are affine so that the \( f^t \) themselves, and consequently the \( x_t = f^t(x_0) \), can be computed in parallel, resulting in a constant time if \( f \) does not depend on \( t \) and \( \log T \) otherwise, again if enough parallel computing units are available.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Affine Operation (ä»¿å°„æ“ä½œ):** ä»¿å°„æ“ä½œæ˜¯æŒ‡çº¿æ€§å˜æ¢åŠ ä¸Šä¸€ä¸ªåç½®é¡¹çš„æ“ä½œã€‚
- **Parallel Computing Units (å¹¶è¡Œè®¡ç®—å•å…ƒ):** å¹¶è¡Œè®¡ç®—å•å…ƒæ˜¯æŒ‡èƒ½å¤ŸåŒæ—¶æ‰§è¡Œå¤šä¸ªè®¡ç®—ä»»åŠ¡çš„ç¡¬ä»¶è®¾å¤‡ã€‚

#### Autoencoder

An **autoencoder (è‡ªç¼–ç å™¨)** is a model that maps an input signal, possibly of high dimension, to a low-dimension **latent representation (æ½œåœ¨è¡¨ç¤º)**, and then maps it back to the original signal, ensuring that information has been preserved. We saw it in Â§ 6.1 for denoising, but it can also be used to automatically discover a meaningful low-dimension representation.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Autoencoder (è‡ªç¼–ç å™¨):** è‡ªç¼–ç å™¨æ˜¯ä¸€ç§é€šè¿‡å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°ä½ç»´ç©ºé—´å¹¶é‡å»ºåŸå§‹æ•°æ®çš„æ¨¡å‹ï¼Œé€šå¸¸ç”¨äºé™ç»´å’Œç‰¹å¾æå–ã€‚
- **Latent Representation (æ½œåœ¨è¡¨ç¤º):** æ½œåœ¨è¡¨ç¤ºæ˜¯æŒ‡æ•°æ®åœ¨ä½ç»´ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œé€šå¸¸ç”¨äºæ•æ‰æ•°æ®çš„ä¸»è¦ç‰¹å¾ã€‚

The **Variational Autoencoder (VAE, å˜åˆ†è‡ªç¼–ç å™¨)** proposed by Kingma and Welling [2013] is a generative model with a similar structure. It imposes, through the loss, a pre-defined distribution on the latent representation. This allows, after training, the generation of new samples by sampling the latent representation according to this imposed distribution and then mapping back through the decoder.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Variational Autoencoder (VAE, å˜åˆ†è‡ªç¼–ç å™¨):** å˜åˆ†è‡ªç¼–ç å™¨æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒæ¥ç”Ÿæˆæ–°çš„æ•°æ®ã€‚
- **Pre-Defined Distribution (é¢„å®šä¹‰åˆ†å¸ƒ):** é¢„å®šä¹‰åˆ†å¸ƒæ˜¯æŒ‡åœ¨æ¨¡å‹è®­ç»ƒå‰è®¾å®šçš„æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒï¼Œé€šå¸¸ä¸ºæ­£æ€åˆ†å¸ƒã€‚

#### Generative Adversarial Networks

Another approach to density modeling is the **Generative Adversarial Networks (GAN, ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)** introduced by Goodfellow et al. [2014]. This method combines a **generator (ç”Ÿæˆå™¨)**, which takes a random input following a fixed distribution as input and produces a structured signal such as an image, and a **discriminator (åˆ¤åˆ«å™¨)**, which takes a sample as input and predicts whether it comes from the training set or if it was generated by the generator.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Generative Adversarial Networks (GAN, ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ):** ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ˜¯ä¸€ç§é€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—æ¥ç”Ÿæˆæ–°æ•°æ®çš„æ¨¡å‹ã€‚
- **Generator (ç”Ÿæˆå™¨):** ç”Ÿæˆå™¨æ˜¯æŒ‡é€šè¿‡å­¦ä¹ å¦‚ä½•ä»éšæœºå™ªå£°ä¸­ç”Ÿæˆæ–°æ•°æ®çš„æ¨¡å‹ã€‚
- **Discriminator (åˆ¤åˆ«å™¨):** åˆ¤åˆ«å™¨æ˜¯æŒ‡é€šè¿‡å­¦ä¹ å¦‚ä½•åŒºåˆ†çœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®çš„æ¨¡å‹ã€‚

Training optimizes the discriminator to minimize a standard cross-entropy loss, and the generator to maximize the discriminatorâ€™s loss. It results in a generator that produces samples that are indistinguishable from real data. In practice, when the gradient flows through the discriminator to the generator, it informs the latter about the cues that the discriminator uses that need to be addressed.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Cross-Entropy Loss (äº¤å‰ç†µæŸå¤±):** äº¤å‰ç†µæŸå¤±æ˜¯æŒ‡ç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´å·®å¼‚çš„æŸå¤±å‡½æ•°ã€‚
- **Gradient Flow (æ¢¯åº¦æµ):** æ¢¯åº¦æµæ˜¯æŒ‡æ¢¯åº¦åœ¨æ¨¡å‹ä¸­çš„ä¼ æ’­è¿‡ç¨‹ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚

#### Graph Neural Networks

Many applications require processing signals which are not organized regularly on a grid. For instance, proteins, 3D meshes, geographic locations, or social interactions are more naturally structured as **graphs (å›¾)**. Standard convolutional networks or even attention models are poorly adapted to process such data, and the tool of choice for such a task is **Graph Neural Networks (GNN, å›¾ç¥ç»ç½‘ç»œ)** [Scarselli et al., 2009].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Graph Neural Networks (GNN, å›¾ç¥ç»ç½‘ç»œ):** å›¾ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç”¨äºå¤„ç†å›¾ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿæ•æ‰èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚
- **Graph (å›¾):** å›¾æ˜¯æŒ‡ç”±èŠ‚ç‚¹å’Œè¾¹ç»„æˆçš„æ•°æ®ç»“æ„ï¼Œé€šå¸¸ç”¨äºè¡¨ç¤ºå¤æ‚çš„å…³ç³»ã€‚

These models are composed of layers that compute activations at each vertex by combining linearly the activations located at its immediate neighboring vertices. This operation is very similar to a standard convolution, except that the data structure does not reflect any geometrical information associated with the feature vectors they carry.

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Vertex (é¡¶ç‚¹):** é¡¶ç‚¹æ˜¯æŒ‡å›¾ä¸­çš„èŠ‚ç‚¹ï¼Œé€šå¸¸ç”¨äºè¡¨ç¤ºå®ä½“ã€‚
- **Neighboring Vertices (ç›¸é‚»é¡¶ç‚¹):** ç›¸é‚»é¡¶ç‚¹æ˜¯æŒ‡ä¸æŸä¸ªé¡¶ç‚¹ç›´æ¥ç›¸è¿çš„å…¶ä»–é¡¶ç‚¹ã€‚

#### Self-Supervised Learning

As stated in Â§ 7.1, even though they are trained only to predict the next word, **Large Language Models (LLM, å¤§è¯­è¨€æ¨¡å‹)** trained on large unlabeled datasets such as **GPT (Generative Pre-trained Transformer, ç”Ÿæˆå¼é¢„è®­ç»ƒå˜å‹å™¨)** (see Â§ 5.3) are able to solve various tasks, such as identifying the grammatical role of a word, answering questions, or even translating from one language to another [Radford et al., 2019].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Self-Supervised Learning (è‡ªç›‘ç£å­¦ä¹ ):** è‡ªç›‘ç£å­¦ä¹ æ˜¯æŒ‡é€šè¿‡æ— æ ‡ç­¾æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **Grammatical Role (è¯­æ³•è§’è‰²):** è¯­æ³•è§’è‰²æ˜¯æŒ‡å•è¯åœ¨å¥å­ä¸­çš„è¯­æ³•åŠŸèƒ½ï¼Œå¦‚ä¸»è¯­ã€å®¾è¯­ç­‰ã€‚

Such models constitute one category of a larger class of methods that fall under the name of **self-supervised learning (è‡ªç›‘ç£å­¦ä¹ )**, and try to take advantage of unlabeled datasets [Balestriero et al., 2023].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Unlabeled Datasets (æ— æ ‡ç­¾æ•°æ®é›†):** æ— æ ‡ç­¾æ•°æ®é›†æ˜¯æŒ‡æ²¡æœ‰æ ‡æ³¨çš„æ•°æ®é›†ï¼Œé€šå¸¸ç”¨äºè‡ªç›‘ç£å­¦ä¹ ã€‚

The key principle of these methods is to define a task that does not require labels but necessitates feature representations which are useful for the real task of interest, for which a small labeled dataset exists. In computer vision, for instance, image features can be optimized so that they are invariant to data transformations that do not change the semantic content of the image, while being statistically uncorrelated [Zbontar et al., 2021].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Feature Representations (ç‰¹å¾è¡¨ç¤º):** ç‰¹å¾è¡¨ç¤ºæ˜¯æŒ‡æ•°æ®åœ¨æ¨¡å‹ä¸­çš„è¡¨ç¤ºï¼Œé€šå¸¸ç”¨äºæ•æ‰æ•°æ®çš„ä¸»è¦ç‰¹å¾ã€‚
- **Invariant (ä¸å˜æ€§):** ä¸å˜æ€§æ˜¯æŒ‡ç‰¹å¾è¡¨ç¤ºåœ¨æ•°æ®å˜æ¢ä¸‹ä¿æŒä¸å˜çš„æ€§è´¨ã€‚

In both NLP and computer vision, a powerful generic strategy is to train a model to recover a corrupted version of the input, for instance, by masking some of its components, or by predicting the missing parts of a sequence [Devlin et al., 2018; Zhou et al., 2021].

**çŸ¥è¯†ç‚¹è®²è§£:**
- **Corrupted Version (æŸåç‰ˆæœ¬):** æŸåç‰ˆæœ¬æ˜¯æŒ‡é€šè¿‡æ·»åŠ å™ªå£°æˆ–åˆ é™¤éƒ¨åˆ†æ•°æ®æ¥ç ´ååŸå§‹æ•°æ®çš„ç‰ˆæœ¬ã€‚
- **Masking (æ©ç ):** æ©ç æ˜¯æŒ‡é€šè¿‡éšè—éƒ¨åˆ†æ•°æ®æ¥è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚

### Bibliography

The bibliography section lists all the references cited in the book, providing the necessary citations for further reading and research.

### Index

The index section provides a quick reference to the key terms and concepts discussed in the book, allowing readers to easily locate specific topics.

---

This concludes the translation and explanation of the remaining chapters from Chapter 7 onwards. The content covers advanced topics in deep learning, including text generation, image generation, model optimization techniques like quantization and adapters, and the merging of models. Each section is accompanied by detailed explanations of the key concepts to enhance understanding.


